<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tim Riedel">
<meta name="author" content="Zainab Zafari">
<meta name="author" content="Sharjeel Shaik">
<meta name="author" content="Babajide Alamu Owoyele">
<meta name="author" content="Wim Pouw">
<meta name="dcterms.date" content="2025-08-08">

<title>MaskBench - A Comprehensive Benchmark Framework for 2D Pose Estimation and Video De-Identification</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="report_files/libs/clipboard/clipboard.min.js"></script>
<script src="report_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="report_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="report_files/libs/quarto-html/popper.min.js"></script>
<script src="report_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="report_files/libs/quarto-html/anchor.min.js"></script>
<link href="report_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="report_files/libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="report_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="report_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="report_files/libs/bootstrap/bootstrap-0207d67f596c720c255f8bb071783b54.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="dependencies/styles.css">
</head>

<body class="quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#sec-abstract" id="toc-sec-abstract" class="nav-link active" data-scroll-target="#sec-abstract"><span class="header-section-number">1</span> Abstract</a></li>
  <li><a href="#sec-installation" id="toc-sec-installation" class="nav-link" data-scroll-target="#sec-installation"><span class="header-section-number">2</span> Getting Started</a>
  <ul class="collapse">
  <li><a href="#sec-installation-setup" id="toc-sec-installation-setup" class="nav-link" data-scroll-target="#sec-installation-setup"><span class="header-section-number">2.1</span> 🛠️ Installation</a></li>
  <li><a href="#sec-installation-usage" id="toc-sec-installation-usage" class="nav-link" data-scroll-target="#sec-installation-usage"><span class="header-section-number">2.2</span> 🚀 Usage</a>
  <ul class="collapse">
  <li><a href="#dataset-structure" id="toc-dataset-structure" class="nav-link" data-scroll-target="#dataset-structure"><span class="header-section-number">2.2.1</span> 📂 Dataset structure</a></li>
  <li><a href="#configuration-files" id="toc-configuration-files" class="nav-link" data-scroll-target="#configuration-files"><span class="header-section-number">2.2.2</span> ⚙️ Configuration Files</a></li>
  <li><a href="#model-variants-and-weights" id="toc-model-variants-and-weights" class="nav-link" data-scroll-target="#model-variants-and-weights"><span class="header-section-number">2.2.3</span> 🏋️ Model Variants and Weights</a></li>
  <li><a href="#output" id="toc-output" class="nav-link" data-scroll-target="#output"><span class="header-section-number">2.2.4</span> 📊 Output</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-related-work" id="toc-sec-related-work" class="nav-link" data-scroll-target="#sec-related-work"><span class="header-section-number">3</span> Related Work</a></li>
  <li><a href="#sec-architecture" id="toc-sec-architecture" class="nav-link" data-scroll-target="#sec-architecture"><span class="header-section-number">4</span> MaskBench Architecture</a>
  <ul class="collapse">
  <li><a href="#sec-architecture-dataset" id="toc-sec-architecture-dataset" class="nav-link" data-scroll-target="#sec-architecture-dataset"><span class="header-section-number">4.1</span> Dataset</a></li>
  <li><a href="#sec-architecture-inference" id="toc-sec-architecture-inference" class="nav-link" data-scroll-target="#sec-architecture-inference"><span class="header-section-number">4.2</span> Inference</a>
  <ul class="collapse">
  <li><a href="#sec-architecture-video-pose-result" id="toc-sec-architecture-video-pose-result" class="nav-link" data-scroll-target="#sec-architecture-video-pose-result"><span class="header-section-number">4.2.1</span> Video Pose Result</a></li>
  <li><a href="#sec-architecture-pose-estimators" id="toc-sec-architecture-pose-estimators" class="nav-link" data-scroll-target="#sec-architecture-pose-estimators"><span class="header-section-number">4.2.2</span> Pose Estimator</a></li>
  <li><a href="#sec-architecture-inference-engine" id="toc-sec-architecture-inference-engine" class="nav-link" data-scroll-target="#sec-architecture-inference-engine"><span class="header-section-number">4.2.3</span> Inference Engine</a></li>
  </ul></li>
  <li><a href="#sec-architecture-evaluation" id="toc-sec-architecture-evaluation" class="nav-link" data-scroll-target="#sec-architecture-evaluation"><span class="header-section-number">4.3</span> Evaluation</a>
  <ul class="collapse">
  <li><a href="#sec-architecture-evaluation-metric" id="toc-sec-architecture-evaluation-metric" class="nav-link" data-scroll-target="#sec-architecture-evaluation-metric"><span class="header-section-number">4.3.1</span> Metric</a></li>
  <li><a href="#sec-architecture-evaluation-metric-result" id="toc-sec-architecture-evaluation-metric-result" class="nav-link" data-scroll-target="#sec-architecture-evaluation-metric-result"><span class="header-section-number">4.3.2</span> Metric Result</a></li>
  <li><a href="#sec-architecture-evaluation-evaluator" id="toc-sec-architecture-evaluation-evaluator" class="nav-link" data-scroll-target="#sec-architecture-evaluation-evaluator"><span class="header-section-number">4.3.3</span> Evaluator</a></li>
  </ul></li>
  <li><a href="#sec-architecture-visualization" id="toc-sec-architecture-visualization" class="nav-link" data-scroll-target="#sec-architecture-visualization"><span class="header-section-number">4.4</span> Visualization</a>
  <ul class="collapse">
  <li><a href="#visualizer" id="toc-visualizer" class="nav-link" data-scroll-target="#visualizer"><span class="header-section-number">4.4.1</span> Visualizer</a></li>
  <li><a href="#plots" id="toc-plots" class="nav-link" data-scroll-target="#plots"><span class="header-section-number">4.4.2</span> Plots</a></li>
  </ul></li>
  <li><a href="#sec-architecture-rendering" id="toc-sec-architecture-rendering" class="nav-link" data-scroll-target="#sec-architecture-rendering"><span class="header-section-number">4.5</span> Rendering</a></li>
  </ul></li>
  <li><a href="#sec-datasets" id="toc-sec-datasets" class="nav-link" data-scroll-target="#sec-datasets"><span class="header-section-number">5</span> Datasets</a>
  <ul class="collapse">
  <li><a href="#sec-datasets-ted-kid" id="toc-sec-datasets-ted-kid" class="nav-link" data-scroll-target="#sec-datasets-ted-kid"><span class="header-section-number">5.1</span> TED Kid Video</a></li>
  <li><a href="#sec-datasets-ted-talks" id="toc-sec-datasets-ted-talks" class="nav-link" data-scroll-target="#sec-datasets-ted-talks"><span class="header-section-number">5.2</span> TED Talks</a></li>
  <li><a href="#sec-datasets-tragic-talkers" id="toc-sec-datasets-tragic-talkers" class="nav-link" data-scroll-target="#sec-datasets-tragic-talkers"><span class="header-section-number">5.3</span> Tragic Talkers</a></li>
  <li><a href="#sec-datasets-masked-video" id="toc-sec-datasets-masked-video" class="nav-link" data-scroll-target="#sec-datasets-masked-video"><span class="header-section-number">5.4</span> Masked Video Dataset</a></li>
  <li><a href="#data-preprocessing" id="toc-data-preprocessing" class="nav-link" data-scroll-target="#data-preprocessing"><span class="header-section-number">5.5</span> Data Preprocessing</a></li>
  </ul></li>
  <li><a href="#sec-metrics" id="toc-sec-metrics" class="nav-link" data-scroll-target="#sec-metrics"><span class="header-section-number">6</span> Evaluation Metrics</a>
  <ul class="collapse">
  <li><a href="#ground-truth-metrics" id="toc-ground-truth-metrics" class="nav-link" data-scroll-target="#ground-truth-metrics"><span class="header-section-number">6.1</span> Ground-Truth Metrics</a>
  <ul class="collapse">
  <li><a href="#sec-metrics-euclidean-distance" id="toc-sec-metrics-euclidean-distance" class="nav-link" data-scroll-target="#sec-metrics-euclidean-distance"><span class="header-section-number">6.1.1</span> Euclidean Distance</a></li>
  <li><a href="#sec-metrics-pck" id="toc-sec-metrics-pck" class="nav-link" data-scroll-target="#sec-metrics-pck"><span class="header-section-number">6.1.2</span> Percentage of Keypoints (PCK)</a></li>
  <li><a href="#sec-metrics-rmse" id="toc-sec-metrics-rmse" class="nav-link" data-scroll-target="#sec-metrics-rmse"><span class="header-section-number">6.1.3</span> Root Mean Square Error (RMSE)</a></li>
  </ul></li>
  <li><a href="#kinematic-metrics" id="toc-kinematic-metrics" class="nav-link" data-scroll-target="#kinematic-metrics"><span class="header-section-number">6.2</span> Kinematic Metrics</a>
  <ul class="collapse">
  <li><a href="#sec-metrics-velocity" id="toc-sec-metrics-velocity" class="nav-link" data-scroll-target="#sec-metrics-velocity"><span class="header-section-number">6.2.1</span> Velocity</a></li>
  <li><a href="#sec-metrics-acceleration" id="toc-sec-metrics-acceleration" class="nav-link" data-scroll-target="#sec-metrics-acceleration"><span class="header-section-number">6.2.2</span> Acceleration</a></li>
  <li><a href="#sec-metrics-jerk" id="toc-sec-metrics-jerk" class="nav-link" data-scroll-target="#sec-metrics-jerk"><span class="header-section-number">6.2.3</span> Jerk</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-experiments" id="toc-sec-experiments" class="nav-link" data-scroll-target="#sec-experiments"><span class="header-section-number">7</span> Experimental Setup</a></li>
  <li><a href="#sec-results" id="toc-sec-results" class="nav-link" data-scroll-target="#sec-results"><span class="header-section-number">8</span> Results</a>
  <ul class="collapse">
  <li><a href="#sec-results-ted-kid" id="toc-sec-results-ted-kid" class="nav-link" data-scroll-target="#sec-results-ted-kid"><span class="header-section-number">8.1</span> TED Kid Video</a></li>
  <li><a href="#sec-results-ted-talks" id="toc-sec-results-ted-talks" class="nav-link" data-scroll-target="#sec-results-ted-talks"><span class="header-section-number">8.2</span> TED Talks</a></li>
  <li><a href="#sec-results-tragic-talkers" id="toc-sec-results-tragic-talkers" class="nav-link" data-scroll-target="#sec-results-tragic-talkers"><span class="header-section-number">8.3</span> Tragic Talkers</a></li>
  <li><a href="#sec-results-inference-raw-masked" id="toc-sec-results-inference-raw-masked" class="nav-link" data-scroll-target="#sec-results-inference-raw-masked"><span class="header-section-number">8.4</span> Inference on Raw vs.&nbsp;Masked Videos</a></li>
  </ul></li>
  <li><a href="#sec-future-work" id="toc-sec-future-work" class="nav-link" data-scroll-target="#sec-future-work"><span class="header-section-number">9</span> Future Work &amp; Limitations</a>
  <ul class="collapse">
  <li><a href="#maskanyone-limitations" id="toc-maskanyone-limitations" class="nav-link" data-scroll-target="#maskanyone-limitations"><span class="header-section-number">9.1</span> MaskAnyone Limitations</a></li>
  <li><a href="#maskbench-outlook" id="toc-maskbench-outlook" class="nav-link" data-scroll-target="#maskbench-outlook"><span class="header-section-number">9.2</span> MaskBench Outlook</a>
  <ul class="collapse">
  <li><a href="#sec-future-work-pipelining" id="toc-sec-future-work-pipelining" class="nav-link" data-scroll-target="#sec-future-work-pipelining"><span class="header-section-number">9.2.1</span> Pipelining</a></li>
  <li><a href="#sec-future-work-downstream-tasks" id="toc-sec-future-work-downstream-tasks" class="nav-link" data-scroll-target="#sec-future-work-downstream-tasks"><span class="header-section-number">9.2.2</span> Evaluation of downstream tasks</a></li>
  <li><a href="#user-interface" id="toc-user-interface" class="nav-link" data-scroll-target="#user-interface"><span class="header-section-number">9.2.3</span> User interface</a></li>
  <li><a href="#additional-improvements" id="toc-additional-improvements" class="nav-link" data-scroll-target="#additional-improvements"><span class="header-section-number">9.2.4</span> Additional improvements</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#sec-conclusion" id="toc-sec-conclusion" class="nav-link" data-scroll-target="#sec-conclusion"><span class="header-section-number">10</span> Conclusion</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">11</span> References</a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">MaskBench - A Comprehensive Benchmark Framework for 2D Pose Estimation and Video De-Identification</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliations</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Tim Riedel </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Hasso Plattner Institute, University of Potsdam, Germany
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Zainab Zafari </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Hasso Plattner Institute, University of Potsdam, Germany
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Sharjeel Shaik </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            University of Potsdam, Germany
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Babajide Alamu Owoyele </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Hasso Plattner Institute, University of Potsdam, Germany
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Wim Pouw </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Tilburg University, Netherlands
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">August 8, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="sec-abstract" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Abstract</h1>
<p>Pose estimation plays a critical role in numerous computer vision applications but remains challenging in scenarios involving privacy-sensitive data and in real-world, unconstrained videos like TED Talks, that are not recorded under controlled laboratory conditions. To address the issue of sharing datasets across academic institutions without compromising privacy, we explore how masking strategies like blurring, pixelation, contour overlays, and solid fills impact pose estimation performance. We introduce MaskBench, a modular and extensible benchmarking framework designed to evaluate pose estimators under varying conditions, including masked video inputs. MaskBench integrates a total of seven pose estimators, including YoloPose, MediaPipePose, OpenPose, and both automated and human-in-the-loop variants of MaskAnyone, a multi-stage pipeline combining segmentation and pose estimation through a mixture-of-expert approach. Our evaluation was done on four datasets with increasing scene complexity, and uses both kinematic metrics like velocity, acceleration, and jerk as well as accuracy-based metrics like Percentage of Correct Keypoints (PCK) and Root Mean Square Error (RMSE). Results show that MaskAnyone variants significantly improve the visual quality of the pose estimation by reducing jitter and improving keypoint stability, especially the human-in-the-loop variants of MaskAnyone-MediaPipe. These visual results are supported by quantitative metrics, with the aforementioned models achieving the lowest acceleration and jerk values across all datasets. YoloPose consistently ranks as the most robust standalone model. Regarding masking techniques, preliminary results suggest that blurring offers a promising balance between privacy and pose estimation quality. However, since this experiment was conducted on a limited set of videos, further investigation is needed to draw general conclusions. These findings highlight the potential of pipelines like MaskAnyone and the extensibility of MaskBench for future research on pose estimation under privacy-preserving constraints.</p>
</section>
<section id="sec-installation" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Getting Started</h1>
<section id="sec-installation-setup" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="sec-installation-setup"><span class="header-section-number">2.1</span> 🛠️ Installation</h2>
<p><strong>System Requirements</strong></p>
<ul>
<li>🐳 <strong>Docker</strong>: Latest stable version<br>
</li>
<li>🎮 <strong>GPU</strong>: NVIDIA (CUDA-enabled)<br>
</li>
<li>💾 <strong>Memory</strong>: 20 GB or more</li>
</ul>
<p>Follow the instructions below to install and run experiments with MaskBench:</p>
<ol type="1">
<li><p><strong>Install Docker</strong> and ensure the daemon is running.</p></li>
<li><p><strong>Clone this repo</strong>:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> clone https://github.com/maskbench/maskbench.git</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><strong>Setup the folder structure</strong>. You can store the datasets, outputs or weights in any location you want (for example, if your dataset is large and stored on a separate disk), however, to minimize setup overhead, we suggest the following folder structure:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">maskbench/</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="ex">├──</span> src</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="ex">└──</span> config/</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="ex">└──</span> your-experiment-config.yml</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="ex">maskbench_assets/</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="ex">├──</span> weights</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="ex">├──</span> output</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="ex">└──</span> datasets/</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="ex">└──</span> your-dataset/</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        <span class="ex">├──</span> videos/</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="ex">│</span>   └── video_name1.mp4</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        <span class="ex">├──</span> labels/</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        <span class="ex">│</span>   └── video_name1.json</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        <span class="ex">├──</span> maskanyone_ui_mediapipe/</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        <span class="ex">│</span>   └── video_name1.json</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        <span class="ex">└──</span> maskanyone_ui_openpose/</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>            <span class="ex">└──</span> video_name1.json</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><strong>Switch to the git repository</strong></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> maskbench</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><strong>Create the environment file</strong>. This file is used to tell MaskBench about your dataset, output and weights directory, as well as the configuration file to use for an experiment. Copy the .env file using:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cp</span> .env.dist .env</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><strong>Edit the .env file</strong>. Open it using <code>vim .env</code> or <code>nano .env.</code>. Adjust the following variables:</p>
<ul>
<li><code>MASKBENCH_GPU_ID:</code> If you are on a multi-GPU setup, tell MaskBench which GPU to use. Either specify a number (0, 1, …) or “all” in which case all available GPUs on the system are used. Currently, MaskBench only supports inference on a single GPU or on all GPUs.</li>
<li><code>MASKBENCH_CONFIG_FILE:</code> The configuration file used to define your experiment setup.</li>
</ul>
<p>The following variables only need to be adjusted, if you use a different folder structure than the one proposed above:</p>
<ul>
<li><code>MASKBENCH_DATASET_DIR:</code> The directory where video files are located. MaskBench supports video files with .mp4 and .avi extensions.</li>
<li><code>MASKBENCH_OUTPUT_DIR:</code> The directory where experiment results will be saved.</li>
<li><code>MASKBENCH_WEIGHTS_DIR:</code> Directory for storing model weights (e.g., MediaPipe, YOLOv11, OpenPose).</li>
</ul></li>
<li><p><strong>Run the MaskBench Docker container</strong>.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> compose up</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If multiple users run MaskBench simultaneously, use <code>docker compose -p $USER up</code>.</p></li>
<li><p><strong>Install MaskAnyone</strong>. If you plan on using the UI version of MaskAnyone to create smooth poses, masked videos and improve raw pose estimation models, follow the installation instructions <a href="https://github.com/MaskAnyone/MaskAnyone">here</a>.</p></li>
</ol>
</section>
<section id="sec-installation-usage" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="sec-installation-usage"><span class="header-section-number">2.2</span> 🚀 Usage</h2>
<p>The following paragraphs describe how to structure your dataset, configure the application, and understand the output of MaskBench. Following these guidelines ensures the application runs smoothly and recognizes your data correctly.</p>
<section id="dataset-structure" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="dataset-structure"><span class="header-section-number">2.2.1</span> 📂 Dataset structure</h3>
<ol type="1">
<li><p><strong>Videos</strong>: Place all videos you want to evaluate in the <code>videos</code> folder.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="ex">your-dataset/</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="ex">├──</span> videos/</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="ex">│</span>    ├── video_name1.mp4</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="ex">│</span>    ├── video_name2.mp4</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><strong>Labels</strong> (Optional): If you provide labels, there must be exactly one label file for each video, with the same file name. Example:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="ex">your-dataset/</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="ex">├──</span> videos/</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="ex">│</span>    └── video_name1.mp4</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="ex">├──</span> labels/</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="ex">│</span>    └── video_name2.json</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><strong>MaskAnyoneUI Output</strong>: If you use MaskAnyoneUI, run the application, download the resulting pose file, store it in either the <code>maskanyone_ui_openpose</code> or <code>maskanyone_ui_mediapipe</code> folder and once again name it exactly like the corresponding video file.</p></li>
</ol>
</section>
<section id="configuration-files" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="configuration-files"><span class="header-section-number">2.2.2</span> ⚙️ Configuration Files</h3>
<p>We provide four sample configuration files from our experiments. Feel free to copy and adapt them to your needs. The following note explains some parameters in more detail.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Explanation of config parameters
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="sourceCode" id="cb8"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># A directory name (MaskBench run name) inside the output directory from which to load existing results.</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># If set, inference is skipped and results are loaded. To run inference from scratch, comment out or set to "None".</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="fu">inference_checkpoint_name</span><span class="kw">:</span><span class="at"> None</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="fu">execute_evaluation</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span><span class="co">                    # Set to false to skip calculating evaluation metrics and plotting.</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="fu">execute_rendering</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span><span class="co">                     # Set to false to skip rendering the videos.</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="fu">dataset</span><span class="kw">:</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> TragicTalkers</span><span class="co">                       # User-definable name of the dataset</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">module</span><span class="kw">:</span><span class="at"> datasets.tragic_talkers_dataset</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">class</span><span class="kw">:</span><span class="at"> TragicTalkersDataset</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">dataset_folder</span><span class="kw">:</span><span class="at"> /datasets/tragic-talkers</span><span class="co">  # Location of the dataset folder (always starts with /datasets, because this refers to the mounted folder in the docker container). You only need to adjust the name of the folder.</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">config</span><span class="kw">:</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">video_folder</span><span class="kw">:</span><span class="at"> videos</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">ground_truth_folder</span><span class="kw">:</span><span class="at"> labels</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">convert_gt_keypoints_to_coco</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="fu">pose_estimators</span><span class="kw">:</span><span class="co">                            # List of pose estimators (specificy as many as needed)</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> YoloPose</span><span class="co">                          # User-definable name of the pose estimator. </span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">enabled</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span><span class="co">                           # Enable or disable this pose estimator.</span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">module</span><span class="kw">:</span><span class="at"> models.yolo_pose_estimator</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">class</span><span class="kw">:</span><span class="at"> YoloPoseEstimator</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">config</span><span class="kw">:</span><span class="co">                                 # Pose estimator specific configuration variables.</span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">weights</span><span class="kw">:</span><span class="at"> yolo11l-pose.pt</span><span class="co">              # Weights file name inside the specified weights directory.</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">save_keypoints_in_coco_format</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span><span class="co">   # Whether to store keypoints in COCO format (18 keypoints) or not)</span></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">confidence_threshold</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.3</span><span class="co">             # Confidence threshold below which keyopints are considered undetected.</span></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> MaskAnyoneUI-MediaPipe</span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">enabled</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">module</span><span class="kw">:</span><span class="at"> models.maskanyone_ui_pose_estimator</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">class</span><span class="kw">:</span><span class="at"> MaskAnyoneUiPoseEstimator  </span></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">config</span><span class="kw">:</span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">dataset_folder_path</span><span class="kw">:</span><span class="at"> /datasets/tragic-talkers/maskanyone_ui_mediapipe</span><span class="co"> # Folder of MaskAnyone poses</span></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">overlay_strategy</span><span class="kw">:</span><span class="at"> mp_pose</span><span class="co">             # Mediapipe: mp_pose, OpenPose: openpose, openpose_body25b</span></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">save_keypoints_in_coco_format</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">confidence_threshold</span><span class="kw">:</span><span class="at"> </span><span class="dv">0</span><span class="co">               # Confidence thresholds not supported by MaskAnyone</span></span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a><span class="fu">metrics</span><span class="kw">:</span><span class="co">                                    # List of metrics  (specificy as many as needed)</span></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> PCK</span><span class="co">                               # User-definable name of the metric</span></span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">module</span><span class="kw">:</span><span class="at"> evaluation.metrics.pck</span></span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">class</span><span class="kw">:</span><span class="at"> PCKMetric</span></span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">config</span><span class="kw">:</span><span class="co">                                 # Metric specific configuration variables.</span></span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">normalize_by</span><span class="kw">:</span><span class="at"> bbox</span></span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">threshold</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.2</span></span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> Velocity</span></span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">module</span><span class="kw">:</span><span class="at"> evaluation.metrics.velocity</span></span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">class</span><span class="kw">:</span><span class="at"> VelocityMetric</span></span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">config</span><span class="kw">:</span></span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">time_unit</span><span class="kw">:</span><span class="at"> frame</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="model-variants-and-weights" class="level3" data-number="2.2.3">
<h3 data-number="2.2.3" class="anchored" data-anchor-id="model-variants-and-weights"><span class="header-section-number">2.2.3</span> 🏋️ Model Variants and Weights</h3>
<p>You only need to modify the weights if you are adding a new pose estimator to MaskBench. In that case, place your weights in the <code>weights/</code> folder. By default, MaskBench automatically downloads the following weights:</p>
<ul>
<li>MediaPipe: <code>pose_landmarker_{lite, full, heavy}.task</code></li>
<li>Yolo11: <code>yolo11{n, s, m, l, x}-pose</code></li>
<li>OpenPose overlay_strategy: <code>BODY_25, BODY_25B, COCO</code></li>
<li>MaskAnyone overlay_strategy: <code>mp_pose, openpose, openpose_body25b</code></li>
</ul>
</section>
<section id="output" class="level3" data-number="2.2.4">
<h3 data-number="2.2.4" class="anchored" data-anchor-id="output"><span class="header-section-number">2.2.4</span> 📊 Output</h3>
<p>All results, including plots, pose files, inference times and renderings, will be saved in the output directory. For each run of MaskBench a folder is created with the name of the dataset and a timestamp. Example:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="ex">output/</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a> <span class="ex">├──</span> TedTalks_2025-08-11_15-42-10/</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a> <span class="ex">│</span>    ├── plots/</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a> <span class="ex">│</span>    ├── poses/</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a> <span class="ex">│</span>    ├── renderings/</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a> <span class="ex">│</span>    ├── inference_times.json</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
</section>
<section id="sec-related-work" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Related Work</h1>
<p>Human pose estimation is a core task in computer vision, concerned with identifying the spatial positions of body joints—such as shoulders, elbows, and knees—from images or video sequences. Existing approaches are typically classified into two broad strategies: top-down and bottom-up. Top-down methods begin by detecting individual persons within an image, after which a separate pose estimation model is applied to each detected instance. In contrast, bottom-up approaches first detect all keypoints across the image and then group them to form full-body poses for each individual <span class="citation" data-cites="saiwa2025openpose kaim2024comparison">(<a href="#ref-saiwa2025openpose" role="doc-biblioref">Saiva 2025</a>; <a href="#ref-kaim2024comparison" role="doc-biblioref">Kaim et al. 2024</a>)</span>.</p>
<p>Among widely used frameworks, <strong>OpenPose</strong> <span class="citation" data-cites="openpose-3">(<a href="#ref-openpose-3" role="doc-biblioref">Zhe Cao et al. 2017</a>)</span> is a prominent example of a bottom-up pose estimation method. It first identifies keypoints across the entire image and then assembles them into person-wise skeletons using Part Affinity Fields (PAFs). OpenPose supports multiple configurations, including 18- and 25-keypoint body models <a href="#fig-openpose" class="quarto-xref">Figure&nbsp;1 (a)</a>, and offers full-body tracking, hand and facial landmark estimation. The framework is optimized for GPU execution and is widely used in applications requiring multi-person pose estimation.</p>
<p><strong>YOLO11</strong> <span class="citation" data-cites="yolo yolo11_ultralytics">(<a href="#ref-yolo" role="doc-biblioref">Redmon et al. 2016</a>; <a href="#ref-yolo11_ultralytics" role="doc-biblioref">Jocher and Qiu 2024</a>)</span>, on the other hand, follows a top-down approach. It extends the YOLO family of real-time object detectors by incorporating pose estimation capabilities. After detecting bounding boxes for each person, YOLO11 predicts 17 body keypoints <a href="#fig-yolo" class="quarto-xref">Figure&nbsp;1 (b)</a> per individual, using a topology aligned with the COCO keypoint format. It is designed for high-performance scenarios and is optimized for GPU usage, making it suitable for real-time, multi-person tracking in high-resolution video streams.</p>
<p><strong>MediaPipe Pose</strong> <span class="citation" data-cites="mediapipe">(<a href="#ref-mediapipe" role="doc-biblioref">Lugaresi et al. 2019</a>)</span> is a lightweight, top-down framework designed specifically for real-time pose estimation on CPU-only devices. Built upon BlazePose, it employs a 33-landmark skeleton <a href="#fig-mediapipe" class="quarto-xref">Figure&nbsp;1 (c)</a> that extends the standard COCO format with additional joints to improve anatomical precision. The pipeline consists of an initial detection and tracking stage, followed by landmark prediction and overlay. MediaPipe is particularly suited for single-person applications in mobile and browser environments, where computational efficiency and low latency are critical.</p>
<div id="fig-keypoints" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-keypoints-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-keypoints" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-openpose" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-openpose-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/openpose-keypointbody25.png" data-ref-parent="fig-keypoints" height="200" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-openpose-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) OpenPose
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-keypoints" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-yolo" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-yolo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/yolo-keypoints.png" data-ref-parent="fig-keypoints" height="200" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-yolo-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) YOLO
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-keypoints" style="flex-basis: 33.3%;justify-content: flex-start;">
<div id="fig-mediapipe" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-mediapipe-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/mediapipe-landmarks.png" data-ref-parent="fig-keypoints" height="200" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-mediapipe-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) MediaPipe
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-keypoints-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Some of the most common pose models and their keypoints. <strong>YOLOv11</strong> uses the COCO format with 17 keypoints. <strong>OpenPose</strong> supports COCO (18) and BODY-25 (25). <strong>MediaPipe</strong> uses a 33-landmark skeleton.
</figcaption>
</figure>
</div>
<p>Several studies have benchmarked popular pose estimation models across different datasets, conditions, and use cases. The following works are particularly relevant to our benchmarking framework:</p>
<ul>
<li><p><strong>Comparision Of ML Models For Posture</strong> <span class="citation" data-cites="kaim2024comparison">(<a href="#ref-kaim2024comparison" role="doc-biblioref">Kaim et al. 2024</a>)</span> Compared YOLOv7 Pose and MediaPipe Pose. YOLOv7 achieved a slightly higher accuracy score of 87.8% versus MediaPipe’s 84.1%. However, MediaPipe demonstrated superior real-time performance on CPU-only devices, achieving 16-18 frames per second (FPS) compared to YOLOv7’s 4-5 FPS. In low-light environments, MediaPipe maintained detection consistency, whereas YOLOv7 performed better in occluded scenarios, successfully recognizing hidden body parts.</p></li>
<li><p><strong>OpenPose vs MediaPipe: A Practical and Architectural Comparison</strong> <span class="citation" data-cites="saiwa2025openpose">(<a href="#ref-saiwa2025openpose" role="doc-biblioref">Saiva 2025</a>)</span> A recent blog post by Saiwa presents a detailed comparison between OpenPose and MediaPipe, discussing their architectural differences, device compatibility, and practical applications. OpenPose uses a bottom-up approach with Part Affinity Fields and is optimized for multi-person full-body tracking, whereas MediaPipe follows a top-down strategy focusing on speed and cross-platform deployment.</p></li>
</ul>
<p>In addition to standalone pose estimation models, <strong>MaskAnyone</strong> <span class="citation" data-cites="maskanyone-github schilling2023maskanyone">(<a href="#ref-maskanyone-github" role="doc-biblioref">Schilling 2024a</a>, <a href="#ref-schilling2023maskanyone" role="doc-biblioref">2024b</a>)</span> is a multi-stage framework developed at the Hasso Plattner Institute (HPI) that combines object detection, segmentation, de-identification, and pose estimation within a unified pipeline. The system begins by detecting human instances in a video using YOLO. For each detected bounding box, SAM2 <span class="citation" data-cites="sam2">(<a href="#ref-sam2" role="doc-biblioref">Ravi et al. 2024</a>)</span> is applied to segment the individual subject. Depending on the configuration, pose estimation is then performed using either OpenPose or MediaPipe. Last but not least, it produces a de-identified video using the SAM2 segmentation masks. The framework supports both fully automatic processing (we refer to it as MaskAnyoneAPI) and a human-in-the-loop approach (referred to as MaskAnyoneUI), where users can manually select specific frames, refine the segmentation output of SAM2 and thereafter start the pose estimation. This combination of automated and user-guided steps allows for finer control in scenarios where automatic segmentation may be insufficient or require correction.</p>
</section>
<section id="sec-architecture" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> MaskBench Architecture</h1>
<div id="fig-architecture" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/maskbench-workflow.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: MaskBench Architecture
</figcaption>
</figure>
</div>
<p>The general workflow of MaskBench is shown in Figure <a href="#fig-architecture" class="quarto-xref">Figure&nbsp;2</a>. It begins with loading the dataset, pose estimators, and evaluation metrics. The application then creates a checkpoint folder in the specified output directory, named according to the dataset and a timestamp (e.g., <code>/output/TedTalks-20250724-121127</code>). Subsequently, inference is performed on all videos in the dataset using the pose estimators specified in the configuration file. For the MaskAnyoneUI pose estimators, the user is required to perform semi-automatic annotation of the videos using MaskAnyone before starting the MaskBench run. A poses folder is created within the checkpoint, containing a subfolder for each pose estimator and a single JSON file for each video. The application then evaluates all specified metrics and generates plots, which are stored in the plots folder within the checkpoint. Finally, for each video, the application produces a set of rendered videos—one for each pose estimator—which are stored in the renderings folder in the checkpoint.</p>
<p>Each component of MaskBench is implemented in a modular way, so it can be easily extended and modified. We will discuss this in the following sections.</p>
<section id="sec-architecture-dataset" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="sec-architecture-dataset"><span class="header-section-number">4.1</span> Dataset</h2>
<p>The dataset provides video data for pose estimation and, if available, ground truth data for evaluation. When adding a new dataset, the user must create a new class that inherits from the Dataset class and override the <code>_load_samples</code> method, which generates one VideoSample object for each video in the dataset. If the dataset provides ground truth data, the user must also override the <code>get_gt_pose_results</code> and <code>get_gt_keypoint_pairs</code> methods. For each video, the <code>get_gt_pose_results</code> method should return a <code>VideoPoseResult</code> object. The <code>get_gt_keypoint_pairs</code> method is used to render the ground truth keypoints and contains a list of tuples, each specifying the indices of two keypoints to be connected in the rendered video. Default keypoint pairs for YoloPose, MediaPipePose, and various implementations of OpenPose models are provided in the <code>keypoint_pairs.py</code> file.</p>
<p>Below is the code implementation for the abstract dataset class, the simple TED Talks dataset (without ground truth), and the more complex TragicTalkers dataset (with pseudo-ground truth data).</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Dataset Class
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>src/datasets/dataset.py</strong></pre>
</div>
<div class="sourceCode" id="cb10" data-filename="src/datasets/dataset.py"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="im">import</span> math</span>
<span id="cb10-2"><a href="#cb10-2"></a><span class="im">from</span> abc <span class="im">import</span> ABC, abstractmethod</span>
<span id="cb10-3"><a href="#cb10-3"></a><span class="im">from</span> typing <span class="im">import</span> Dict, List</span>
<span id="cb10-4"><a href="#cb10-4"></a></span>
<span id="cb10-5"><a href="#cb10-5"></a><span class="im">from</span> .video_sample <span class="im">import</span> VideoSample</span>
<span id="cb10-6"><a href="#cb10-6"></a><span class="im">from</span> inference <span class="im">import</span> VideoPoseResult</span>
<span id="cb10-7"><a href="#cb10-7"></a></span>
<span id="cb10-8"><a href="#cb10-8"></a></span>
<span id="cb10-9"><a href="#cb10-9"></a><span class="kw">class</span> Dataset(ABC):</span>
<span id="cb10-10"><a href="#cb10-10"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, name: <span class="bu">str</span>, dataset_folder: <span class="bu">str</span>, config: <span class="bu">dict</span> <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb10-11"><a href="#cb10-11"></a>        <span class="va">self</span>.name <span class="op">=</span> name</span>
<span id="cb10-12"><a href="#cb10-12"></a>        <span class="va">self</span>.dataset_folder <span class="op">=</span> dataset_folder</span>
<span id="cb10-13"><a href="#cb10-13"></a>        <span class="va">self</span>.config <span class="op">=</span> config</span>
<span id="cb10-14"><a href="#cb10-14"></a>        <span class="va">self</span>.samples <span class="op">=</span> <span class="va">self</span>._load_samples()</span>
<span id="cb10-15"><a href="#cb10-15"></a></span>
<span id="cb10-16"><a href="#cb10-16"></a>    <span class="at">@abstractmethod</span></span>
<span id="cb10-17"><a href="#cb10-17"></a>    <span class="kw">def</span> _load_samples(<span class="va">self</span>) <span class="op">-&gt;</span> List[VideoSample]:</span>
<span id="cb10-18"><a href="#cb10-18"></a>        <span class="co">"""</span></span>
<span id="cb10-19"><a href="#cb10-19"></a><span class="co">        Load video samples from the dataset folder.</span></span>
<span id="cb10-20"><a href="#cb10-20"></a><span class="co">        This method should be overridden by subclasses to implement specific loading logic.</span></span>
<span id="cb10-21"><a href="#cb10-21"></a><span class="co">        It should return one VideoSample for each video in the dataset.</span></span>
<span id="cb10-22"><a href="#cb10-22"></a><span class="co">        """</span></span>
<span id="cb10-23"><a href="#cb10-23"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span>()</span>
<span id="cb10-24"><a href="#cb10-24"></a></span>
<span id="cb10-25"><a href="#cb10-25"></a>    <span class="kw">def</span> get_gt_pose_results(<span class="va">self</span>) <span class="op">-&gt;</span> Dict[<span class="bu">str</span>, VideoPoseResult]:</span>
<span id="cb10-26"><a href="#cb10-26"></a>        <span class="co">"""</span></span>
<span id="cb10-27"><a href="#cb10-27"></a><span class="co">        Load ground truth pose results from the dataset folder.</span></span>
<span id="cb10-28"><a href="#cb10-28"></a><span class="co">        This method should be overridden by subclasses if ground truth data is available.</span></span>
<span id="cb10-29"><a href="#cb10-29"></a><span class="co">        The returned dictionary should map video names to ground truth`VideoPoseResult` objects.</span></span>
<span id="cb10-30"><a href="#cb10-30"></a><span class="co">        """</span></span>
<span id="cb10-31"><a href="#cb10-31"></a>        <span class="cf">return</span> {}</span>
<span id="cb10-32"><a href="#cb10-32"></a></span>
<span id="cb10-33"><a href="#cb10-33"></a>    <span class="kw">def</span> get_gt_keypoint_pairs(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="va">None</span> <span class="op">|</span> List[<span class="bu">tuple</span>]:</span>
<span id="cb10-34"><a href="#cb10-34"></a>        <span class="co">"""</span></span>
<span id="cb10-35"><a href="#cb10-35"></a><span class="co">        Load ground truth keypoint pairs from the dataset folder.</span></span>
<span id="cb10-36"><a href="#cb10-36"></a><span class="co">        This method should be overridden by subclasses if ground truth data is available.</span></span>
<span id="cb10-37"><a href="#cb10-37"></a><span class="co">        """</span></span>
<span id="cb10-38"><a href="#cb10-38"></a>        <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb10-39"><a href="#cb10-39"></a></span>
<span id="cb10-40"><a href="#cb10-40"></a>    <span class="kw">def</span> <span class="fu">__iter__</span>(<span class="va">self</span>):</span>
<span id="cb10-41"><a href="#cb10-41"></a>        <span class="cf">return</span> <span class="bu">iter</span>(<span class="va">self</span>.samples)</span>
<span id="cb10-42"><a href="#cb10-42"></a></span>
<span id="cb10-43"><a href="#cb10-43"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb10-44"><a href="#cb10-44"></a>        <span class="cf">return</span> math.ceil(<span class="bu">len</span>(<span class="va">self</span>.samples))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
TED Talks Dataset
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>src/datasets/ted_dataset.py</strong></pre>
</div>
<div class="sourceCode" id="cb11" data-filename="src/datasets/ted_dataset.py"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="im">import</span> os</span>
<span id="cb11-2"><a href="#cb11-2"></a><span class="im">from</span> typing <span class="im">import</span> List</span>
<span id="cb11-3"><a href="#cb11-3"></a></span>
<span id="cb11-4"><a href="#cb11-4"></a><span class="im">from</span> .dataset <span class="im">import</span> Dataset</span>
<span id="cb11-5"><a href="#cb11-5"></a><span class="im">from</span> .video_sample <span class="im">import</span> VideoSample</span>
<span id="cb11-6"><a href="#cb11-6"></a></span>
<span id="cb11-7"><a href="#cb11-7"></a></span>
<span id="cb11-8"><a href="#cb11-8"></a><span class="kw">class</span> TedDataset(Dataset):</span>
<span id="cb11-9"><a href="#cb11-9"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, name: <span class="bu">str</span>, dataset_folder: <span class="bu">str</span>, config: <span class="bu">dict</span> <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb11-10"><a href="#cb11-10"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(name, dataset_folder, config)</span>
<span id="cb11-11"><a href="#cb11-11"></a>        <span class="va">self</span>.samples <span class="op">=</span> <span class="va">self</span>._load_samples()</span>
<span id="cb11-12"><a href="#cb11-12"></a></span>
<span id="cb11-13"><a href="#cb11-13"></a>    <span class="kw">def</span> _load_samples(<span class="va">self</span>) <span class="op">-&gt;</span> List[VideoSample]:</span>
<span id="cb11-14"><a href="#cb11-14"></a>        video_extensions <span class="op">=</span> (<span class="st">".avi"</span>, <span class="st">".mp4"</span>)</span>
<span id="cb11-15"><a href="#cb11-15"></a>        samples <span class="op">=</span> []</span>
<span id="cb11-16"><a href="#cb11-16"></a></span>
<span id="cb11-17"><a href="#cb11-17"></a>        <span class="cf">for</span> filename <span class="kw">in</span> os.listdir(<span class="va">self</span>.dataset_folder):</span>
<span id="cb11-18"><a href="#cb11-18"></a>            video_path <span class="op">=</span> os.path.join(<span class="va">self</span>.dataset_folder, filename)</span>
<span id="cb11-19"><a href="#cb11-19"></a>            <span class="cf">if</span> filename.endswith(video_extensions):</span>
<span id="cb11-20"><a href="#cb11-20"></a>                samples.append(VideoSample(video_path))</span>
<span id="cb11-21"><a href="#cb11-21"></a></span>
<span id="cb11-22"><a href="#cb11-22"></a>        <span class="cf">return</span> samples</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tragic Talkers Dataset
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>src/datasets/tragic_talkers_dataset.py</strong></pre>
</div>
<div class="sourceCode" id="cb12" data-filename="src/datasets/tragic_talkers_dataset.py"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="im">import</span> os</span>
<span id="cb12-2"><a href="#cb12-2"></a><span class="im">import</span> json</span>
<span id="cb12-3"><a href="#cb12-3"></a><span class="im">import</span> glob</span>
<span id="cb12-4"><a href="#cb12-4"></a><span class="im">from</span> typing <span class="im">import</span> Dict, List</span>
<span id="cb12-5"><a href="#cb12-5"></a><span class="im">from</span> dataclasses <span class="im">import</span> asdict</span>
<span id="cb12-6"><a href="#cb12-6"></a></span>
<span id="cb12-7"><a href="#cb12-7"></a><span class="im">from</span> inference <span class="im">import</span> FramePoseResult, PersonPoseResult, PoseKeypoint, VideoPoseResult</span>
<span id="cb12-8"><a href="#cb12-8"></a><span class="im">from</span> keypoint_pairs <span class="im">import</span> COCO_KEYPOINT_PAIRS, COCO_TO_OPENPOSE_BODY25, OPENPOSE_BODY25_KEYPOINT_PAIRS</span>
<span id="cb12-9"><a href="#cb12-9"></a><span class="im">from</span> utils <span class="im">import</span> convert_keypoints_to_coco_format</span>
<span id="cb12-10"><a href="#cb12-10"></a><span class="im">from</span> .dataset <span class="im">import</span> Dataset</span>
<span id="cb12-11"><a href="#cb12-11"></a><span class="im">from</span> .video_sample <span class="im">import</span> VideoSample</span>
<span id="cb12-12"><a href="#cb12-12"></a></span>
<span id="cb12-13"><a href="#cb12-13"></a><span class="kw">class</span> TragicTalkersDataset(Dataset):</span>
<span id="cb12-14"><a href="#cb12-14"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, name: <span class="bu">str</span>, dataset_folder: <span class="bu">str</span>, config: <span class="bu">dict</span> <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb12-15"><a href="#cb12-15"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(name, dataset_folder, config)</span>
<span id="cb12-16"><a href="#cb12-16"></a>        <span class="va">self</span>.convert_gt_keypoints_to_coco <span class="op">=</span> config.get(<span class="st">"convert_gt_keypoints_to_coco"</span>, <span class="va">False</span>)</span>
<span id="cb12-17"><a href="#cb12-17"></a>    </span>
<span id="cb12-18"><a href="#cb12-18"></a>    <span class="kw">def</span> _load_samples(<span class="va">self</span>) <span class="op">-&gt;</span> List[VideoSample]:</span>
<span id="cb12-19"><a href="#cb12-19"></a>        <span class="va">self</span>.video_folder <span class="op">=</span> os.path.join(<span class="va">self</span>.dataset_folder, <span class="va">self</span>.config.get(<span class="st">"video_folder"</span>)) <span class="co"># adjust according to folder structure</span></span>
<span id="cb12-20"><a href="#cb12-20"></a>        <span class="va">self</span>.gt_folder <span class="op">=</span> os.path.join(<span class="va">self</span>.dataset_folder, <span class="va">self</span>.config.get(<span class="st">"ground_truth_folder"</span>)) <span class="co"># adjust according to folder structure</span></span>
<span id="cb12-21"><a href="#cb12-21"></a></span>
<span id="cb12-22"><a href="#cb12-22"></a>        samples <span class="op">=</span> []</span>
<span id="cb12-23"><a href="#cb12-23"></a>        video_extensions <span class="op">=</span> (<span class="st">".avi"</span>, <span class="st">".mp4"</span>)</span>
<span id="cb12-24"><a href="#cb12-24"></a>        list_of_videos <span class="op">=</span> glob.glob(os.path.join(<span class="va">self</span>.video_folder, <span class="st">"*"</span>, <span class="st">"*"</span>))</span>
<span id="cb12-25"><a href="#cb12-25"></a></span>
<span id="cb12-26"><a href="#cb12-26"></a>        <span class="cf">for</span> video <span class="kw">in</span> list_of_videos:</span>
<span id="cb12-27"><a href="#cb12-27"></a>            <span class="cf">if</span> video.endswith(video_extensions):</span>
<span id="cb12-28"><a href="#cb12-28"></a>                samples.append(VideoSample(video))</span>
<span id="cb12-29"><a href="#cb12-29"></a>      </span>
<span id="cb12-30"><a href="#cb12-30"></a>        <span class="cf">return</span> samples</span>
<span id="cb12-31"><a href="#cb12-31"></a></span>
<span id="cb12-32"><a href="#cb12-32"></a>    <span class="kw">def</span> get_gt_keypoint_pairs(<span class="va">self</span>) <span class="op">-&gt;</span> List[<span class="bu">tuple</span>]:</span>
<span id="cb12-33"><a href="#cb12-33"></a>        <span class="cf">if</span> <span class="va">self</span>.convert_gt_keypoints_to_coco:</span>
<span id="cb12-34"><a href="#cb12-34"></a>            <span class="cf">return</span> COCO_KEYPOINT_PAIRS</span>
<span id="cb12-35"><a href="#cb12-35"></a>        <span class="cf">else</span>:</span>
<span id="cb12-36"><a href="#cb12-36"></a>            <span class="co"># Tragic Talkers uses the BODY_25 model</span></span>
<span id="cb12-37"><a href="#cb12-37"></a>            <span class="cf">return</span> OPENPOSE_BODY25_KEYPOINT_PAIRS</span>
<span id="cb12-38"><a href="#cb12-38"></a></span>
<span id="cb12-39"><a href="#cb12-39"></a>    <span class="kw">def</span> get_gt_pose_results(<span class="va">self</span>) <span class="op">-&gt;</span> Dict[<span class="bu">str</span>, VideoPoseResult]:</span>
<span id="cb12-40"><a href="#cb12-40"></a>        gt_pose_results <span class="op">=</span> {}</span>
<span id="cb12-41"><a href="#cb12-41"></a>        video_json_folders <span class="op">=</span> glob.glob(os.path.join(<span class="va">self</span>.gt_folder, <span class="st">"*"</span>, <span class="st">"*"</span>)) <span class="co"># for every video &amp; camera angle</span></span>
<span id="cb12-42"><a href="#cb12-42"></a>        <span class="cf">for</span> video_json_folder <span class="kw">in</span> video_json_folders:</span>
<span id="cb12-43"><a href="#cb12-43"></a>            video_name <span class="op">=</span> <span class="va">self</span>._extract_video_name_from_labels_folder(video_json_folder)</span>
<span id="cb12-44"><a href="#cb12-44"></a>            gt_pose_result <span class="op">=</span> <span class="va">self</span>.combine_json_files_for_video(video_json_folder, video_name)</span>
<span id="cb12-45"><a href="#cb12-45"></a>            <span class="cf">if</span> <span class="va">self</span>.convert_gt_keypoints_to_coco:</span>
<span id="cb12-46"><a href="#cb12-46"></a>                gt_pose_result.frames <span class="op">=</span> convert_keypoints_to_coco_format(gt_pose_result.frames, COCO_TO_OPENPOSE_BODY25)</span>
<span id="cb12-47"><a href="#cb12-47"></a>            gt_pose_results[video_name] <span class="op">=</span> gt_pose_result</span>
<span id="cb12-48"><a href="#cb12-48"></a>        <span class="cf">return</span> gt_pose_results</span>
<span id="cb12-49"><a href="#cb12-49"></a></span>
<span id="cb12-50"><a href="#cb12-50"></a>    <span class="kw">def</span> combine_json_files_for_video(<span class="va">self</span>, video_json_folder: <span class="bu">str</span>, video_name: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb12-51"><a href="#cb12-51"></a>        all_json_files <span class="op">=</span> glob.glob(os.path.join(video_json_folder, <span class="st">"*"</span>))</span>
<span id="cb12-52"><a href="#cb12-52"></a>        all_json_files <span class="op">=</span> <span class="bu">sorted</span>(all_json_files, key<span class="op">=</span><span class="kw">lambda</span> x: <span class="bu">int</span>(os.path.basename(x).split(<span class="st">'-'</span>)[<span class="dv">1</span>].split(<span class="st">'_'</span>)[<span class="dv">0</span>])) <span class="co"># we need to sort them by frame number</span></span>
<span id="cb12-53"><a href="#cb12-53"></a></span>
<span id="cb12-54"><a href="#cb12-54"></a>        all_frames_keypoints <span class="op">=</span> []</span>
<span id="cb12-55"><a href="#cb12-55"></a>        <span class="cf">for</span> frame_idx, <span class="bu">file</span> <span class="kw">in</span> <span class="bu">enumerate</span>(all_json_files): <span class="co"># for every frame</span></span>
<span id="cb12-56"><a href="#cb12-56"></a>            frame_keypoints <span class="op">=</span> [] </span>
<span id="cb12-57"><a href="#cb12-57"></a>            <span class="cf">with</span> <span class="bu">open</span>(<span class="bu">file</span>, <span class="st">'r'</span>) <span class="im">as</span> f:</span>
<span id="cb12-58"><a href="#cb12-58"></a>                data <span class="op">=</span> json.load(f)</span>
<span id="cb12-59"><a href="#cb12-59"></a>                people <span class="op">=</span> data.get(<span class="st">'people'</span>, [])</span>
<span id="cb12-60"><a href="#cb12-60"></a>                <span class="cf">if</span> people:</span>
<span id="cb12-61"><a href="#cb12-61"></a>                    <span class="cf">for</span> person <span class="kw">in</span> people: <span class="co"># for every person in the frame</span></span>
<span id="cb12-62"><a href="#cb12-62"></a>                        person_keypoints <span class="op">=</span> []</span>
<span id="cb12-63"><a href="#cb12-63"></a>                        pose_keypoints <span class="op">=</span> person.get(<span class="st">'pose_keypoints_2d'</span>, [])</span>
<span id="cb12-64"><a href="#cb12-64"></a>                        <span class="cf">if</span> pose_keypoints: <span class="co"># get keypoints for that person</span></span>
<span id="cb12-65"><a href="#cb12-65"></a>                            person_keypoints <span class="op">=</span> [</span>
<span id="cb12-66"><a href="#cb12-66"></a>                                PoseKeypoint(x<span class="op">=</span>pose_keypoints[i], y<span class="op">=</span>pose_keypoints[i<span class="op">+</span><span class="dv">1</span>], confidence<span class="op">=</span>pose_keypoints[i<span class="op">+</span><span class="dv">2</span>])</span>
<span id="cb12-67"><a href="#cb12-67"></a>                                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(pose_keypoints), <span class="dv">3</span>)]</span>
<span id="cb12-68"><a href="#cb12-68"></a>                        frame_keypoints.append(PersonPoseResult(keypoints<span class="op">=</span>person_keypoints))</span>
<span id="cb12-69"><a href="#cb12-69"></a>            all_frames_keypoints.append(FramePoseResult(persons<span class="op">=</span>frame_keypoints, frame_idx<span class="op">=</span>frame_idx))</span>
<span id="cb12-70"><a href="#cb12-70"></a></span>
<span id="cb12-71"><a href="#cb12-71"></a>        <span class="cf">return</span> VideoPoseResult(</span>
<span id="cb12-72"><a href="#cb12-72"></a>            fps<span class="op">=</span><span class="dv">30</span>,</span>
<span id="cb12-73"><a href="#cb12-73"></a>            frame_width<span class="op">=</span><span class="dv">2448</span>,</span>
<span id="cb12-74"><a href="#cb12-74"></a>            frame_height<span class="op">=</span><span class="dv">2048</span>,</span>
<span id="cb12-75"><a href="#cb12-75"></a>            video_name<span class="op">=</span>video_name,</span>
<span id="cb12-76"><a href="#cb12-76"></a>            frames<span class="op">=</span>all_frames_keypoints</span>
<span id="cb12-77"><a href="#cb12-77"></a>        )</span>
<span id="cb12-78"><a href="#cb12-78"></a></span>
<span id="cb12-79"><a href="#cb12-79"></a>    <span class="kw">def</span> _extract_video_name_from_labels_folder(<span class="va">self</span>, path: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb12-80"><a href="#cb12-80"></a>        <span class="co">"""</span></span>
<span id="cb12-81"><a href="#cb12-81"></a><span class="co">        Extract video name from a labels folder.</span></span>
<span id="cb12-82"><a href="#cb12-82"></a><span class="co">        For example, a path like /datasets/tragic_talkers/labels/conversation1_t3/cam-022 will be converted to conversation1_t3-cam22.</span></span>
<span id="cb12-83"><a href="#cb12-83"></a><span class="co">        """</span></span>
<span id="cb12-84"><a href="#cb12-84"></a>        parts <span class="op">=</span> path.split(os.sep)</span>
<span id="cb12-85"><a href="#cb12-85"></a>        conversation <span class="op">=</span> parts[<span class="op">-</span><span class="dv">2</span>]  <span class="co"># e.g. conversation1_t3</span></span>
<span id="cb12-86"><a href="#cb12-86"></a>        camera <span class="op">=</span> parts[<span class="op">-</span><span class="dv">1</span>]  <span class="co"># e.g. cam-022</span></span>
<span id="cb12-87"><a href="#cb12-87"></a>        </span>
<span id="cb12-88"><a href="#cb12-88"></a>        <span class="co"># Extract camera number and format it</span></span>
<span id="cb12-89"><a href="#cb12-89"></a>        cam_number <span class="op">=</span> camera.split(<span class="st">'-'</span>)[<span class="dv">1</span>]  <span class="co"># e.g. 022</span></span>
<span id="cb12-90"><a href="#cb12-90"></a>        cam_number <span class="op">=</span> cam_number[<span class="dv">1</span>:] <span class="co"># cam number in labels has 3 digits, we need to remove the leading one</span></span>
<span id="cb12-91"><a href="#cb12-91"></a>        </span>
<span id="cb12-92"><a href="#cb12-92"></a>        <span class="cf">return</span> <span class="ss">f"</span><span class="sc">{</span>conversation<span class="sc">}</span><span class="ss">-cam</span><span class="sc">{</span>cam_number<span class="sc">}</span><span class="ss">"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="sec-architecture-inference" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="sec-architecture-inference"><span class="header-section-number">4.2</span> Inference</h2>
<section id="sec-architecture-video-pose-result" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="sec-architecture-video-pose-result"><span class="header-section-number">4.2.1</span> Video Pose Result</h3>
<p>The VideoPoseResult object represents the standardized output of a pose prediction model. It is a nested structure containing a <code>FramePoseResult</code> object for each frame in the video. Each frame pose result includes a list of <code>PersonPoseResult</code> objects, one for each person detected in the frame. Every person’s result contains a list of <code>PoseKeypoint</code> objects, one for each keypoint in the model’s output format, providing x and y coordinates along with an optional confidence score.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Video Pose Result Class
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>src/inference/pose_result.py</strong></pre>
</div>
<div class="sourceCode" id="cb13" data-filename="src/inference/pose_result.py"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="im">from</span> dataclasses <span class="im">import</span> asdict, dataclass</span>
<span id="cb13-2"><a href="#cb13-2"></a><span class="im">from</span> typing <span class="im">import</span> List, Optional</span>
<span id="cb13-3"><a href="#cb13-3"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-4"><a href="#cb13-4"></a><span class="im">import</span> numpy.ma <span class="im">as</span> ma</span>
<span id="cb13-5"><a href="#cb13-5"></a></span>
<span id="cb13-6"><a href="#cb13-6"></a>np.set_printoptions(threshold<span class="op">=</span>np.inf)</span>
<span id="cb13-7"><a href="#cb13-7"></a></span>
<span id="cb13-8"><a href="#cb13-8"></a></span>
<span id="cb13-9"><a href="#cb13-9"></a><span class="at">@dataclass</span></span>
<span id="cb13-10"><a href="#cb13-10"></a><span class="kw">class</span> PoseKeypoint:</span>
<span id="cb13-11"><a href="#cb13-11"></a>    x: <span class="bu">float</span></span>
<span id="cb13-12"><a href="#cb13-12"></a>    y: <span class="bu">float</span></span>
<span id="cb13-13"><a href="#cb13-13"></a>    confidence: Optional[<span class="bu">float</span>] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb13-14"><a href="#cb13-14"></a></span>
<span id="cb13-15"><a href="#cb13-15"></a></span>
<span id="cb13-16"><a href="#cb13-16"></a><span class="at">@dataclass</span></span>
<span id="cb13-17"><a href="#cb13-17"></a><span class="kw">class</span> PersonPoseResult:</span>
<span id="cb13-18"><a href="#cb13-18"></a>    keypoints: List[PoseKeypoint]  <span class="co"># Fixed length per pose estimator (e.g., 17 for COCO)</span></span>
<span id="cb13-19"><a href="#cb13-19"></a>    <span class="bu">id</span>: Optional[<span class="bu">int</span>] <span class="op">=</span> <span class="va">None</span>  <span class="co"># for tracking across frames</span></span>
<span id="cb13-20"><a href="#cb13-20"></a></span>
<span id="cb13-21"><a href="#cb13-21"></a></span>
<span id="cb13-22"><a href="#cb13-22"></a><span class="at">@dataclass</span></span>
<span id="cb13-23"><a href="#cb13-23"></a><span class="kw">class</span> FramePoseResult:</span>
<span id="cb13-24"><a href="#cb13-24"></a>    persons: List[PersonPoseResult]</span>
<span id="cb13-25"><a href="#cb13-25"></a>    frame_idx: <span class="bu">int</span></span>
<span id="cb13-26"><a href="#cb13-26"></a></span>
<span id="cb13-27"><a href="#cb13-27"></a></span>
<span id="cb13-28"><a href="#cb13-28"></a><span class="kw">class</span> VideoPoseResult:</span>
<span id="cb13-29"><a href="#cb13-29"></a>    <span class="co">"""</span></span>
<span id="cb13-30"><a href="#cb13-30"></a><span class="co">    This class is the main output of the pose estimation models.</span></span>
<span id="cb13-31"><a href="#cb13-31"></a><span class="co">    It contains the pose estimation results for a video.</span></span>
<span id="cb13-32"><a href="#cb13-32"></a><span class="co">    It is a nested object that contains a `FramePoseResult`object for each frame in the video.</span></span>
<span id="cb13-33"><a href="#cb13-33"></a><span class="co">    Within each frame pose result, there is a list of `PersonPoseResult` objects, one for each person in the frame.</span></span>
<span id="cb13-34"><a href="#cb13-34"></a><span class="co">    Every `PersonPoseResult` contains a list of `PoseKeypoint` objects, one for each keypoint in the model output format, with the x, y coordinates and a confidence score.</span></span>
<span id="cb13-35"><a href="#cb13-35"></a><span class="co">    """</span></span>
<span id="cb13-36"><a href="#cb13-36"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb13-37"><a href="#cb13-37"></a>        <span class="va">self</span>,</span>
<span id="cb13-38"><a href="#cb13-38"></a>        fps: <span class="bu">int</span>,</span>
<span id="cb13-39"><a href="#cb13-39"></a>        frame_width: <span class="bu">int</span>,</span>
<span id="cb13-40"><a href="#cb13-40"></a>        frame_height: <span class="bu">int</span>,</span>
<span id="cb13-41"><a href="#cb13-41"></a>        frames: List[FramePoseResult],</span>
<span id="cb13-42"><a href="#cb13-42"></a>        video_name: <span class="bu">str</span>,</span>
<span id="cb13-43"><a href="#cb13-43"></a>    ):</span>
<span id="cb13-44"><a href="#cb13-44"></a>        <span class="va">self</span>.fps <span class="op">=</span> fps</span>
<span id="cb13-45"><a href="#cb13-45"></a>        <span class="va">self</span>.frame_width <span class="op">=</span> frame_width</span>
<span id="cb13-46"><a href="#cb13-46"></a>        <span class="va">self</span>.frame_height <span class="op">=</span> frame_height</span>
<span id="cb13-47"><a href="#cb13-47"></a>        <span class="va">self</span>.frames <span class="op">=</span> frames</span>
<span id="cb13-48"><a href="#cb13-48"></a>        <span class="va">self</span>.video_name <span class="op">=</span> video_name</span>
<span id="cb13-49"><a href="#cb13-49"></a></span>
<span id="cb13-50"><a href="#cb13-50"></a>    <span class="kw">def</span> __info__(<span class="va">self</span>, num_of_sample_frames: <span class="bu">int</span> <span class="op">=</span> <span class="dv">3</span>) <span class="op">-&gt;</span> <span class="bu">dict</span>:</span>
<span id="cb13-51"><a href="#cb13-51"></a>        <span class="cf">return</span> {</span>
<span id="cb13-52"><a href="#cb13-52"></a>            <span class="st">"video_name"</span>: <span class="va">self</span>.video_name,</span>
<span id="cb13-53"><a href="#cb13-53"></a>            <span class="st">"fps"</span>: <span class="va">self</span>.fps,</span>
<span id="cb13-54"><a href="#cb13-54"></a>            <span class="st">"frame_width"</span>: <span class="va">self</span>.frame_width,</span>
<span id="cb13-55"><a href="#cb13-55"></a>            <span class="st">"frame_height"</span>: <span class="va">self</span>.frame_height,</span>
<span id="cb13-56"><a href="#cb13-56"></a>            <span class="st">"num_frames"</span>: <span class="bu">len</span>(<span class="va">self</span>.frames),</span>
<span id="cb13-57"><a href="#cb13-57"></a>            <span class="st">"sample_frames"</span>: <span class="va">self</span>.frames[:num_of_sample_frames] <span class="cf">if</span> <span class="bu">len</span>(<span class="va">self</span>.frames) <span class="op">&gt;</span> num_of_sample_frames <span class="cf">else</span> <span class="va">self</span>.frames,</span>
<span id="cb13-58"><a href="#cb13-58"></a>        }</span>
<span id="cb13-59"><a href="#cb13-59"></a>    </span>
<span id="cb13-60"><a href="#cb13-60"></a>    <span class="kw">def</span> to_numpy_ma(<span class="va">self</span>) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb13-61"><a href="#cb13-61"></a>        <span class="co">"""</span></span>
<span id="cb13-62"><a href="#cb13-62"></a><span class="co">        Convert the video pose results from a nested object to a masked array.</span></span>
<span id="cb13-63"><a href="#cb13-63"></a><span class="co">        This method is useful for evaluation and plotting in order to work</span></span>
<span id="cb13-64"><a href="#cb13-64"></a><span class="co">        with arrays rather than nested objects.</span></span>
<span id="cb13-65"><a href="#cb13-65"></a><span class="co">        </span></span>
<span id="cb13-66"><a href="#cb13-66"></a><span class="co">        Returns:</span></span>
<span id="cb13-67"><a href="#cb13-67"></a><span class="co">            Masked array with shape (num_frames, max_persons, num_keypoints, 2)</span></span>
<span id="cb13-68"><a href="#cb13-68"></a><span class="co">            where 2 represents x and y coordinates. Max_persons is the maximum number</span></span>
<span id="cb13-69"><a href="#cb13-69"></a><span class="co">            of detected persons in the entire video. Values are masked for frames with </span></span>
<span id="cb13-70"><a href="#cb13-70"></a><span class="co">            fewer persons than max_persons, which means that these values are not included</span></span>
<span id="cb13-71"><a href="#cb13-71"></a><span class="co">            in computations (e.g. evaluation or plotting).</span></span>
<span id="cb13-72"><a href="#cb13-72"></a><span class="co">        """</span></span>
<span id="cb13-73"><a href="#cb13-73"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.frames:</span>
<span id="cb13-74"><a href="#cb13-74"></a>            <span class="bu">print</span>(<span class="st">"Warning: No frames in video pose result."</span>)</span>
<span id="cb13-75"><a href="#cb13-75"></a>            <span class="cf">return</span> ma.array(np.zeros((<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">2</span>)))</span>
<span id="cb13-76"><a href="#cb13-76"></a>            </span>
<span id="cb13-77"><a href="#cb13-77"></a>        <span class="co"># Get dimensions</span></span>
<span id="cb13-78"><a href="#cb13-78"></a>        num_frames <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.frames)</span>
<span id="cb13-79"><a href="#cb13-79"></a>        max_persons <span class="op">=</span> <span class="bu">max</span>(<span class="bu">len</span>(frame.persons) <span class="cf">for</span> frame <span class="kw">in</span> <span class="va">self</span>.frames)</span>
<span id="cb13-80"><a href="#cb13-80"></a>        num_keypoints <span class="op">=</span> <span class="bu">max</span>(</span>
<span id="cb13-81"><a href="#cb13-81"></a>            <span class="bu">len</span>(person.keypoints)</span>
<span id="cb13-82"><a href="#cb13-82"></a>            <span class="cf">for</span> frame <span class="kw">in</span> <span class="va">self</span>.frames</span>
<span id="cb13-83"><a href="#cb13-83"></a>            <span class="cf">for</span> person <span class="kw">in</span> frame.persons</span>
<span id="cb13-84"><a href="#cb13-84"></a>        ) <span class="cf">if</span> <span class="bu">any</span>(frame.persons <span class="cf">for</span> frame <span class="kw">in</span> <span class="va">self</span>.frames) <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb13-85"><a href="#cb13-85"></a>        </span>
<span id="cb13-86"><a href="#cb13-86"></a>        <span class="cf">if</span> max_persons <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> num_keypoints <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb13-87"><a href="#cb13-87"></a>            <span class="bu">print</span>(<span class="st">"Warning: No persons or keypoints found in video pose result."</span>)</span>
<span id="cb13-88"><a href="#cb13-88"></a>            <span class="cf">return</span> ma.array(np.zeros((num_frames, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">2</span>)))</span>
<span id="cb13-89"><a href="#cb13-89"></a>        </span>
<span id="cb13-90"><a href="#cb13-90"></a>        <span class="co"># Initialize arrays - all values masked by default</span></span>
<span id="cb13-91"><a href="#cb13-91"></a>        values <span class="op">=</span> np.zeros((num_frames, max_persons, num_keypoints, <span class="dv">2</span>))</span>
<span id="cb13-92"><a href="#cb13-92"></a>        mask <span class="op">=</span> np.ones_like(values, dtype<span class="op">=</span><span class="bu">bool</span>)  <span class="co"># True means masked</span></span>
<span id="cb13-93"><a href="#cb13-93"></a>        </span>
<span id="cb13-94"><a href="#cb13-94"></a>        <span class="cf">for</span> frame_idx, frame <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.frames):</span>
<span id="cb13-95"><a href="#cb13-95"></a>            <span class="co"># Only fill and unmask values for persons that exist</span></span>
<span id="cb13-96"><a href="#cb13-96"></a>            <span class="cf">for</span> person_idx, person <span class="kw">in</span> <span class="bu">enumerate</span>(frame.persons):</span>
<span id="cb13-97"><a href="#cb13-97"></a>                <span class="cf">for</span> kpt_idx, keypoint <span class="kw">in</span> <span class="bu">enumerate</span>(person.keypoints):</span>
<span id="cb13-98"><a href="#cb13-98"></a>                    values[frame_idx, person_idx, kpt_idx, <span class="dv">0</span>] <span class="op">=</span> keypoint.x</span>
<span id="cb13-99"><a href="#cb13-99"></a>                    values[frame_idx, person_idx, kpt_idx, <span class="dv">1</span>] <span class="op">=</span> keypoint.y</span>
<span id="cb13-100"><a href="#cb13-100"></a>                    mask[frame_idx, person_idx, kpt_idx] <span class="op">=</span> <span class="va">False</span>  <span class="co"># Unmask only existing values</span></span>
<span id="cb13-101"><a href="#cb13-101"></a>        </span>
<span id="cb13-102"><a href="#cb13-102"></a>        <span class="cf">return</span> ma.array(values, mask<span class="op">=</span>mask)</span>
<span id="cb13-103"><a href="#cb13-103"></a></span>
<span id="cb13-104"><a href="#cb13-104"></a>    <span class="kw">def</span> to_json(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="bu">dict</span>:</span>
<span id="cb13-105"><a href="#cb13-105"></a>        <span class="cf">return</span> {</span>
<span id="cb13-106"><a href="#cb13-106"></a>            <span class="st">"fps"</span>: <span class="va">self</span>.fps,</span>
<span id="cb13-107"><a href="#cb13-107"></a>            <span class="st">"frame_width"</span>: <span class="va">self</span>.frame_width,</span>
<span id="cb13-108"><a href="#cb13-108"></a>            <span class="st">"frame_height"</span>: <span class="va">self</span>.frame_height,</span>
<span id="cb13-109"><a href="#cb13-109"></a>            <span class="st">"frames"</span>: [asdict(frame) <span class="cf">for</span> frame <span class="kw">in</span> <span class="va">self</span>.frames],</span>
<span id="cb13-110"><a href="#cb13-110"></a>            <span class="st">"video_name"</span>: <span class="va">self</span>.video_name,</span>
<span id="cb13-111"><a href="#cb13-111"></a>        }</span>
<span id="cb13-112"><a href="#cb13-112"></a></span>
<span id="cb13-113"><a href="#cb13-113"></a>    <span class="kw">def</span> <span class="fu">__str__</span>(<span class="va">self</span>):</span>
<span id="cb13-114"><a href="#cb13-114"></a>        array <span class="op">=</span> <span class="va">self</span>.to_numpy_ma()</span>
<span id="cb13-115"><a href="#cb13-115"></a>        <span class="cf">return</span> <span class="ss">f"VideoPoseResult(fps=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>fps<span class="sc">}</span><span class="ss">, frame_width=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>frame_width<span class="sc">}</span><span class="ss">, frame_height=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>frame_height<span class="sc">}</span><span class="ss">, video_name=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>video_name<span class="sc">}</span><span class="ss">), frame_values: </span><span class="ch">\n</span><span class="sc">{</span>array<span class="sc">}</span><span class="ss">"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section>
<section id="sec-architecture-pose-estimators" class="level3" data-number="4.2.2">
<h3 data-number="4.2.2" class="anchored" data-anchor-id="sec-architecture-pose-estimators"><span class="header-section-number">4.2.2</span> Pose Estimator</h3>
<p>Pose estimators are responsible for predicting the poses of persons in a video by wrapping calls to specific AI models or pose estimation pipelines. Each model is implemented in a separate class that inherits from the abstract <code>PoseEstimator</code> class. The output of each estimator is a standardized <code>VideoPoseResult</code> object.</p>
<p>To add a new pose estimator, users must implement methods for pose estimation and for retrieving keypoint pairs. Special care must be taken to ensure that the output meets the following constraints:</p>
<ol type="1">
<li>The number of frames in the pose results matches the number of frames in the video.</li>
<li>If no persons are detected in a frame, the persons list should be empty.</li>
<li>For detected persons with missing keypoints, those keypoints should have values <code>x=0, y=0, confidence=None</code>.</li>
<li>The number of keypoints per person remains constant across all frames.</li>
<li>Keypoints with low confidence should be masked out using the <code>confidence_threshold</code> configuration parameter.</li>
<li>Keypoints must be mapped to the COCO format if the <code>save_keypoints_in_coco_format</code> configuration parameter is set to true.</li>
</ol>
<p>As an example, we provide the implementation of the abstract pose estimator class and the implementation of the YOLO model.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Pose Estimator Class
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>src/models/pose_estimator.py</strong></pre>
</div>
<div class="sourceCode" id="cb14" data-filename="src/models/pose_estimator.py"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a><span class="im">from</span> abc <span class="im">import</span> ABC, abstractmethod</span>
<span id="cb14-2"><a href="#cb14-2"></a><span class="im">import</span> cv2</span>
<span id="cb14-3"><a href="#cb14-3"></a></span>
<span id="cb14-4"><a href="#cb14-4"></a><span class="im">from</span> inference.pose_result <span class="im">import</span> VideoPoseResult</span>
<span id="cb14-5"><a href="#cb14-5"></a></span>
<span id="cb14-6"><a href="#cb14-6"></a></span>
<span id="cb14-7"><a href="#cb14-7"></a><span class="kw">class</span> PoseEstimator(ABC):</span>
<span id="cb14-8"><a href="#cb14-8"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, name: <span class="bu">str</span>, config: <span class="bu">dict</span>):</span>
<span id="cb14-9"><a href="#cb14-9"></a>        <span class="co">"""</span></span>
<span id="cb14-10"><a href="#cb14-10"></a><span class="co">        Initialize the PoseEstimator with a name and configuration.</span></span>
<span id="cb14-11"><a href="#cb14-11"></a><span class="co">        </span></span>
<span id="cb14-12"><a href="#cb14-12"></a><span class="co">        Args:</span></span>
<span id="cb14-13"><a href="#cb14-13"></a><span class="co">            name (str): The name of the estimator (e.g. "YoloPose", "MediaPipe", "OpenPose", ...).</span></span>
<span id="cb14-14"><a href="#cb14-14"></a><span class="co">            config (dict): Configuration dictionary for the pose estimator. This can include arbitrary parameters for the model that are necessary for inference (e.g. "confidence_threshold", "weights_file_name", ...). The config parameter "confidence_threshold" is required. This has no effect for MaskAnyonePoseEstimators, because they do not provide confidence scores. If you do not want to filter, set confidence_threshold to 0.</span></span>
<span id="cb14-15"><a href="#cb14-15"></a><span class="co">        """</span></span>
<span id="cb14-16"><a href="#cb14-16"></a>        <span class="cf">if</span> <span class="kw">not</span> config <span class="kw">or</span> <span class="st">"confidence_threshold"</span> <span class="kw">not</span> <span class="kw">in</span> config:</span>
<span id="cb14-17"><a href="#cb14-17"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"Config for </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss"> must include a 'confidence_threshold' key."</span>)</span>
<span id="cb14-18"><a href="#cb14-18"></a></span>
<span id="cb14-19"><a href="#cb14-19"></a>        <span class="va">self</span>.name <span class="op">=</span> name</span>
<span id="cb14-20"><a href="#cb14-20"></a>        <span class="va">self</span>.config <span class="op">=</span> config</span>
<span id="cb14-21"><a href="#cb14-21"></a>        <span class="va">self</span>.confidence_threshold <span class="op">=</span> config[<span class="st">"confidence_threshold"</span>]</span>
<span id="cb14-22"><a href="#cb14-22"></a></span>
<span id="cb14-23"><a href="#cb14-23"></a>    <span class="at">@abstractmethod</span></span>
<span id="cb14-24"><a href="#cb14-24"></a>    <span class="kw">def</span> estimate_pose(<span class="va">self</span>, video_path: <span class="bu">str</span>) <span class="op">-&gt;</span> VideoPoseResult:</span>
<span id="cb14-25"><a href="#cb14-25"></a>        <span class="co">"""</span></span>
<span id="cb14-26"><a href="#cb14-26"></a><span class="co">        Abstract method to estimate the pose of a video using the specific pose estimation model.</span></span>
<span id="cb14-27"><a href="#cb14-27"></a><span class="co">        This method should be implemented by subclasses.</span></span>
<span id="cb14-28"><a href="#cb14-28"></a><span class="co">        It receives the full path to the input video file and returns a VideoPoseResult object.</span></span>
<span id="cb14-29"><a href="#cb14-29"></a><span class="co">        The user is responsible for the following three steps after creating the initial VideoPoseResult object:</span></span>
<span id="cb14-30"><a href="#cb14-30"></a><span class="co">        1. Assert that the number of frames in the frame results matches the number of frames in the video (call assert_frame_count_is_correct)</span></span>
<span id="cb14-31"><a href="#cb14-31"></a><span class="co">        2. Filter out low confidence keypoints (call filter_low_confidence_keypoints)</span></span>
<span id="cb14-32"><a href="#cb14-32"></a><span class="co">        3. If the config contains a "save_keypoints_in_coco_format" key, convert the keypoints to the COCO format (call utils.convert_keypoints_to_coco_format, providing a mapping from the model output format to the COCO format)</span></span>
<span id="cb14-33"><a href="#cb14-33"></a></span>
<span id="cb14-34"><a href="#cb14-34"></a><span class="co">        Args:</span></span>
<span id="cb14-35"><a href="#cb14-35"></a><span class="co">            video_path (str): The full path to the input video file.</span></span>
<span id="cb14-36"><a href="#cb14-36"></a><span class="co">        Returns:</span></span>
<span id="cb14-37"><a href="#cb14-37"></a><span class="co">            VideoPoseResult: An object containing the pose estimation results for the video.</span></span>
<span id="cb14-38"><a href="#cb14-38"></a><span class="co">                  The VideoPoseResult object must contain as many FramePoseResult objects as there are frames in the video (asserted by assert_frame_count_is_correct).</span></span>
<span id="cb14-39"><a href="#cb14-39"></a><span class="co">                  The FramePoseResult contains a list of PersonPoseResult objects, one for each person in the frame. If there are no persons in the frame, the list is empty (persons=[]).</span></span>
<span id="cb14-40"><a href="#cb14-40"></a><span class="co">                  Every PersonPoseResult contains a list of PoseKeypoints, one for each keypoint in the model output format. If a keypoint is not detected, the PoseKeypoint object should have x=0 and y=0 and confidence=None.</span></span>
<span id="cb14-41"><a href="#cb14-41"></a><span class="co">        """</span></span>
<span id="cb14-42"><a href="#cb14-42"></a>        <span class="cf">pass</span></span>
<span id="cb14-43"><a href="#cb14-43"></a></span>
<span id="cb14-44"><a href="#cb14-44"></a>    <span class="at">@abstractmethod</span></span>
<span id="cb14-45"><a href="#cb14-45"></a>    <span class="kw">def</span> get_keypoint_pairs(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="bu">list</span>:</span>
<span id="cb14-46"><a href="#cb14-46"></a>        <span class="co">"""</span></span>
<span id="cb14-47"><a href="#cb14-47"></a><span class="co">        Abstract method to get the pairs of keypoints that should be connected when visualizing the pose.</span></span>
<span id="cb14-48"><a href="#cb14-48"></a><span class="co">        This method should be implemented by subclasses.</span></span>
<span id="cb14-49"><a href="#cb14-49"></a><span class="co">        There are pre-defined list of keypoint pairs for various models available in the project.</span></span>
<span id="cb14-50"><a href="#cb14-50"></a><span class="co">        </span></span>
<span id="cb14-51"><a href="#cb14-51"></a><span class="co">        Returns:</span></span>
<span id="cb14-52"><a href="#cb14-52"></a><span class="co">            list: A list of tuples, where each tuple contains two integers representing the indices </span></span>
<span id="cb14-53"><a href="#cb14-53"></a><span class="co">                 of keypoints that should be connected with a line when visualizing the pose.</span></span>
<span id="cb14-54"><a href="#cb14-54"></a><span class="co">                 For example, [(0,1), (1,2)] means that keypoint 0 should be connected to keypoint 1,</span></span>
<span id="cb14-55"><a href="#cb14-55"></a><span class="co">                 and keypoint 1 should be connected to keypoint 2.</span></span>
<span id="cb14-56"><a href="#cb14-56"></a><span class="co">        """</span></span>
<span id="cb14-57"><a href="#cb14-57"></a>        <span class="cf">pass</span></span>
<span id="cb14-58"><a href="#cb14-58"></a></span>
<span id="cb14-59"><a href="#cb14-59"></a>    <span class="kw">def</span> assert_frame_count_is_correct(<span class="va">self</span>, video_pose_result: VideoPoseResult, video_metadata: <span class="bu">dict</span>):</span>
<span id="cb14-60"><a href="#cb14-60"></a>        <span class="co">"""</span></span>
<span id="cb14-61"><a href="#cb14-61"></a><span class="co">        Assert that the number of frames in the frame results matches the number of frames in the video. Should be called at the end of the estimate_pose method.</span></span>
<span id="cb14-62"><a href="#cb14-62"></a></span>
<span id="cb14-63"><a href="#cb14-63"></a><span class="co">        Args:</span></span>
<span id="cb14-64"><a href="#cb14-64"></a><span class="co">            frame_results (list): A list of FramePoseResult objects.</span></span>
<span id="cb14-65"><a href="#cb14-65"></a><span class="co">            video_metadata (dict): A dictionary containing the video metadata with the key "frame_count".</span></span>
<span id="cb14-66"><a href="#cb14-66"></a><span class="co">        Raises:</span></span>
<span id="cb14-67"><a href="#cb14-67"></a><span class="co">            Exception: If the number of frames in the frame results does not match the number of frames in the video.</span></span>
<span id="cb14-68"><a href="#cb14-68"></a><span class="co">        """</span></span>
<span id="cb14-69"><a href="#cb14-69"></a>        <span class="cf">if</span> <span class="bu">len</span>(video_pose_result.frames) <span class="op">!=</span> video_metadata.get(<span class="st">"frame_count"</span>):</span>
<span id="cb14-70"><a href="#cb14-70"></a>            <span class="cf">raise</span> <span class="pp">Exception</span>(<span class="ss">f"Number of frames in the video (</span><span class="sc">{</span>video_metadata<span class="sc">.</span>get(<span class="st">'frame_count'</span>)<span class="sc">}</span><span class="ss">) does not match the number of frames in the frame results (</span><span class="sc">{</span><span class="bu">len</span>(video_pose_result.frames)<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb14-71"><a href="#cb14-71"></a></span>
<span id="cb14-72"><a href="#cb14-72"></a>    <span class="kw">def</span> filter_low_confidence_keypoints(<span class="va">self</span>, video_pose_result: VideoPoseResult):</span>
<span id="cb14-73"><a href="#cb14-73"></a>        <span class="cf">for</span> frame_result <span class="kw">in</span> video_pose_result.frames:</span>
<span id="cb14-74"><a href="#cb14-74"></a>            <span class="cf">for</span> person_result <span class="kw">in</span> frame_result.persons:</span>
<span id="cb14-75"><a href="#cb14-75"></a>                <span class="cf">for</span> keypoint <span class="kw">in</span> person_result.keypoints:</span>
<span id="cb14-76"><a href="#cb14-76"></a>                    <span class="cf">if</span> keypoint.confidence <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> keypoint.confidence <span class="op">&lt;</span> <span class="va">self</span>.confidence_threshold:</span>
<span id="cb14-77"><a href="#cb14-77"></a>                        keypoint.x <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-78"><a href="#cb14-78"></a>                        keypoint.y <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-79"><a href="#cb14-79"></a>                        keypoint.confidence <span class="op">=</span> <span class="va">None</span></span>
<span id="cb14-80"><a href="#cb14-80"></a>        <span class="cf">return</span> video_pose_result</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
YOLO Model
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>src/models/yolo_pose_estimator.py</strong></pre>
</div>
<div class="sourceCode" id="cb15" data-filename="src/models/yolo_pose_estimator.py"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a><span class="im">import</span> os</span>
<span id="cb15-2"><a href="#cb15-2"></a><span class="im">import</span> torch</span>
<span id="cb15-3"><a href="#cb15-3"></a><span class="im">import</span> utils</span>
<span id="cb15-4"><a href="#cb15-4"></a><span class="im">from</span> ultralytics <span class="im">import</span> YOLO</span>
<span id="cb15-5"><a href="#cb15-5"></a></span>
<span id="cb15-6"><a href="#cb15-6"></a><span class="im">from</span> models <span class="im">import</span> PoseEstimator</span>
<span id="cb15-7"><a href="#cb15-7"></a><span class="im">from</span> inference <span class="im">import</span> FramePoseResult, PersonPoseResult, PoseKeypoint, VideoPoseResult</span>
<span id="cb15-8"><a href="#cb15-8"></a><span class="im">from</span> keypoint_pairs <span class="im">import</span> COCO_KEYPOINT_PAIRS</span>
<span id="cb15-9"><a href="#cb15-9"></a></span>
<span id="cb15-10"><a href="#cb15-10"></a><span class="kw">class</span> YoloPoseEstimator(PoseEstimator):</span>
<span id="cb15-11"><a href="#cb15-11"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, name: <span class="bu">str</span>, config: <span class="bu">dict</span>):</span>
<span id="cb15-12"><a href="#cb15-12"></a>        <span class="co">"""</span></span>
<span id="cb15-13"><a href="#cb15-13"></a><span class="co">        Initialize the YoloPoseEstimator with a model name and configuration.</span></span>
<span id="cb15-14"><a href="#cb15-14"></a><span class="co">        Args:</span></span>
<span id="cb15-15"><a href="#cb15-15"></a><span class="co">            name (str): The name of the model (e.g. "YoloPose").</span></span>
<span id="cb15-16"><a href="#cb15-16"></a><span class="co">            config (dict): Configuration dictionary for the model. It must contain the key "weights" with the path to the weights file relative to the weights folder, otherwise it uses 'yolo11n-pose.pt'.</span></span>
<span id="cb15-17"><a href="#cb15-17"></a><span class="co">        """</span></span>
<span id="cb15-18"><a href="#cb15-18"></a></span>
<span id="cb15-19"><a href="#cb15-19"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(name, config)</span>
<span id="cb15-20"><a href="#cb15-20"></a></span>
<span id="cb15-21"><a href="#cb15-21"></a>        weights_file <span class="op">=</span> <span class="va">self</span>.config.get(<span class="st">"weights"</span>, <span class="st">"yolo11n-pose.pt"</span>)</span>
<span id="cb15-22"><a href="#cb15-22"></a>        <span class="bu">print</span>(<span class="st">"Using weights file: "</span>, weights_file)</span>
<span id="cb15-23"><a href="#cb15-23"></a>        pre_built_weights_file_path <span class="op">=</span> os.path.join(<span class="st">"/weights/pre_built"</span>, weights_file)</span>
<span id="cb15-24"><a href="#cb15-24"></a>        user_weights_file_path <span class="op">=</span> os.path.join(<span class="st">"/weights/user_weights"</span>, weights_file)</span>
<span id="cb15-25"><a href="#cb15-25"></a></span>
<span id="cb15-26"><a href="#cb15-26"></a>        <span class="cf">if</span> os.path.exists(user_weights_file_path):</span>
<span id="cb15-27"><a href="#cb15-27"></a>            weights_file_path <span class="op">=</span> user_weights_file_path</span>
<span id="cb15-28"><a href="#cb15-28"></a>        <span class="cf">elif</span> os.path.exists(pre_built_weights_file_path):</span>
<span id="cb15-29"><a href="#cb15-29"></a>            weights_file_path <span class="op">=</span> pre_built_weights_file_path</span>
<span id="cb15-30"><a href="#cb15-30"></a>        <span class="cf">else</span>:</span>
<span id="cb15-31"><a href="#cb15-31"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(</span>
<span id="cb15-32"><a href="#cb15-32"></a>                <span class="ss">f"Could not find weights file </span><span class="sc">{</span>weights_file<span class="sc">}</span><span class="ss">. Please download the weights from https://docs.ultralytics.com/tasks/pose/ and place them in the weights folder."</span></span>
<span id="cb15-33"><a href="#cb15-33"></a>            )</span>
<span id="cb15-34"><a href="#cb15-34"></a></span>
<span id="cb15-35"><a href="#cb15-35"></a>        <span class="va">self</span>.model <span class="op">=</span> YOLO(weights_file_path)</span>
<span id="cb15-36"><a href="#cb15-36"></a>        <span class="co"># only for dev</span></span>
<span id="cb15-37"><a href="#cb15-37"></a>        device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb15-38"><a href="#cb15-38"></a>        <span class="va">self</span>.model.to(device)</span>
<span id="cb15-39"><a href="#cb15-39"></a></span>
<span id="cb15-40"><a href="#cb15-40"></a>    <span class="kw">def</span> get_keypoint_pairs(<span class="va">self</span>):</span>
<span id="cb15-41"><a href="#cb15-41"></a>        <span class="co"># Yolo keypoints are stored in Coco format</span></span>
<span id="cb15-42"><a href="#cb15-42"></a>        <span class="cf">return</span> COCO_KEYPOINT_PAIRS</span>
<span id="cb15-43"><a href="#cb15-43"></a></span>
<span id="cb15-44"><a href="#cb15-44"></a>    <span class="kw">def</span> estimate_pose(<span class="va">self</span>, video_path: <span class="bu">str</span>) <span class="op">-&gt;</span> VideoPoseResult:</span>
<span id="cb15-45"><a href="#cb15-45"></a>        <span class="co">"""</span></span>
<span id="cb15-46"><a href="#cb15-46"></a><span class="co">        Estimate the pose of a video using YOLO pose estimation.</span></span>
<span id="cb15-47"><a href="#cb15-47"></a></span>
<span id="cb15-48"><a href="#cb15-48"></a><span class="co">        Args:</span></span>
<span id="cb15-49"><a href="#cb15-49"></a><span class="co">            video_path (str): The path to the input video file.</span></span>
<span id="cb15-50"><a href="#cb15-50"></a><span class="co">        Returns:</span></span>
<span id="cb15-51"><a href="#cb15-51"></a><span class="co">            VideoPoseResult: A standardized result object containing the pose estimation results for the video.</span></span>
<span id="cb15-52"><a href="#cb15-52"></a><span class="co">        """</span></span>
<span id="cb15-53"><a href="#cb15-53"></a></span>
<span id="cb15-54"><a href="#cb15-54"></a>        cap, video_metadata <span class="op">=</span> utils.get_video_metadata(video_path)</span>
<span id="cb15-55"><a href="#cb15-55"></a>        video_name <span class="op">=</span> os.path.splitext(os.path.basename(video_path))[<span class="dv">0</span>]</span>
<span id="cb15-56"><a href="#cb15-56"></a>        cap.release()</span>
<span id="cb15-57"><a href="#cb15-57"></a></span>
<span id="cb15-58"><a href="#cb15-58"></a>        results <span class="op">=</span> <span class="va">self</span>.model.track(</span>
<span id="cb15-59"><a href="#cb15-59"></a>            video_path, conf<span class="op">=</span><span class="va">self</span>.confidence_threshold, stream<span class="op">=</span><span class="va">True</span>, verbose<span class="op">=</span><span class="va">False</span></span>
<span id="cb15-60"><a href="#cb15-60"></a>        )</span>
<span id="cb15-61"><a href="#cb15-61"></a></span>
<span id="cb15-62"><a href="#cb15-62"></a>        frame_results <span class="op">=</span> []</span>
<span id="cb15-63"><a href="#cb15-63"></a>        <span class="cf">for</span> frame_idx, frame_result <span class="kw">in</span> <span class="bu">enumerate</span>(results):</span>
<span id="cb15-64"><a href="#cb15-64"></a>            <span class="cf">if</span> <span class="kw">not</span> frame_result.keypoints:  <span class="co"># if no keypoints detected</span></span>
<span id="cb15-65"><a href="#cb15-65"></a>                frame_results.append(FramePoseResult(persons<span class="op">=</span>[], frame_idx<span class="op">=</span>frame_idx))</span>
<span id="cb15-66"><a href="#cb15-66"></a>                <span class="cf">continue</span></span>
<span id="cb15-67"><a href="#cb15-67"></a></span>
<span id="cb15-68"><a href="#cb15-68"></a>            xys <span class="op">=</span> frame_result.keypoints.xy.cpu().numpy()</span>
<span id="cb15-69"><a href="#cb15-69"></a>            confidences <span class="op">=</span> frame_result.keypoints.conf</span>
<span id="cb15-70"><a href="#cb15-70"></a></span>
<span id="cb15-71"><a href="#cb15-71"></a>            <span class="cf">if</span> xys.size <span class="op">==</span> <span class="dv">0</span>: <span class="co"># if no persons detected</span></span>
<span id="cb15-72"><a href="#cb15-72"></a>                frame_results.append(FramePoseResult(persons<span class="op">=</span>[], frame_idx<span class="op">=</span>frame_idx))</span>
<span id="cb15-73"><a href="#cb15-73"></a>                <span class="cf">continue</span></span>
<span id="cb15-74"><a href="#cb15-74"></a></span>
<span id="cb15-75"><a href="#cb15-75"></a>            persons <span class="op">=</span> []</span>
<span id="cb15-76"><a href="#cb15-76"></a>            num_persons <span class="op">=</span> frame_result.keypoints.shape[<span class="dv">0</span>]</span>
<span id="cb15-77"><a href="#cb15-77"></a>            num_keypoints <span class="op">=</span> frame_result.keypoints.shape[<span class="dv">1</span>]</span>
<span id="cb15-78"><a href="#cb15-78"></a></span>
<span id="cb15-79"><a href="#cb15-79"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_persons):</span>
<span id="cb15-80"><a href="#cb15-80"></a>                keypoints <span class="op">=</span> []</span>
<span id="cb15-81"><a href="#cb15-81"></a>                <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(num_keypoints):</span>
<span id="cb15-82"><a href="#cb15-82"></a>                    conf <span class="op">=</span> <span class="bu">float</span>(confidences[i, j]) <span class="cf">if</span> (confidences <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>) <span class="kw">and</span> (xys[i, j, <span class="dv">0</span>] <span class="op">!=</span> <span class="dv">0</span> <span class="kw">and</span> xys[i, j, <span class="dv">1</span>] <span class="op">!=</span> <span class="dv">0</span>) <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb15-83"><a href="#cb15-83"></a>                    kp <span class="op">=</span> PoseKeypoint(</span>
<span id="cb15-84"><a href="#cb15-84"></a>                        x<span class="op">=</span><span class="bu">float</span>(xys[i, j, <span class="dv">0</span>]),</span>
<span id="cb15-85"><a href="#cb15-85"></a>                        y<span class="op">=</span><span class="bu">float</span>(xys[i, j, <span class="dv">1</span>]),</span>
<span id="cb15-86"><a href="#cb15-86"></a>                        confidence<span class="op">=</span>conf,</span>
<span id="cb15-87"><a href="#cb15-87"></a>                    )</span>
<span id="cb15-88"><a href="#cb15-88"></a></span>
<span id="cb15-89"><a href="#cb15-89"></a>                    keypoints.append(kp)</span>
<span id="cb15-90"><a href="#cb15-90"></a>                persons.append(PersonPoseResult(keypoints<span class="op">=</span>keypoints))</span>
<span id="cb15-91"><a href="#cb15-91"></a>            frame_results.append(FramePoseResult(persons<span class="op">=</span>persons, frame_idx<span class="op">=</span>frame_idx))</span>
<span id="cb15-92"><a href="#cb15-92"></a></span>
<span id="cb15-93"><a href="#cb15-93"></a>        video_pose_result <span class="op">=</span> VideoPoseResult(</span>
<span id="cb15-94"><a href="#cb15-94"></a>            fps<span class="op">=</span>video_metadata.get(<span class="st">"fps"</span>),</span>
<span id="cb15-95"><a href="#cb15-95"></a>            frame_width<span class="op">=</span>video_metadata.get(<span class="st">"width"</span>),</span>
<span id="cb15-96"><a href="#cb15-96"></a>            frame_height<span class="op">=</span>video_metadata.get(<span class="st">"height"</span>),</span>
<span id="cb15-97"><a href="#cb15-97"></a>            frames<span class="op">=</span>frame_results,</span>
<span id="cb15-98"><a href="#cb15-98"></a>            video_name<span class="op">=</span>video_name,</span>
<span id="cb15-99"><a href="#cb15-99"></a>        )</span>
<span id="cb15-100"><a href="#cb15-100"></a></span>
<span id="cb15-101"><a href="#cb15-101"></a>        <span class="va">self</span>.assert_frame_count_is_correct(video_pose_result, video_metadata)</span>
<span id="cb15-102"><a href="#cb15-102"></a>        video_pose_result <span class="op">=</span> <span class="va">self</span>.filter_low_confidence_keypoints(video_pose_result)</span>
<span id="cb15-103"><a href="#cb15-103"></a>        <span class="co"># we do not convert keypoints to coco format, because yolo already stores keypoints in coco format</span></span>
<span id="cb15-104"><a href="#cb15-104"></a>        <span class="cf">return</span> video_pose_result</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<p>MaskBench supports seven pose estimators, including pure AI models such as YOLOv11-Pose, MediaPipePose, and OpenPose. Additionally, it incorporates MaskAnyone as a pose estimator, which combines multiple expert models. We distinguish between two variants of the MaskAnyone estimator: the MaskAnyoneAPI pose estimator, which runs fully automatically during inference, and the MaskAnyoneUI pose estimator, which employs a human-in-the-loop approach allowing manual adjustment of the mask for the persons of interest. The latter requires manual execution by the user prior to running MaskBench, with the resulting pose files provided as one file per video.</p>
</section>
<section id="sec-architecture-inference-engine" class="level3" data-number="4.2.3">
<h3 data-number="4.2.3" class="anchored" data-anchor-id="sec-architecture-inference-engine"><span class="header-section-number">4.2.3</span> Inference Engine</h3>
<p>The inference engine is responsible for running pose estimators on videos and saving the results as JSON files in the poses folder. If a checkpoint name is specified in the configuration file, the inference engine will load existing results from the checkpoint and skip inference for videos that already have corresponding outputs. This feature allows the user to resume an already started inference process or to bypass the time-consuming inference entirely and perform only metric evaluation and rendering. The inference engine returns a nested dictionary that maps pose estimator names to video names and their corresponding <code>VideoPoseResult</code> objects. Additionally, it records the inference times for each pose estimator and video, saving this information as a JSON file within the checkpoint folder.</p>
</section>
</section>
<section id="sec-architecture-evaluation" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="sec-architecture-evaluation"><span class="header-section-number">4.3</span> Evaluation</h2>
<section id="sec-architecture-evaluation-metric" class="level3" data-number="4.3.1">
<h3 data-number="4.3.1" class="anchored" data-anchor-id="sec-architecture-evaluation-metric"><span class="header-section-number">4.3.1</span> Metric</h3>
<p>Metrics play a crucial role in quantitatively evaluating the accuracy and quality of pose predictions. Each metric inherits from the abstract <code>Metric</code> class and implements a computation method that takes as input a predicted video pose result, an optional ground truth pose result, and the name of the pose estimator. The <code>compute</code> method of a metric outputs a MetricResult object containing the metric values for the video (see section <a href="#sec-architecture-evaluation-metric-result" class="quarto-xref">Section&nbsp;4.3.2</a>).</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Metric Class
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>src/evaluation/metrics/metric.py</strong></pre>
</div>
<div class="sourceCode" id="cb16" data-filename="src/evaluation/metrics/metric.py"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a><span class="im">from</span> abc <span class="im">import</span> ABC, abstractmethod</span>
<span id="cb16-2"><a href="#cb16-2"></a><span class="im">from</span> typing <span class="im">import</span> Dict, List, Optional, Any</span>
<span id="cb16-3"><a href="#cb16-3"></a></span>
<span id="cb16-4"><a href="#cb16-4"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb16-5"><a href="#cb16-5"></a><span class="im">import</span> numpy.ma <span class="im">as</span> ma</span>
<span id="cb16-6"><a href="#cb16-6"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> linear_sum_assignment</span>
<span id="cb16-7"><a href="#cb16-7"></a></span>
<span id="cb16-8"><a href="#cb16-8"></a><span class="im">from</span> inference.pose_result <span class="im">import</span> VideoPoseResult</span>
<span id="cb16-9"><a href="#cb16-9"></a><span class="im">from</span> evaluation.metrics.metric_result <span class="im">import</span> MetricResult, FRAME_AXIS, PERSON_AXIS, KEYPOINT_AXIS</span>
<span id="cb16-10"><a href="#cb16-10"></a></span>
<span id="cb16-11"><a href="#cb16-11"></a></span>
<span id="cb16-12"><a href="#cb16-12"></a><span class="kw">class</span> Metric(ABC):</span>
<span id="cb16-13"><a href="#cb16-13"></a>    <span class="co">"""Base class for all metrics in MaskBench."""</span></span>
<span id="cb16-14"><a href="#cb16-14"></a>    </span>
<span id="cb16-15"><a href="#cb16-15"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, name: <span class="bu">str</span>, config: Optional[Dict[<span class="bu">str</span>, Any]] <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb16-16"><a href="#cb16-16"></a>        <span class="co">"""</span></span>
<span id="cb16-17"><a href="#cb16-17"></a><span class="co">        Initialize a metric.</span></span>
<span id="cb16-18"><a href="#cb16-18"></a><span class="co">        </span></span>
<span id="cb16-19"><a href="#cb16-19"></a><span class="co">        Args:</span></span>
<span id="cb16-20"><a href="#cb16-20"></a><span class="co">            name: Unique name of the metric</span></span>
<span id="cb16-21"><a href="#cb16-21"></a><span class="co">            config: Optional configuration dictionary for the metric</span></span>
<span id="cb16-22"><a href="#cb16-22"></a><span class="co">        """</span></span>
<span id="cb16-23"><a href="#cb16-23"></a>        <span class="va">self</span>.name <span class="op">=</span> name</span>
<span id="cb16-24"><a href="#cb16-24"></a>        <span class="va">self</span>.config <span class="op">=</span> config <span class="kw">or</span> {}</span>
<span id="cb16-25"><a href="#cb16-25"></a>    </span>
<span id="cb16-26"><a href="#cb16-26"></a>    <span class="at">@abstractmethod</span></span>
<span id="cb16-27"><a href="#cb16-27"></a>    <span class="kw">def</span> compute(</span>
<span id="cb16-28"><a href="#cb16-28"></a>        <span class="va">self</span>,</span>
<span id="cb16-29"><a href="#cb16-29"></a>        video_result: VideoPoseResult,</span>
<span id="cb16-30"><a href="#cb16-30"></a>        gt_video_result: Optional[VideoPoseResult] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb16-31"><a href="#cb16-31"></a>        model_name: Optional[<span class="bu">str</span>] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb16-32"><a href="#cb16-32"></a>    ) <span class="op">-&gt;</span> MetricResult:</span>
<span id="cb16-33"><a href="#cb16-33"></a>        <span class="co">"""</span></span>
<span id="cb16-34"><a href="#cb16-34"></a><span class="co">        Compute the metric for a video.</span></span>
<span id="cb16-35"><a href="#cb16-35"></a><span class="co">        </span></span>
<span id="cb16-36"><a href="#cb16-36"></a><span class="co">        Args:</span></span>
<span id="cb16-37"><a href="#cb16-37"></a><span class="co">            video_result: Pose estimation results for the video</span></span>
<span id="cb16-38"><a href="#cb16-38"></a><span class="co">            gt_video_result: Optional ground truth pose results</span></span>
<span id="cb16-39"><a href="#cb16-39"></a><span class="co">            model_name: Name of the model being evaluated</span></span>
<span id="cb16-40"><a href="#cb16-40"></a><span class="co">            </span></span>
<span id="cb16-41"><a href="#cb16-41"></a><span class="co">        Returns:</span></span>
<span id="cb16-42"><a href="#cb16-42"></a><span class="co">            MetricResult containing the metric values for the video</span></span>
<span id="cb16-43"><a href="#cb16-43"></a><span class="co">        """</span></span>
<span id="cb16-44"><a href="#cb16-44"></a>        <span class="cf">pass</span></span>
<span id="cb16-45"><a href="#cb16-45"></a></span>
<span id="cb16-46"><a href="#cb16-46"></a>    <span class="kw">def</span> _match_person_indices(<span class="va">self</span>, poses_to_match: ma.MaskedArray, reference: ma.MaskedArray) <span class="op">-&gt;</span> ma.MaskedArray:</span>
<span id="cb16-47"><a href="#cb16-47"></a>        <span class="co">"""</span></span>
<span id="cb16-48"><a href="#cb16-48"></a><span class="co">        Match the predictions to the reference (e.g. ground truth or previous frame) for a single frame.</span></span>
<span id="cb16-49"><a href="#cb16-49"></a><span class="co">        This is useful for metrics that are order-dependent, such as PCK, acceleration or RMSE.</span></span>
<span id="cb16-50"><a href="#cb16-50"></a><span class="co">        It uses the Hungarian algorithm to find the best match between the predictions and the reference.</span></span>
<span id="cb16-51"><a href="#cb16-51"></a><span class="co">        If there are no predictions, it returns an array of infinities of reference shape. Infinities are used instead of nans</span></span>
<span id="cb16-52"><a href="#cb16-52"></a><span class="co">        to have an infinitely large mean center of a pose, which will not be matched to any reference (for example in a previous frame for kinematic metrics).</span></span>
<span id="cb16-53"><a href="#cb16-53"></a></span>
<span id="cb16-54"><a href="#cb16-54"></a><span class="co">        Args:</span></span>
<span id="cb16-55"><a href="#cb16-55"></a><span class="co">            poses_to_match: Predicted poses array of shape (M, K, 2) where M is number of persons</span></span>
<span id="cb16-56"><a href="#cb16-56"></a><span class="co">            reference: Reference poses array of shape (N, K, 2) where N is number of persons</span></span>
<span id="cb16-57"><a href="#cb16-57"></a><span class="co">            </span></span>
<span id="cb16-58"><a href="#cb16-58"></a><span class="co">        Returns:</span></span>
<span id="cb16-59"><a href="#cb16-59"></a><span class="co">            Sorted predictions array of shape (max(M,N), K, 2) where:</span></span>
<span id="cb16-60"><a href="#cb16-60"></a><span class="co">            - First N positions contain predictions matched to reference (or infinity if no match)</span></span>
<span id="cb16-61"><a href="#cb16-61"></a><span class="co">            - Remaining M-N positions (if M&gt;N) contain unmatched predictions</span></span>
<span id="cb16-62"><a href="#cb16-62"></a><span class="co">        """</span></span>
<span id="cb16-63"><a href="#cb16-63"></a>        M, K, _ <span class="op">=</span> poses_to_match.shape</span>
<span id="cb16-64"><a href="#cb16-64"></a>        N, _, _ <span class="op">=</span> reference.shape</span>
<span id="cb16-65"><a href="#cb16-65"></a>        </span>
<span id="cb16-66"><a href="#cb16-66"></a>        <span class="co"># If no predictions, return array of infinities of reference shape</span></span>
<span id="cb16-67"><a href="#cb16-67"></a>        <span class="cf">if</span> M <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb16-68"><a href="#cb16-68"></a>            <span class="cf">return</span> np.full_like(reference, np.inf)</span>
<span id="cb16-69"><a href="#cb16-69"></a>            </span>
<span id="cb16-70"><a href="#cb16-70"></a>        <span class="co"># If no reference, return predictions as is</span></span>
<span id="cb16-71"><a href="#cb16-71"></a>        <span class="cf">if</span> N <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb16-72"><a href="#cb16-72"></a>            <span class="cf">return</span> poses_to_match</span>
<span id="cb16-73"><a href="#cb16-73"></a>            </span>
<span id="cb16-74"><a href="#cb16-74"></a></span>
<span id="cb16-75"><a href="#cb16-75"></a>        <span class="co"># Calculate mean positions</span></span>
<span id="cb16-76"><a href="#cb16-76"></a>        mean_poses_to_match <span class="op">=</span> np.nanmean(poses_to_match, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb16-77"><a href="#cb16-77"></a>        mean_ref_poses <span class="op">=</span> np.nanmean(reference, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb16-78"><a href="#cb16-78"></a></span>
<span id="cb16-79"><a href="#cb16-79"></a>        valid_M <span class="op">=</span> M</span>
<span id="cb16-80"><a href="#cb16-80"></a>        valid_N <span class="op">=</span> N</span>
<span id="cb16-81"><a href="#cb16-81"></a>        </span>
<span id="cb16-82"><a href="#cb16-82"></a>        <span class="co"># Limit the cost matrix only to the valid persons (i.e. where the person is not completely masked)</span></span>
<span id="cb16-83"><a href="#cb16-83"></a>        <span class="co"># Therefore, we need to create a mapping of the original person indices to the valid person indices</span></span>
<span id="cb16-84"><a href="#cb16-84"></a>        poses_to_match_index_mapping <span class="op">=</span> []</span>
<span id="cb16-85"><a href="#cb16-85"></a>        reference_index_mapping <span class="op">=</span> []</span>
<span id="cb16-86"><a href="#cb16-86"></a>        is_poses_to_match_masked_array <span class="op">=</span> <span class="bu">isinstance</span>(poses_to_match, ma.MaskedArray)</span>
<span id="cb16-87"><a href="#cb16-87"></a>        is_reference_masked_array <span class="op">=</span> <span class="bu">isinstance</span>(reference, ma.MaskedArray)</span>
<span id="cb16-88"><a href="#cb16-88"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(M):</span>
<span id="cb16-89"><a href="#cb16-89"></a>            <span class="cf">if</span> is_poses_to_match_masked_array <span class="kw">and</span> poses_to_match.mask.<span class="bu">all</span>(axis<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">2</span>))[i]: <span class="co"># Reduce the number of valid poses to match (if the person is completely masked)</span></span>
<span id="cb16-90"><a href="#cb16-90"></a>                valid_M <span class="op">-=</span> <span class="dv">1</span></span>
<span id="cb16-91"><a href="#cb16-91"></a>            <span class="cf">else</span>:</span>
<span id="cb16-92"><a href="#cb16-92"></a>                poses_to_match_index_mapping.append(i)</span>
<span id="cb16-93"><a href="#cb16-93"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(N):</span>
<span id="cb16-94"><a href="#cb16-94"></a>            <span class="cf">if</span> is_reference_masked_array <span class="kw">and</span> reference.mask.<span class="bu">all</span>(axis<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">2</span>))[i]: <span class="co"># Reduce the number of valid references (if the person is completely masked)</span></span>
<span id="cb16-95"><a href="#cb16-95"></a>                valid_N <span class="op">-=</span> <span class="dv">1</span></span>
<span id="cb16-96"><a href="#cb16-96"></a>            <span class="cf">else</span>:</span>
<span id="cb16-97"><a href="#cb16-97"></a>                reference_index_mapping.append(i)</span>
<span id="cb16-98"><a href="#cb16-98"></a></span>
<span id="cb16-99"><a href="#cb16-99"></a>        <span class="co"># Calculate cost matrix based on Euclidian distance between each prediction (valid_M) in the rows and references (valid_N) in the columns</span></span>
<span id="cb16-100"><a href="#cb16-100"></a>        cost_matrix <span class="op">=</span> np.zeros((valid_M, valid_N))</span>
<span id="cb16-101"><a href="#cb16-101"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(valid_M):</span>
<span id="cb16-102"><a href="#cb16-102"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(valid_N):</span>
<span id="cb16-103"><a href="#cb16-103"></a>                pos_to_match_idx <span class="op">=</span> poses_to_match_index_mapping[i]</span>
<span id="cb16-104"><a href="#cb16-104"></a>                ref_idx <span class="op">=</span> reference_index_mapping[j]</span>
<span id="cb16-105"><a href="#cb16-105"></a>                cost_matrix[i, j] <span class="op">=</span> np.linalg.norm(mean_poses_to_match[pos_to_match_idx] <span class="op">-</span> mean_ref_poses[ref_idx])</span>
<span id="cb16-106"><a href="#cb16-106"></a>        <span class="co"># Remove rows where all entries are nan, which might happen if the shape N or M is </span></span>
<span id="cb16-107"><a href="#cb16-107"></a>        <span class="co"># greater than the maximum number of persons in the reference or predictions.</span></span>
<span id="cb16-108"><a href="#cb16-108"></a>        valid_rows <span class="op">=</span> <span class="op">~</span>np.<span class="bu">all</span>(np.isnan(cost_matrix), axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb16-109"><a href="#cb16-109"></a>        cost_matrix <span class="op">=</span> cost_matrix[valid_rows]</span>
<span id="cb16-110"><a href="#cb16-110"></a>                </span>
<span id="cb16-111"><a href="#cb16-111"></a>        <span class="co"># Apply Hungarian algorithm</span></span>
<span id="cb16-112"><a href="#cb16-112"></a>        row_ind, col_ind <span class="op">=</span> linear_sum_assignment(cost_matrix)</span>
<span id="cb16-113"><a href="#cb16-113"></a>        mapped_row_ind <span class="op">=</span> [poses_to_match_index_mapping[i] <span class="cf">for</span> i <span class="kw">in</span> row_ind]</span>
<span id="cb16-114"><a href="#cb16-114"></a>        mapped_col_ind <span class="op">=</span> [reference_index_mapping[i] <span class="cf">for</span> i <span class="kw">in</span> col_ind]</span>
<span id="cb16-115"><a href="#cb16-115"></a>        </span>
<span id="cb16-116"><a href="#cb16-116"></a>        <span class="co"># Create output array that can hold all predictions</span></span>
<span id="cb16-117"><a href="#cb16-117"></a>        max_persons <span class="op">=</span> <span class="bu">max</span>(M, N)</span>
<span id="cb16-118"><a href="#cb16-118"></a>        sorted_poses_to_match <span class="op">=</span> np.full((max_persons, K, <span class="dv">2</span>), np.inf)</span>
<span id="cb16-119"><a href="#cb16-119"></a>        </span>
<span id="cb16-120"><a href="#cb16-120"></a>        <span class="co"># First, fill the matched predictions in reference order and their dedicated mapped index</span></span>
<span id="cb16-121"><a href="#cb16-121"></a>        used_pred_indices <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb16-122"><a href="#cb16-122"></a>        <span class="cf">for</span> pred_idx, gt_idx <span class="kw">in</span> <span class="bu">zip</span>(mapped_row_ind, mapped_col_ind):</span>
<span id="cb16-123"><a href="#cb16-123"></a>            <span class="cf">if</span> pred_idx <span class="op">&lt;</span> M <span class="kw">and</span> gt_idx <span class="op">&lt;</span> N:  <span class="co"># Only use valid matches</span></span>
<span id="cb16-124"><a href="#cb16-124"></a>                sorted_poses_to_match[gt_idx] <span class="op">=</span> poses_to_match[pred_idx]</span>
<span id="cb16-125"><a href="#cb16-125"></a>                used_pred_indices.add(pred_idx)</span>
<span id="cb16-126"><a href="#cb16-126"></a>        </span>
<span id="cb16-127"><a href="#cb16-127"></a>        <span class="co"># Then append any unused predictions at the end (i.e. additional persons)</span></span>
<span id="cb16-128"><a href="#cb16-128"></a>        <span class="cf">if</span> valid_M <span class="op">&gt;</span> valid_N:</span>
<span id="cb16-129"><a href="#cb16-129"></a>            extra_idx <span class="op">=</span> <span class="bu">len</span>(used_pred_indices)  <span class="co"># Start after used predictions</span></span>
<span id="cb16-130"><a href="#cb16-130"></a>            <span class="cf">for</span> pred_idx <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, M):</span>
<span id="cb16-131"><a href="#cb16-131"></a>                <span class="cf">if</span> pred_idx <span class="kw">not</span> <span class="kw">in</span> used_pred_indices:</span>
<span id="cb16-132"><a href="#cb16-132"></a>                    sorted_poses_to_match[extra_idx] <span class="op">=</span> poses_to_match[pred_idx]</span>
<span id="cb16-133"><a href="#cb16-133"></a>                    extra_idx <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb16-134"><a href="#cb16-134"></a>                </span>
<span id="cb16-135"><a href="#cb16-135"></a>        <span class="co"># Create masked array where persons that are all 0 or inf are masked</span></span>
<span id="cb16-136"><a href="#cb16-136"></a>        masked_poses <span class="op">=</span> ma.array(sorted_poses_to_match)</span>
<span id="cb16-137"><a href="#cb16-137"></a>        person_mask <span class="op">=</span> (</span>
<span id="cb16-138"><a href="#cb16-138"></a>            (masked_poses <span class="op">==</span> <span class="dv">0</span>).<span class="bu">all</span>(axis<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">2</span>)) <span class="op">|</span> </span>
<span id="cb16-139"><a href="#cb16-139"></a>            (np.isinf(masked_poses)).<span class="bu">all</span>(axis<span class="op">=</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb16-140"><a href="#cb16-140"></a>        )</span>
<span id="cb16-141"><a href="#cb16-141"></a>        masked_poses[person_mask] <span class="op">=</span> ma.masked</span>
<span id="cb16-142"><a href="#cb16-142"></a>        <span class="cf">return</span> masked_poses</span>
<span id="cb16-143"><a href="#cb16-143"></a></span>
<span id="cb16-144"><a href="#cb16-144"></a></span>
<span id="cb16-145"><a href="#cb16-145"></a><span class="kw">class</span> DummyMetric(Metric):</span>
<span id="cb16-146"><a href="#cb16-146"></a>    <span class="co">"""</span></span>
<span id="cb16-147"><a href="#cb16-147"></a><span class="co">    A simple metric that just returns the raw pose data.</span></span>
<span id="cb16-148"><a href="#cb16-148"></a><span class="co">    Useful for testing and as an example of how to implement a metric.</span></span>
<span id="cb16-149"><a href="#cb16-149"></a><span class="co">    """</span></span>
<span id="cb16-150"><a href="#cb16-150"></a>    </span>
<span id="cb16-151"><a href="#cb16-151"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: Optional[Dict[<span class="bu">str</span>, Any]] <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb16-152"><a href="#cb16-152"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(name<span class="op">=</span><span class="st">"DummyMetric"</span>, config<span class="op">=</span>config)</span>
<span id="cb16-153"><a href="#cb16-153"></a>    </span>
<span id="cb16-154"><a href="#cb16-154"></a>    <span class="kw">def</span> compute(</span>
<span id="cb16-155"><a href="#cb16-155"></a>        <span class="va">self</span>,</span>
<span id="cb16-156"><a href="#cb16-156"></a>        video_result: VideoPoseResult,</span>
<span id="cb16-157"><a href="#cb16-157"></a>        gt_video_result: Optional[VideoPoseResult] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb16-158"><a href="#cb16-158"></a>        model_name: Optional[<span class="bu">str</span>] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb16-159"><a href="#cb16-159"></a>    ) <span class="op">-&gt;</span> MetricResult:</span>
<span id="cb16-160"><a href="#cb16-160"></a>        <span class="co">"""</span></span>
<span id="cb16-161"><a href="#cb16-161"></a><span class="co">        Simply convert the pose data to a MetricResult without any computation.</span></span>
<span id="cb16-162"><a href="#cb16-162"></a><span class="co">        """</span></span>
<span id="cb16-163"><a href="#cb16-163"></a>        <span class="co"># Convert pose data to masked array and take only x coordinates</span></span>
<span id="cb16-164"><a href="#cb16-164"></a>        values <span class="op">=</span> video_result.to_numpy_ma()[:, :, :, <span class="dv">0</span>]</span>
<span id="cb16-165"><a href="#cb16-165"></a></span>
<span id="cb16-166"><a href="#cb16-166"></a>        <span class="cf">return</span> MetricResult(</span>
<span id="cb16-167"><a href="#cb16-167"></a>            values<span class="op">=</span>values,</span>
<span id="cb16-168"><a href="#cb16-168"></a>            axis_names<span class="op">=</span>[FRAME_AXIS, PERSON_AXIS, KEYPOINT_AXIS],</span>
<span id="cb16-169"><a href="#cb16-169"></a>            metric_name<span class="op">=</span><span class="va">self</span>.name,</span>
<span id="cb16-170"><a href="#cb16-170"></a>            video_name<span class="op">=</span>video_result.video_name,</span>
<span id="cb16-171"><a href="#cb16-171"></a>            model_name<span class="op">=</span>model_name,</span>
<span id="cb16-172"><a href="#cb16-172"></a>        ) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<p>MaskBench currently implements ground truth-based metrics for Euclidean Distance, Percentage of Correct Keypoints (PCK), and Root Mean Square Error (RMSE). Furthermore, we provide kinematic metrics for velocity, acceleration, and jerk. Section <a href="#sec-metrics" class="quarto-xref">Section&nbsp;6</a> contains a more extensive description of the implemented metrics.</p>
<p><strong>Matching Person Indices</strong></p>
<p>For some metrics, it is essential to ensure that the order of persons in the predicted video pose results matches that of the reference. The metric class provides a method called <code>match_person_indices</code> to align person indices between ground truth and predicted results. This method is used not only in ground-truth-based metrics but also in kinematic metrics, which require consistent person indices across consecutive frames to compute velocity, acceleration, and other temporal measures. The implementation employs the Hungarian algorithm, using the mean position of a person’s keypoints to find the optimal matching between all persons in the reference and predicted pose results.</p>
<p>Let <span class="math inline">\(N\)</span> denote the number of persons in the reference, <span class="math inline">\(M\)</span> the number in the prediction, and <span class="math inline">\(K\)</span> the number of keypoints per person. The output of the <code>match_person_indices</code> method is an array with shape <span class="math inline">\(\text{max}(N, M) \times K \times 2\)</span>. The first <span class="math inline">\(N\)</span> entries correspond to persons ordered as in the reference, while the remaining <span class="math inline">\(M - N\)</span> entries (if <span class="math inline">\(M &gt; N\)</span>) represent additional persons present only in the prediction.</p>
<p>Edge cases include situations where a person appears in one frame but not in the next. In such cases, the unmatched person is assigned an index with infinite values to indicate absence, while the other persons retain consistent indices. This also applies when the prediction contains fewer persons than the reference (M &lt; N). Each metric can then handle these infinite values appropriately, for example, by converting them to <code>NaN</code> in kinematic metrics or assigning predefined values in Euclidean distance and ground truth–based metrics.</p>
<p><strong>Unit Testing</strong></p>
<p>Implementing unit tests for metric classes is essential to ensure that their outputs are accurate and consistent. We provide unit tests for all metrics in the <code>src/tests</code> folder, which can be executed using the <code>pytest</code> command. Running these tests after any modifications to the metric classes helps guarantee that existing functionality remains intact.</p>
</section>
<section id="sec-architecture-evaluation-metric-result" class="level3" data-number="4.3.2">
<h3 data-number="4.3.2" class="anchored" data-anchor-id="sec-architecture-evaluation-metric-result"><span class="header-section-number">4.3.2</span> Metric Result</h3>
<p>The output of a metric’s compute method is a <code>MetricResult</code> object. This object contains metric values stored in a multi-dimensional array, where each axis is labeled with descriptive names such as “frame,” “person,” and “keypoint”. The class provides an <code>aggregate</code> function that reduces these values using a specified method along selected axes only. Currently, MaskBench supports aggregation methods including mean, median, Root Mean Square Error (RMSE), vector magnitude, sum, minimum, and maximum. The result of the aggregation is another MetricResult object with reduced dimensionality, retaining only the axes that were not aggregated.</p>
<p>This flexible approach of storing the results with their axes names and using the names in the aggregation method allows for the visualization of the results in a variety of ways, for example, as a per-keypoint plot, distribution plot, or as a single scalar value. Furthermore, it allows extending the framework with new metrics (possibly containing different axis names) and also different visualizations.</p>
</section>
<section id="sec-architecture-evaluation-evaluator" class="level3" data-number="4.3.3">
<h3 data-number="4.3.3" class="anchored" data-anchor-id="sec-architecture-evaluation-evaluator"><span class="header-section-number">4.3.3</span> Evaluator</h3>
<p>Given a list of metrics, the evaluator executes each configured metric on the pose estimation results for all pose estimators and videos. It returns a nested dictionary that maps metric names to pose estimator names, then to video names, and finally to their corresponding <code>MetricResult</code> objects. It does not perform aggregation over the videos or pose estimators in order to allow for more flexibility in the visualization of the results.</p>
</section>
</section>
<section id="sec-architecture-visualization" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="sec-architecture-visualization"><span class="header-section-number">4.4</span> Visualization</h2>
<p>After evaluation, the results are visualized in plots and tables.</p>
<section id="visualizer" class="level3" data-number="4.4.1">
<h3 data-number="4.4.1" class="anchored" data-anchor-id="visualizer"><span class="header-section-number">4.4.1</span> Visualizer</h3>
<p>An abstract <code>BaseVisualizer</code> class defines the interface for all visualization components. We implemented a MaskBench-specific visualizer class tailored to our experiments, which can be reused for other studies or extended to accommodate new types of visualizations.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
MaskBench Visualizer
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>src/visualization/maskbench_visualizer.py</strong></pre>
</div>
<div class="sourceCode" id="cb17" data-filename="src/visualization/maskbench_visualizer.py"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a><span class="im">import</span> os</span>
<span id="cb17-2"><a href="#cb17-2"></a><span class="im">from</span> typing <span class="im">import</span> Dict</span>
<span id="cb17-3"><a href="#cb17-3"></a></span>
<span id="cb17-4"><a href="#cb17-4"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb17-5"><a href="#cb17-5"></a></span>
<span id="cb17-6"><a href="#cb17-6"></a><span class="im">from</span> evaluation.metrics <span class="im">import</span> MetricResult</span>
<span id="cb17-7"><a href="#cb17-7"></a><span class="im">from</span> evaluation.plots <span class="im">import</span> KinematicDistributionPlot, CocoKeypointPlot, generate_result_table, InferenceTimePlot</span>
<span id="cb17-8"><a href="#cb17-8"></a><span class="im">from</span> checkpointer <span class="im">import</span> Checkpointer</span>
<span id="cb17-9"><a href="#cb17-9"></a><span class="im">from</span> evaluation.metrics.metric_result <span class="im">import</span> COORDINATE_AXIS</span>
<span id="cb17-10"><a href="#cb17-10"></a><span class="im">from</span> .base_visualizer <span class="im">import</span> Visualizer</span>
<span id="cb17-11"><a href="#cb17-11"></a></span>
<span id="cb17-12"><a href="#cb17-12"></a></span>
<span id="cb17-13"><a href="#cb17-13"></a><span class="kw">class</span> MaskBenchVisualizer(Visualizer):</span>
<span id="cb17-14"><a href="#cb17-14"></a>    <span class="co">"""</span></span>
<span id="cb17-15"><a href="#cb17-15"></a><span class="co">    This class contains specific plots and tables for the MaskBench project evaluation. </span></span>
<span id="cb17-16"><a href="#cb17-16"></a><span class="co">    """</span></span>
<span id="cb17-17"><a href="#cb17-17"></a>        </span>
<span id="cb17-18"><a href="#cb17-18"></a>    <span class="kw">def</span> generate_all_plots(<span class="va">self</span>, pose_results: Dict[<span class="bu">str</span>, Dict[<span class="bu">str</span>, Dict[<span class="bu">str</span>, MetricResult]]]):</span>
<span id="cb17-19"><a href="#cb17-19"></a>        os.makedirs(<span class="va">self</span>.plots_dir, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-20"><a href="#cb17-20"></a></span>
<span id="cb17-21"><a href="#cb17-21"></a>        <span class="cf">if</span> <span class="st">"Velocity"</span> <span class="kw">in</span> pose_results.keys():</span>
<span id="cb17-22"><a href="#cb17-22"></a>            velocity_distribution_plot <span class="op">=</span> KinematicDistributionPlot(metric_name<span class="op">=</span><span class="st">"Velocity"</span>)</span>
<span id="cb17-23"><a href="#cb17-23"></a>            fig, filename <span class="op">=</span> velocity_distribution_plot.draw(pose_results, add_title<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb17-24"><a href="#cb17-24"></a>            <span class="va">self</span>._save_plot(fig, filename)</span>
<span id="cb17-25"><a href="#cb17-25"></a></span>
<span id="cb17-26"><a href="#cb17-26"></a>        <span class="cf">if</span> <span class="st">"Acceleration"</span> <span class="kw">in</span> pose_results.keys():</span>
<span id="cb17-27"><a href="#cb17-27"></a>            acceleration_distribution_plot <span class="op">=</span> KinematicDistributionPlot(metric_name<span class="op">=</span><span class="st">"Acceleration"</span>)</span>
<span id="cb17-28"><a href="#cb17-28"></a>            fig, filename <span class="op">=</span> acceleration_distribution_plot.draw(pose_results, add_title<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb17-29"><a href="#cb17-29"></a>            <span class="va">self</span>._save_plot(fig, filename)</span>
<span id="cb17-30"><a href="#cb17-30"></a></span>
<span id="cb17-31"><a href="#cb17-31"></a>            coco_keypoint_plot <span class="op">=</span> CocoKeypointPlot(metric_name<span class="op">=</span><span class="st">"Acceleration"</span>)</span>
<span id="cb17-32"><a href="#cb17-32"></a>            fig, filename <span class="op">=</span> coco_keypoint_plot.draw(pose_results, add_title<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb17-33"><a href="#cb17-33"></a>            <span class="va">self</span>._save_plot(fig, filename)</span>
<span id="cb17-34"><a href="#cb17-34"></a></span>
<span id="cb17-35"><a href="#cb17-35"></a>        <span class="cf">if</span> <span class="st">"Jerk"</span> <span class="kw">in</span> pose_results.keys():</span>
<span id="cb17-36"><a href="#cb17-36"></a>            jerk_distribution_plot <span class="op">=</span> KinematicDistributionPlot(metric_name<span class="op">=</span><span class="st">"Jerk"</span>)</span>
<span id="cb17-37"><a href="#cb17-37"></a>            fig, filename <span class="op">=</span> jerk_distribution_plot.draw(pose_results, add_title<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb17-38"><a href="#cb17-38"></a>            <span class="va">self</span>._save_plot(fig, filename)</span>
<span id="cb17-39"><a href="#cb17-39"></a></span>
<span id="cb17-40"><a href="#cb17-40"></a>        inference_times <span class="op">=</span> <span class="va">self</span>.checkpointer.load_inference_times()</span>
<span id="cb17-41"><a href="#cb17-41"></a>        <span class="cf">if</span> inference_times:</span>
<span id="cb17-42"><a href="#cb17-42"></a>            inference_times <span class="op">=</span> <span class="va">self</span>.set_maskanyone_ui_inference_times(inference_times)</span>
<span id="cb17-43"><a href="#cb17-43"></a>            inference_times <span class="op">=</span> <span class="va">self</span>.sort_inference_times_pose_estimator_order(inference_times, pose_results)</span>
<span id="cb17-44"><a href="#cb17-44"></a>            inference_time_plot <span class="op">=</span> InferenceTimePlot()</span>
<span id="cb17-45"><a href="#cb17-45"></a>            fig, filename <span class="op">=</span> inference_time_plot.draw(inference_times)</span>
<span id="cb17-46"><a href="#cb17-46"></a>            <span class="va">self</span>._save_plot(fig, filename)</span>
<span id="cb17-47"><a href="#cb17-47"></a></span>
<span id="cb17-48"><a href="#cb17-48"></a>        pose_results <span class="op">=</span> <span class="va">self</span>.calculate_kinematic_magnitudes(pose_results)</span>
<span id="cb17-49"><a href="#cb17-49"></a>        table_df <span class="op">=</span> generate_result_table(pose_results)</span>
<span id="cb17-50"><a href="#cb17-50"></a>        <span class="va">self</span>._save_table(table_df, <span class="st">"result_table.csv"</span>)</span>
<span id="cb17-51"><a href="#cb17-51"></a></span>
<span id="cb17-52"><a href="#cb17-52"></a>        </span>
<span id="cb17-53"><a href="#cb17-53"></a>    <span class="kw">def</span> set_maskanyone_ui_inference_times(<span class="va">self</span>, inference_times: Dict[<span class="bu">str</span>, Dict[<span class="bu">str</span>, <span class="bu">float</span>]]) <span class="op">-&gt;</span> Dict[<span class="bu">str</span>, Dict[<span class="bu">str</span>, <span class="bu">float</span>]]:</span>
<span id="cb17-54"><a href="#cb17-54"></a>        <span class="co">"""</span></span>
<span id="cb17-55"><a href="#cb17-55"></a><span class="co">        Set the inference times for MaskAnyoneUI to be equal to the corresponding MaskAnyoneAPI models.</span></span>
<span id="cb17-56"><a href="#cb17-56"></a><span class="co">        """</span></span>
<span id="cb17-57"><a href="#cb17-57"></a>        <span class="co"># Create a copy to avoid modifying the original</span></span>
<span id="cb17-58"><a href="#cb17-58"></a>        mapped_times <span class="op">=</span> inference_times.copy()</span>
<span id="cb17-59"><a href="#cb17-59"></a>        </span>
<span id="cb17-60"><a href="#cb17-60"></a>        <span class="co"># Define the mapping pairs</span></span>
<span id="cb17-61"><a href="#cb17-61"></a>        ui_to_api_mapping <span class="op">=</span> {</span>
<span id="cb17-62"><a href="#cb17-62"></a>            <span class="st">'MaskAnyoneUI-MediaPipe'</span>: <span class="st">'MaskAnyoneAPI-MediaPipe'</span>,</span>
<span id="cb17-63"><a href="#cb17-63"></a>            <span class="st">'MaskAnyoneUI-OpenPose'</span>: <span class="st">'MaskAnyoneAPI-OpenPose'</span></span>
<span id="cb17-64"><a href="#cb17-64"></a>        }</span>
<span id="cb17-65"><a href="#cb17-65"></a>        </span>
<span id="cb17-66"><a href="#cb17-66"></a>        <span class="co"># For each UI model, set its times to the corresponding API model</span></span>
<span id="cb17-67"><a href="#cb17-67"></a>        <span class="cf">for</span> ui_model, api_model <span class="kw">in</span> ui_to_api_mapping.items():</span>
<span id="cb17-68"><a href="#cb17-68"></a>            <span class="cf">if</span> ui_model <span class="kw">in</span> inference_times <span class="kw">and</span> api_model <span class="kw">in</span> inference_times:</span>
<span id="cb17-69"><a href="#cb17-69"></a>                mapped_times[ui_model] <span class="op">=</span> mapped_times[api_model].copy()</span>
<span id="cb17-70"><a href="#cb17-70"></a>                    </span>
<span id="cb17-71"><a href="#cb17-71"></a>        <span class="cf">return</span> mapped_times</span>
<span id="cb17-72"><a href="#cb17-72"></a></span>
<span id="cb17-73"><a href="#cb17-73"></a>    <span class="kw">def</span> calculate_kinematic_magnitudes(<span class="va">self</span>, pose_results: Dict[<span class="bu">str</span>, Dict[<span class="bu">str</span>, Dict[<span class="bu">str</span>, MetricResult]]]) <span class="op">-&gt;</span> Dict[<span class="bu">str</span>, Dict[<span class="bu">str</span>, Dict[<span class="bu">str</span>, MetricResult]]]:</span>
<span id="cb17-74"><a href="#cb17-74"></a>        <span class="co">"""</span></span>
<span id="cb17-75"><a href="#cb17-75"></a><span class="co">        Calculate the magnitude of the kinematic metrics.</span></span>
<span id="cb17-76"><a href="#cb17-76"></a><span class="co">        """</span></span>
<span id="cb17-77"><a href="#cb17-77"></a>        <span class="cf">for</span> metric_name <span class="kw">in</span> [<span class="st">"Velocity"</span>, <span class="st">"Acceleration"</span>, <span class="st">"Jerk"</span>]:</span>
<span id="cb17-78"><a href="#cb17-78"></a>            <span class="cf">if</span> metric_name <span class="kw">in</span> pose_results.keys():</span>
<span id="cb17-79"><a href="#cb17-79"></a>                <span class="cf">for</span> model_name, video_results <span class="kw">in</span> pose_results[metric_name].items():</span>
<span id="cb17-80"><a href="#cb17-80"></a>                    <span class="cf">for</span> video_name, metric_result <span class="kw">in</span> video_results.items():</span>
<span id="cb17-81"><a href="#cb17-81"></a>                        magnitude_values <span class="op">=</span> metric_result.aggregate([COORDINATE_AXIS], method<span class="op">=</span><span class="st">'vector_magnitude'</span>)</span>
<span id="cb17-82"><a href="#cb17-82"></a>                        pose_results[metric_name][model_name][video_name] <span class="op">=</span> magnitude_values</span>
<span id="cb17-83"><a href="#cb17-83"></a>        <span class="cf">return</span> pose_results</span>
<span id="cb17-84"><a href="#cb17-84"></a></span>
<span id="cb17-85"><a href="#cb17-85"></a>    <span class="kw">def</span> sort_inference_times_pose_estimator_order(<span class="va">self</span>, inference_times: Dict[<span class="bu">str</span>, Dict[<span class="bu">str</span>, <span class="bu">float</span>]], pose_results: Dict[<span class="bu">str</span>, Dict[<span class="bu">str</span>, Dict[<span class="bu">str</span>, MetricResult]]]) <span class="op">-&gt;</span> Dict[<span class="bu">str</span>, Dict[<span class="bu">str</span>, <span class="bu">float</span>]]:</span>
<span id="cb17-86"><a href="#cb17-86"></a>        <span class="co">"""</span></span>
<span id="cb17-87"><a href="#cb17-87"></a><span class="co">        Sort the inference times according to the order in pose_results.</span></span>
<span id="cb17-88"><a href="#cb17-88"></a><span class="co">        </span></span>
<span id="cb17-89"><a href="#cb17-89"></a><span class="co">        Args:</span></span>
<span id="cb17-90"><a href="#cb17-90"></a><span class="co">            inference_times: Dictionary containing inference times for each pose estimator</span></span>
<span id="cb17-91"><a href="#cb17-91"></a><span class="co">            pose_results: Dictionary containing pose estimation results, used to determine the order</span></span>
<span id="cb17-92"><a href="#cb17-92"></a><span class="co">            </span></span>
<span id="cb17-93"><a href="#cb17-93"></a><span class="co">        Returns:</span></span>
<span id="cb17-94"><a href="#cb17-94"></a><span class="co">            Dictionary containing sorted inference times</span></span>
<span id="cb17-95"><a href="#cb17-95"></a><span class="co">        """</span></span>
<span id="cb17-96"><a href="#cb17-96"></a>        <span class="co"># Get the list of pose estimators from any metric in pose_results</span></span>
<span id="cb17-97"><a href="#cb17-97"></a>        first_metric <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(pose_results))</span>
<span id="cb17-98"><a href="#cb17-98"></a>        pose_estimator_order <span class="op">=</span> <span class="bu">list</span>(pose_results[first_metric].keys())</span>
<span id="cb17-99"><a href="#cb17-99"></a>        </span>
<span id="cb17-100"><a href="#cb17-100"></a>        sorted_inference_times <span class="op">=</span> {}</span>
<span id="cb17-101"><a href="#cb17-101"></a>        <span class="cf">for</span> pose_estimator <span class="kw">in</span> pose_estimator_order:</span>
<span id="cb17-102"><a href="#cb17-102"></a>            <span class="cf">if</span> pose_estimator <span class="kw">in</span> inference_times:</span>
<span id="cb17-103"><a href="#cb17-103"></a>                sorted_inference_times[pose_estimator] <span class="op">=</span> inference_times[pose_estimator]</span>
<span id="cb17-104"><a href="#cb17-104"></a>        <span class="cf">return</span> sorted_inference_times</span>
<span id="cb17-105"><a href="#cb17-105"></a></span>
<span id="cb17-106"><a href="#cb17-106"></a>        </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<p>The visualizer saves the plots and tables in the <code>plots</code> folder in the checkpoint.</p>
</section>
<section id="plots" class="level3" data-number="4.4.2">
<h3 data-number="4.4.2" class="anchored" data-anchor-id="plots"><span class="header-section-number">4.4.2</span> Plots</h3>
<p>Each plot inherits from the abstract <code>Plot</code> class and implements the <code>draw</code> method. This method accepts various forms of input data, most commonly the results produced by the evaluator. Each plot can define a specific approach to aggregating and organizing the data, such as computing the median over all videos for a given pose estimator.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Plot Class
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>src/evaluation/plots/plot.py</strong></pre>
</div>
<div class="sourceCode" id="cb18" data-filename="src/evaluation/plots/plot.py"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a><span class="im">from</span> abc <span class="im">import</span> ABC, abstractmethod</span>
<span id="cb18-2"><a href="#cb18-2"></a><span class="im">from</span> typing <span class="im">import</span> Dict, Optional, Tuple</span>
<span id="cb18-3"><a href="#cb18-3"></a></span>
<span id="cb18-4"><a href="#cb18-4"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb18-5"><a href="#cb18-5"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb18-6"><a href="#cb18-6"></a><span class="im">from</span> evaluation.metrics.metric_result <span class="im">import</span> MetricResult</span>
<span id="cb18-7"><a href="#cb18-7"></a><span class="im">from</span> utils <span class="im">import</span> get_color_palette</span>
<span id="cb18-8"><a href="#cb18-8"></a></span>
<span id="cb18-9"><a href="#cb18-9"></a></span>
<span id="cb18-10"><a href="#cb18-10"></a><span class="kw">class</span> Plot(ABC):</span>
<span id="cb18-11"><a href="#cb18-11"></a>    <span class="co">"""Base class for all plots in MaskBench."""</span></span>
<span id="cb18-12"><a href="#cb18-12"></a>    </span>
<span id="cb18-13"><a href="#cb18-13"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, name: <span class="bu">str</span>, config: Optional[Dict[<span class="bu">str</span>, <span class="bu">any</span>]] <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb18-14"><a href="#cb18-14"></a>        <span class="co">"""</span></span>
<span id="cb18-15"><a href="#cb18-15"></a><span class="co">        Initialize a plot.</span></span>
<span id="cb18-16"><a href="#cb18-16"></a><span class="co">        </span></span>
<span id="cb18-17"><a href="#cb18-17"></a><span class="co">        Args:</span></span>
<span id="cb18-18"><a href="#cb18-18"></a><span class="co">            name: Unique name of the plot</span></span>
<span id="cb18-19"><a href="#cb18-19"></a><span class="co">            config: Optional configuration dictionary for the plot</span></span>
<span id="cb18-20"><a href="#cb18-20"></a><span class="co">                   Common config options:</span></span>
<span id="cb18-21"><a href="#cb18-21"></a><span class="co">                   - style: str for seaborn style (default: white)</span></span>
<span id="cb18-22"><a href="#cb18-22"></a><span class="co">                   - figsize: tuple for figure size (default: (10, 5))</span></span>
<span id="cb18-23"><a href="#cb18-23"></a><span class="co">                   - dpi: int for figure resolution (default: 300)</span></span>
<span id="cb18-24"><a href="#cb18-24"></a><span class="co">                   - title: str for plot title</span></span>
<span id="cb18-25"><a href="#cb18-25"></a><span class="co">                   - xlabel: str for x-axis label</span></span>
<span id="cb18-26"><a href="#cb18-26"></a><span class="co">                   - ylabel: str for y-axis label</span></span>
<span id="cb18-27"><a href="#cb18-27"></a><span class="co">                   - legend: bool for showing legend</span></span>
<span id="cb18-28"><a href="#cb18-28"></a><span class="co">        """</span></span>
<span id="cb18-29"><a href="#cb18-29"></a>        <span class="va">self</span>.name <span class="op">=</span> name</span>
<span id="cb18-30"><a href="#cb18-30"></a>        </span>
<span id="cb18-31"><a href="#cb18-31"></a>        <span class="va">self</span>.config <span class="op">=</span> config <span class="kw">or</span> {}</span>
<span id="cb18-32"><a href="#cb18-32"></a>        <span class="cf">if</span> <span class="st">'figsize'</span> <span class="kw">not</span> <span class="kw">in</span> <span class="va">self</span>.config:</span>
<span id="cb18-33"><a href="#cb18-33"></a>            <span class="va">self</span>.config[<span class="st">'figsize'</span>] <span class="op">=</span> (<span class="dv">10</span>, <span class="dv">5</span>)</span>
<span id="cb18-34"><a href="#cb18-34"></a>        <span class="cf">if</span> <span class="st">'dpi'</span> <span class="kw">not</span> <span class="kw">in</span> <span class="va">self</span>.config:</span>
<span id="cb18-35"><a href="#cb18-35"></a>            <span class="va">self</span>.config[<span class="st">'dpi'</span>] <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb18-36"><a href="#cb18-36"></a>        <span class="cf">if</span> <span class="st">'style'</span> <span class="kw">not</span> <span class="kw">in</span> <span class="va">self</span>.config:</span>
<span id="cb18-37"><a href="#cb18-37"></a>            <span class="va">self</span>.config[<span class="st">'style'</span>] <span class="op">=</span> <span class="st">'white'</span></span>
<span id="cb18-38"><a href="#cb18-38"></a>        <span class="va">self</span>.config[<span class="st">'palette'</span>] <span class="op">=</span> get_color_palette()</span>
<span id="cb18-39"><a href="#cb18-39"></a>        </span>
<span id="cb18-40"><a href="#cb18-40"></a>        sns.set_style(<span class="va">self</span>.config[<span class="st">'style'</span>])</span>
<span id="cb18-41"><a href="#cb18-41"></a>        sns.set_palette(<span class="va">self</span>.config[<span class="st">'palette'</span>])</span>
<span id="cb18-42"><a href="#cb18-42"></a>        sns.set_context(<span class="st">"paper"</span>)</span>
<span id="cb18-43"><a href="#cb18-43"></a>        </span>
<span id="cb18-44"><a href="#cb18-44"></a>    <span class="at">@abstractmethod</span></span>
<span id="cb18-45"><a href="#cb18-45"></a>    <span class="kw">def</span> draw(</span>
<span id="cb18-46"><a href="#cb18-46"></a>        <span class="va">self</span>,</span>
<span id="cb18-47"><a href="#cb18-47"></a>        results: Dict[<span class="bu">str</span>, Dict[<span class="bu">str</span>, Dict[<span class="bu">str</span>, MetricResult]]],</span>
<span id="cb18-48"><a href="#cb18-48"></a>        add_title: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb18-49"><a href="#cb18-49"></a>    ) <span class="op">-&gt;</span> Tuple[plt.Figure, <span class="bu">str</span>]:</span>
<span id="cb18-50"><a href="#cb18-50"></a>        <span class="co">"""</span></span>
<span id="cb18-51"><a href="#cb18-51"></a><span class="co">        Draw the plot using the provided results.</span></span>
<span id="cb18-52"><a href="#cb18-52"></a><span class="co">        </span></span>
<span id="cb18-53"><a href="#cb18-53"></a><span class="co">        Args:</span></span>
<span id="cb18-54"><a href="#cb18-54"></a><span class="co">            results: Dictionary mapping:</span></span>
<span id="cb18-55"><a href="#cb18-55"></a><span class="co">                    metric_name -&gt; model_name -&gt; video_name -&gt; MetricResult</span></span>
<span id="cb18-56"><a href="#cb18-56"></a><span class="co">            add_title: Whether to add the title to the plot (default: True)</span></span>
<span id="cb18-57"><a href="#cb18-57"></a><span class="co">            </span></span>
<span id="cb18-58"><a href="#cb18-58"></a><span class="co">        Returns:</span></span>
<span id="cb18-59"><a href="#cb18-59"></a><span class="co">            Tuple containing:</span></span>
<span id="cb18-60"><a href="#cb18-60"></a><span class="co">                - plt.Figure: The generated matplotlib figure</span></span>
<span id="cb18-61"><a href="#cb18-61"></a><span class="co">                - str: The suggested filename for saving the plot</span></span>
<span id="cb18-62"><a href="#cb18-62"></a><span class="co">        """</span></span>
<span id="cb18-63"><a href="#cb18-63"></a>        <span class="cf">pass</span></span>
<span id="cb18-64"><a href="#cb18-64"></a>    </span>
<span id="cb18-65"><a href="#cb18-65"></a>    <span class="kw">def</span> _setup_figure(<span class="va">self</span>, add_title: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span>) <span class="op">-&gt;</span> plt.Figure:</span>
<span id="cb18-66"><a href="#cb18-66"></a>        <span class="co">"""</span></span>
<span id="cb18-67"><a href="#cb18-67"></a><span class="co">        Set up the figure with standard configuration.</span></span>
<span id="cb18-68"><a href="#cb18-68"></a><span class="co">        </span></span>
<span id="cb18-69"><a href="#cb18-69"></a><span class="co">        Args:</span></span>
<span id="cb18-70"><a href="#cb18-70"></a><span class="co">            add_title: Whether to add the title to the plot (default: True)</span></span>
<span id="cb18-71"><a href="#cb18-71"></a><span class="co">        """</span></span>
<span id="cb18-72"><a href="#cb18-72"></a>        fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span><span class="va">self</span>.config[<span class="st">'figsize'</span>], dpi<span class="op">=</span><span class="va">self</span>.config[<span class="st">'dpi'</span>])</span>
<span id="cb18-73"><a href="#cb18-73"></a>        plt.tight_layout()</span>
<span id="cb18-74"><a href="#cb18-74"></a></span>
<span id="cb18-75"><a href="#cb18-75"></a>        <span class="co"># Remove plot edges</span></span>
<span id="cb18-76"><a href="#cb18-76"></a>        plt.gca().spines[<span class="st">'top'</span>].set_visible(<span class="va">False</span>)</span>
<span id="cb18-77"><a href="#cb18-77"></a>        plt.gca().spines[<span class="st">'right'</span>].set_visible(<span class="va">False</span>)</span>
<span id="cb18-78"><a href="#cb18-78"></a>        plt.gca().spines[<span class="st">'left'</span>].set_visible(<span class="va">False</span>)</span>
<span id="cb18-79"><a href="#cb18-79"></a>        plt.gca().spines[<span class="st">'bottom'</span>].set_visible(<span class="va">False</span>)</span>
<span id="cb18-80"><a href="#cb18-80"></a></span>
<span id="cb18-81"><a href="#cb18-81"></a>        </span>
<span id="cb18-82"><a href="#cb18-82"></a>        <span class="cf">if</span> add_title <span class="kw">and</span> <span class="st">'title'</span> <span class="kw">in</span> <span class="va">self</span>.config:</span>
<span id="cb18-83"><a href="#cb18-83"></a>            plt.title(<span class="va">self</span>.config[<span class="st">'title'</span>])</span>
<span id="cb18-84"><a href="#cb18-84"></a>        <span class="cf">if</span> <span class="st">'xlabel'</span> <span class="kw">in</span> <span class="va">self</span>.config:</span>
<span id="cb18-85"><a href="#cb18-85"></a>            plt.xlabel(<span class="va">self</span>.config[<span class="st">'xlabel'</span>], labelpad<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb18-86"><a href="#cb18-86"></a>        <span class="cf">if</span> <span class="st">'ylabel'</span> <span class="kw">in</span> <span class="va">self</span>.config:</span>
<span id="cb18-87"><a href="#cb18-87"></a>            plt.ylabel(<span class="va">self</span>.config[<span class="st">'ylabel'</span>], labelpad<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb18-88"><a href="#cb18-88"></a>            </span>
<span id="cb18-89"><a href="#cb18-89"></a>        <span class="cf">return</span> fig</span>
<span id="cb18-90"><a href="#cb18-90"></a></span>
<span id="cb18-91"><a href="#cb18-91"></a>    <span class="kw">def</span> _group_by_video(<span class="va">self</span>, results: Dict[<span class="bu">str</span>, Dict[<span class="bu">str</span>, Dict[<span class="bu">str</span>, MetricResult]]]) <span class="op">-&gt;</span> Dict[<span class="bu">str</span>, Dict[<span class="bu">str</span>, Dict[<span class="bu">str</span>, MetricResult]]]:</span>
<span id="cb18-92"><a href="#cb18-92"></a>        <span class="co">"""Group the results by video."""</span></span>
<span id="cb18-93"><a href="#cb18-93"></a>        video_to_models <span class="op">=</span> {}</span>
<span id="cb18-94"><a href="#cb18-94"></a>        <span class="cf">for</span> model_name, video_results <span class="kw">in</span> results.items():</span>
<span id="cb18-95"><a href="#cb18-95"></a>            <span class="cf">for</span> video_name, metric_result <span class="kw">in</span> video_results.items():</span>
<span id="cb18-96"><a href="#cb18-96"></a>                <span class="cf">if</span> video_name <span class="kw">not</span> <span class="kw">in</span> video_to_models:</span>
<span id="cb18-97"><a href="#cb18-97"></a>                    video_to_models[video_name] <span class="op">=</span> {}</span>
<span id="cb18-98"><a href="#cb18-98"></a>                video_to_models[video_name][model_name] <span class="op">=</span> metric_result</span>
<span id="cb18-99"><a href="#cb18-99"></a>        <span class="cf">return</span> video_to_models</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Kinematic Distribution Plot
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>src/evaluation/plots/kinematic_distribution_plot.py</strong></pre>
</div>
<div class="sourceCode" id="cb19" data-filename="src/evaluation/plots/kinematic_distribution_plot.py"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a><span class="im">from</span> typing <span class="im">import</span> Dict, List, Tuple</span>
<span id="cb19-2"><a href="#cb19-2"></a><span class="im">from</span> itertools <span class="im">import</span> cycle</span>
<span id="cb19-3"><a href="#cb19-3"></a></span>
<span id="cb19-4"><a href="#cb19-4"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb19-5"><a href="#cb19-5"></a><span class="im">import</span> numpy.ma <span class="im">as</span> ma</span>
<span id="cb19-6"><a href="#cb19-6"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb19-7"><a href="#cb19-7"></a></span>
<span id="cb19-8"><a href="#cb19-8"></a><span class="im">from</span> evaluation.metrics.metric_result <span class="im">import</span> COORDINATE_AXIS, MetricResult</span>
<span id="cb19-9"><a href="#cb19-9"></a><span class="im">from</span> .plot <span class="im">import</span> Plot</span>
<span id="cb19-10"><a href="#cb19-10"></a></span>
<span id="cb19-11"><a href="#cb19-11"></a></span>
<span id="cb19-12"><a href="#cb19-12"></a><span class="kw">class</span> KinematicDistributionPlot(Plot):</span>
<span id="cb19-13"><a href="#cb19-13"></a>    <span class="co">"""Plot class for visualizing kinematic distributions (velocity, acceleration, jerk) for different models."""</span></span>
<span id="cb19-14"><a href="#cb19-14"></a>    </span>
<span id="cb19-15"><a href="#cb19-15"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, metric_name: <span class="bu">str</span>, kinematic_limit: <span class="bu">float</span> <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb19-16"><a href="#cb19-16"></a>        <span class="co">"""</span></span>
<span id="cb19-17"><a href="#cb19-17"></a><span class="co">        Initialize the kinematic distribution plot.</span></span>
<span id="cb19-18"><a href="#cb19-18"></a><span class="co">        </span></span>
<span id="cb19-19"><a href="#cb19-19"></a><span class="co">        Args:</span></span>
<span id="cb19-20"><a href="#cb19-20"></a><span class="co">            metric_name: Name of the kinematic metric ('Velocity', 'Acceleration', or 'Jerk')</span></span>
<span id="cb19-21"><a href="#cb19-21"></a><span class="co">            kinematic_limit: Optional limit for the kinematic values. All values greater than this value will land in one bucket.</span></span>
<span id="cb19-22"><a href="#cb19-22"></a><span class="co">        """</span></span>
<span id="cb19-23"><a href="#cb19-23"></a>        <span class="cf">if</span> metric_name <span class="kw">not</span> <span class="kw">in</span> [<span class="st">'Velocity'</span>, <span class="st">'Acceleration'</span>, <span class="st">'Jerk'</span>]:</span>
<span id="cb19-24"><a href="#cb19-24"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"Metric name must be one of ['Velocity, 'Acceleration', 'Jerk']"</span>)</span>
<span id="cb19-25"><a href="#cb19-25"></a>            </span>
<span id="cb19-26"><a href="#cb19-26"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(</span>
<span id="cb19-27"><a href="#cb19-27"></a>            name<span class="op">=</span><span class="ss">f"</span><span class="sc">{</span>metric_name<span class="sc">}</span><span class="ss">Distribution"</span>,</span>
<span id="cb19-28"><a href="#cb19-28"></a>            config<span class="op">=</span>{</span>
<span id="cb19-29"><a href="#cb19-29"></a>                <span class="st">'title'</span>: <span class="ss">f'</span><span class="sc">{</span>metric_name<span class="sc">}</span><span class="ss"> Keypoint Distribution'</span>,</span>
<span id="cb19-30"><a href="#cb19-30"></a>                <span class="st">'xlabel'</span>: <span class="ss">f'</span><span class="sc">{</span>metric_name<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb19-31"><a href="#cb19-31"></a>                <span class="st">'ylabel'</span>: <span class="st">'Percentage'</span>,</span>
<span id="cb19-32"><a href="#cb19-32"></a>            }</span>
<span id="cb19-33"><a href="#cb19-33"></a>        )</span>
<span id="cb19-34"><a href="#cb19-34"></a>        </span>
<span id="cb19-35"><a href="#cb19-35"></a>        <span class="va">self</span>.metric_name <span class="op">=</span> metric_name</span>
<span id="cb19-36"><a href="#cb19-36"></a>        <span class="va">self</span>.unit <span class="op">=</span> <span class="va">None</span></span>
<span id="cb19-37"><a href="#cb19-37"></a>        <span class="va">self</span>.n_bins <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb19-38"><a href="#cb19-38"></a>        <span class="va">self</span>.kinematic_limit <span class="op">=</span> kinematic_limit</span>
<span id="cb19-39"><a href="#cb19-39"></a>        </span>
<span id="cb19-40"><a href="#cb19-40"></a>        <span class="co"># Define a variety of marker shapes for different models</span></span>
<span id="cb19-41"><a href="#cb19-41"></a>        <span class="co"># o: circle, s: square, ^: triangle up, v: triangle down, </span></span>
<span id="cb19-42"><a href="#cb19-42"></a>        <span class="co"># D: diamond, p: pentagon, h: hexagon, 8: octagon,</span></span>
<span id="cb19-43"><a href="#cb19-43"></a>        <span class="co"># *: star, P: plus filled</span></span>
<span id="cb19-44"><a href="#cb19-44"></a>        <span class="va">self</span>.markers <span class="op">=</span> [<span class="st">'^'</span>, <span class="st">'*'</span>, <span class="st">'h'</span>, <span class="st">'s'</span>, <span class="st">'D'</span>, <span class="st">'o'</span>, <span class="st">'p'</span>, <span class="st">'h'</span>, <span class="st">'8'</span>, <span class="st">'P'</span>]</span>
<span id="cb19-45"><a href="#cb19-45"></a></span>
<span id="cb19-46"><a href="#cb19-46"></a>    <span class="kw">def</span> _flatten_clip_validate(<span class="va">self</span>, values: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb19-47"><a href="#cb19-47"></a>        <span class="co">"""</span></span>
<span id="cb19-48"><a href="#cb19-48"></a><span class="co">        Process input values by:</span></span>
<span id="cb19-49"><a href="#cb19-49"></a><span class="co">        1. Removing masked and NaN values</span></span>
<span id="cb19-50"><a href="#cb19-50"></a><span class="co">        2. Flattening the array</span></span>
<span id="cb19-51"><a href="#cb19-51"></a><span class="co">        3. Clipping values to the kinematic limit</span></span>
<span id="cb19-52"><a href="#cb19-52"></a><span class="co">        </span></span>
<span id="cb19-53"><a href="#cb19-53"></a><span class="co">        Args:</span></span>
<span id="cb19-54"><a href="#cb19-54"></a><span class="co">            values: Input numpy masked array with potential NaN values</span></span>
<span id="cb19-55"><a href="#cb19-55"></a><span class="co">            </span></span>
<span id="cb19-56"><a href="#cb19-56"></a><span class="co">        Returns:</span></span>
<span id="cb19-57"><a href="#cb19-57"></a><span class="co">            Flattened array of valid values clipped to kinematic limit</span></span>
<span id="cb19-58"><a href="#cb19-58"></a><span class="co">        """</span></span>
<span id="cb19-59"><a href="#cb19-59"></a>        <span class="co"># Handle masked values if it's a masked array</span></span>
<span id="cb19-60"><a href="#cb19-60"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(values, ma.MaskedArray):</span>
<span id="cb19-61"><a href="#cb19-61"></a>            valid_values <span class="op">=</span> values[<span class="op">~</span>values.mask].data</span>
<span id="cb19-62"><a href="#cb19-62"></a>        <span class="cf">else</span>:</span>
<span id="cb19-63"><a href="#cb19-63"></a>            valid_values <span class="op">=</span> values</span>
<span id="cb19-64"><a href="#cb19-64"></a>            </span>
<span id="cb19-65"><a href="#cb19-65"></a>        valid_values <span class="op">=</span> valid_values[<span class="op">~</span>np.isnan(valid_values)] <span class="co"># Remove NaN values</span></span>
<span id="cb19-66"><a href="#cb19-66"></a>        flattened_values <span class="op">=</span> valid_values.flatten()</span>
<span id="cb19-67"><a href="#cb19-67"></a>        clipped_values <span class="op">=</span> np.clip(flattened_values, <span class="op">-</span><span class="va">self</span>.kinematic_limit, <span class="va">self</span>.kinematic_limit)</span>
<span id="cb19-68"><a href="#cb19-68"></a>        <span class="cf">return</span> clipped_values</span>
<span id="cb19-69"><a href="#cb19-69"></a></span>
<span id="cb19-70"><a href="#cb19-70"></a>    <span class="kw">def</span> _compute_distribution(<span class="va">self</span>, values: np.ndarray, bin_edges: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb19-71"><a href="#cb19-71"></a>        hist, _ <span class="op">=</span> np.histogram(values, bins<span class="op">=</span>bin_edges)</span>
<span id="cb19-72"><a href="#cb19-72"></a>        <span class="cf">return</span> (hist <span class="op">/</span> <span class="bu">len</span>(values)) <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb19-73"><a href="#cb19-73"></a>    </span>
<span id="cb19-74"><a href="#cb19-74"></a>    <span class="kw">def</span> _create_bin_edges_and_labels(<span class="va">self</span>) <span class="op">-&gt;</span> Tuple[np.ndarray, List[<span class="bu">str</span>]]:</span>
<span id="cb19-75"><a href="#cb19-75"></a>        <span class="co">"""</span></span>
<span id="cb19-76"><a href="#cb19-76"></a><span class="co">        Create bin edges and corresponding labels for the kinematic distribution.</span></span>
<span id="cb19-77"><a href="#cb19-77"></a><span class="co">        </span></span>
<span id="cb19-78"><a href="#cb19-78"></a><span class="co">        Returns:</span></span>
<span id="cb19-79"><a href="#cb19-79"></a><span class="co">            Tuple containing:</span></span>
<span id="cb19-80"><a href="#cb19-80"></a><span class="co">                - np.ndarray: Bin edges for histogram computation</span></span>
<span id="cb19-81"><a href="#cb19-81"></a><span class="co">                - List[str]: Human-readable labels for the bins</span></span>
<span id="cb19-82"><a href="#cb19-82"></a><span class="co">        """</span></span>
<span id="cb19-83"><a href="#cb19-83"></a>        diff <span class="op">=</span> <span class="va">self</span>.kinematic_limit <span class="op">/</span> <span class="va">self</span>.n_bins</span>
<span id="cb19-84"><a href="#cb19-84"></a>        <span class="co"># if n_bins is 10, then we want to have 11 bins, for the last bin which contains all values greater than the kinematic limit</span></span>
<span id="cb19-85"><a href="#cb19-85"></a>        <span class="co"># the last value in np.linspace is excluded (that's why we add 2 to n_bins)</span></span>
<span id="cb19-86"><a href="#cb19-86"></a>        bin_edges <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="va">self</span>.kinematic_limit <span class="op">+</span> diff, <span class="va">self</span>.n_bins <span class="op">+</span> <span class="dv">2</span>).astype(<span class="bu">int</span>)</span>
<span id="cb19-87"><a href="#cb19-87"></a>        </span>
<span id="cb19-88"><a href="#cb19-88"></a>        bin_labels <span class="op">=</span> []</span>
<span id="cb19-89"><a href="#cb19-89"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(bin_edges) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb19-90"><a href="#cb19-90"></a>            <span class="cf">if</span> i <span class="op">==</span> <span class="bu">len</span>(bin_edges) <span class="op">-</span> <span class="dv">2</span>:</span>
<span id="cb19-91"><a href="#cb19-91"></a>                bin_labels.append(<span class="ss">f'&gt; </span><span class="sc">{</span>bin_edges[i]<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb19-92"><a href="#cb19-92"></a>            <span class="cf">else</span>:</span>
<span id="cb19-93"><a href="#cb19-93"></a>                bin_labels.append(<span class="ss">f'[</span><span class="sc">{</span>bin_edges[i]<span class="sc">}</span><span class="ss">,</span><span class="sc">{</span>bin_edges[i<span class="op">+</span><span class="dv">1</span>]<span class="sc">}</span><span class="ss">]'</span>)</span>
<span id="cb19-94"><a href="#cb19-94"></a>                </span>
<span id="cb19-95"><a href="#cb19-95"></a>        <span class="cf">return</span> bin_edges, bin_labels</span>
<span id="cb19-96"><a href="#cb19-96"></a></span>
<span id="cb19-97"><a href="#cb19-97"></a>    <span class="kw">def</span> _round_to_nearest_magnitude(<span class="va">self</span>, value: <span class="bu">float</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb19-98"><a href="#cb19-98"></a>        <span class="co">"""Round a value to the nearest magnitude based on its range.</span></span>
<span id="cb19-99"><a href="#cb19-99"></a><span class="co">        </span></span>
<span id="cb19-100"><a href="#cb19-100"></a><span class="co">        For values:</span></span>
<span id="cb19-101"><a href="#cb19-101"></a><span class="co">        - Between 0-100: Round to nearest 10</span></span>
<span id="cb19-102"><a href="#cb19-102"></a><span class="co">        - Between 100-1000: Round to nearest 100 </span></span>
<span id="cb19-103"><a href="#cb19-103"></a><span class="co">        - Between 1000-10000: Round to nearest 1000</span></span>
<span id="cb19-104"><a href="#cb19-104"></a><span class="co">        And so on up to 1,000,000</span></span>
<span id="cb19-105"><a href="#cb19-105"></a><span class="co">        """</span></span>
<span id="cb19-106"><a href="#cb19-106"></a>        <span class="cf">if</span> value <span class="op">&lt;=</span> <span class="dv">0</span>:</span>
<span id="cb19-107"><a href="#cb19-107"></a>            <span class="cf">return</span> <span class="dv">0</span></span>
<span id="cb19-108"><a href="#cb19-108"></a>            </span>
<span id="cb19-109"><a href="#cb19-109"></a>        magnitude <span class="op">=</span> <span class="dv">10</span> <span class="op">**</span> (<span class="bu">len</span>(<span class="bu">str</span>(<span class="bu">int</span>(value))) <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb19-110"><a href="#cb19-110"></a>        <span class="cf">if</span> magnitude <span class="op">&lt;</span> <span class="dv">10</span>:</span>
<span id="cb19-111"><a href="#cb19-111"></a>            magnitude <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb19-112"><a href="#cb19-112"></a>        <span class="cf">return</span> np.ceil(value <span class="op">/</span> magnitude) <span class="op">*</span> magnitude</span>
<span id="cb19-113"><a href="#cb19-113"></a>    </span>
<span id="cb19-114"><a href="#cb19-114"></a>    <span class="kw">def</span> draw(</span>
<span id="cb19-115"><a href="#cb19-115"></a>        <span class="va">self</span>,</span>
<span id="cb19-116"><a href="#cb19-116"></a>        results: Dict[<span class="bu">str</span>, Dict[<span class="bu">str</span>, Dict[<span class="bu">str</span>, MetricResult]]],</span>
<span id="cb19-117"><a href="#cb19-117"></a>        add_title: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb19-118"><a href="#cb19-118"></a>    ) <span class="op">-&gt;</span> Tuple[plt.Figure, <span class="bu">str</span>]:</span>
<span id="cb19-119"><a href="#cb19-119"></a></span>
<span id="cb19-120"><a href="#cb19-120"></a>        <span class="co"># First pass: compute the magnitude of the kinematic values and take median over videos</span></span>
<span id="cb19-121"><a href="#cb19-121"></a>        pose_estimator_results <span class="op">=</span> results[<span class="va">self</span>.metric_name]</span>
<span id="cb19-122"><a href="#cb19-122"></a>        pose_estimator_medians <span class="op">=</span> {} <span class="co"># store the median for each pose estimator over all videos</span></span>
<span id="cb19-123"><a href="#cb19-123"></a>        pose_estimator_magnitude_results <span class="op">=</span> {} <span class="co"># store the magnitude results for each pose estimator</span></span>
<span id="cb19-124"><a href="#cb19-124"></a></span>
<span id="cb19-125"><a href="#cb19-125"></a>        <span class="cf">for</span> pose_estimator_name, video_results <span class="kw">in</span> pose_estimator_results.items():</span>
<span id="cb19-126"><a href="#cb19-126"></a>            <span class="va">self</span>.unit <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(video_results.values())).unit <span class="cf">if</span> <span class="va">self</span>.unit <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> <span class="va">self</span>.unit</span>
<span id="cb19-127"><a href="#cb19-127"></a></span>
<span id="cb19-128"><a href="#cb19-128"></a>            video_magnitudes <span class="op">=</span> []</span>
<span id="cb19-129"><a href="#cb19-129"></a>            pose_estimator_magnitude_results[pose_estimator_name] <span class="op">=</span> {}</span>
<span id="cb19-130"><a href="#cb19-130"></a>            <span class="cf">for</span> video_name, metric_result <span class="kw">in</span> video_results.items():</span>
<span id="cb19-131"><a href="#cb19-131"></a>                magnitude_result <span class="op">=</span> metric_result.aggregate([COORDINATE_AXIS], method<span class="op">=</span><span class="st">'vector_magnitude'</span>)</span>
<span id="cb19-132"><a href="#cb19-132"></a></span>
<span id="cb19-133"><a href="#cb19-133"></a>                video_magnitudes.append(magnitude_result.aggregate_all(method<span class="op">=</span><span class="st">'median'</span>))</span>
<span id="cb19-134"><a href="#cb19-134"></a>                pose_estimator_magnitude_results[pose_estimator_name][video_name] <span class="op">=</span> magnitude_result</span>
<span id="cb19-135"><a href="#cb19-135"></a></span>
<span id="cb19-136"><a href="#cb19-136"></a>            pose_estimator_medians[pose_estimator_name] <span class="op">=</span> np.median(video_magnitudes)</span>
<span id="cb19-137"><a href="#cb19-137"></a></span>
<span id="cb19-138"><a href="#cb19-138"></a>        <span class="cf">if</span> <span class="va">self</span>.kinematic_limit <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb19-139"><a href="#cb19-139"></a>            <span class="co"># Calculate the maximum average magnitude of all pose estimators to set the bounds of the plot</span></span>
<span id="cb19-140"><a href="#cb19-140"></a>            <span class="co"># This maximum value is increased by 20%</span></span>
<span id="cb19-141"><a href="#cb19-141"></a>            raw_limit <span class="op">=</span> <span class="bu">max</span>(pose_estimator_medians.values()) <span class="op">*</span> <span class="fl">1.20</span></span>
<span id="cb19-142"><a href="#cb19-142"></a>            <span class="va">self</span>.kinematic_limit <span class="op">=</span> <span class="va">self</span>._round_to_nearest_magnitude(raw_limit)</span>
<span id="cb19-143"><a href="#cb19-143"></a>        </span>
<span id="cb19-144"><a href="#cb19-144"></a>        <span class="cf">if</span> <span class="va">self</span>.unit:</span>
<span id="cb19-145"><a href="#cb19-145"></a>            <span class="va">self</span>.config[<span class="st">'xlabel'</span>] <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>metric_name<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>unit<span class="sc">}</span><span class="ss">)'</span></span>
<span id="cb19-146"><a href="#cb19-146"></a>        fig <span class="op">=</span> <span class="va">self</span>._setup_figure(add_title<span class="op">=</span>add_title)</span>
<span id="cb19-147"><a href="#cb19-147"></a></span>
<span id="cb19-148"><a href="#cb19-148"></a>        <span class="co"># Store lines for updating legend later</span></span>
<span id="cb19-149"><a href="#cb19-149"></a>        lines <span class="op">=</span> []</span>
<span id="cb19-150"><a href="#cb19-150"></a>        labels <span class="op">=</span> []</span>
<span id="cb19-151"><a href="#cb19-151"></a>        marker_cycle <span class="op">=</span> cycle(<span class="va">self</span>.markers)</span>
<span id="cb19-152"><a href="#cb19-152"></a>        bin_edges, bin_labels <span class="op">=</span> <span class="va">self</span>._create_bin_edges_and_labels()</span>
<span id="cb19-153"><a href="#cb19-153"></a>        </span>
<span id="cb19-154"><a href="#cb19-154"></a>        <span class="co"># Create x positions that span the full width</span></span>
<span id="cb19-155"><a href="#cb19-155"></a>        x_positions <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="bu">len</span>(bin_labels))</span>
<span id="cb19-156"><a href="#cb19-156"></a>        </span>
<span id="cb19-157"><a href="#cb19-157"></a>        <span class="co"># Second pass: flatten and clip the values</span></span>
<span id="cb19-158"><a href="#cb19-158"></a>        <span class="cf">for</span> model_name, video_results <span class="kw">in</span> pose_estimator_results.items():</span>
<span id="cb19-159"><a href="#cb19-159"></a>            model_values <span class="op">=</span> []</span>
<span id="cb19-160"><a href="#cb19-160"></a>            <span class="cf">for</span> metric_result <span class="kw">in</span> video_results.values():</span>
<span id="cb19-161"><a href="#cb19-161"></a>                values <span class="op">=</span> metric_result.values</span>
<span id="cb19-162"><a href="#cb19-162"></a>                flattened_valid_clipped_vals <span class="op">=</span> <span class="va">self</span>._flatten_clip_validate(values)</span>
<span id="cb19-163"><a href="#cb19-163"></a>                model_values.extend(np.<span class="bu">abs</span>(flattened_valid_clipped_vals.flatten()))</span>
<span id="cb19-164"><a href="#cb19-164"></a>                </span>
<span id="cb19-165"><a href="#cb19-165"></a>            distribution <span class="op">=</span> <span class="va">self</span>._compute_distribution(model_values, bin_edges)</span>
<span id="cb19-166"><a href="#cb19-166"></a>            </span>
<span id="cb19-167"><a href="#cb19-167"></a>            marker <span class="op">=</span> <span class="bu">next</span>(marker_cycle)</span>
<span id="cb19-168"><a href="#cb19-168"></a>            plt.plot(x_positions, distribution, </span>
<span id="cb19-169"><a href="#cb19-169"></a>                    marker<span class="op">=</span>marker,</span>
<span id="cb19-170"><a href="#cb19-170"></a>                    markersize<span class="op">=</span><span class="dv">6</span>,</span>
<span id="cb19-171"><a href="#cb19-171"></a>            )</span>
<span id="cb19-172"><a href="#cb19-172"></a>            </span>
<span id="cb19-173"><a href="#cb19-173"></a>            scatter <span class="op">=</span> plt.scatter([], [], </span>
<span id="cb19-174"><a href="#cb19-174"></a>                                marker<span class="op">=</span>marker,</span>
<span id="cb19-175"><a href="#cb19-175"></a>                                s<span class="op">=</span><span class="dv">36</span>,</span>
<span id="cb19-176"><a href="#cb19-176"></a>                                label<span class="op">=</span>model_name,</span>
<span id="cb19-177"><a href="#cb19-177"></a>                                color<span class="op">=</span>plt.gca().lines[<span class="op">-</span><span class="dv">1</span>].get_color())</span>
<span id="cb19-178"><a href="#cb19-178"></a>            lines.append(scatter)</span>
<span id="cb19-179"><a href="#cb19-179"></a>            labels.append(model_name)</span>
<span id="cb19-180"><a href="#cb19-180"></a>        </span>
<span id="cb19-181"><a href="#cb19-181"></a>        <span class="va">self</span>._finish_label_grid_axes_styling(x_positions, bin_labels, lines, labels)</span>
<span id="cb19-182"><a href="#cb19-182"></a>        <span class="cf">return</span> fig, <span class="ss">f"</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>metric_name<span class="sc">.</span>lower()<span class="sc">}</span><span class="ss">_distribution"</span></span>
<span id="cb19-183"><a href="#cb19-183"></a></span>
<span id="cb19-184"><a href="#cb19-184"></a>    <span class="kw">def</span> _finish_label_grid_axes_styling(<span class="va">self</span>, x_positions: np.ndarray, bin_labels: List[<span class="bu">str</span>], lines: List[plt.Line2D], labels: List[<span class="bu">str</span>]):</span>
<span id="cb19-185"><a href="#cb19-185"></a>        <span class="co"># Configure x-axis</span></span>
<span id="cb19-186"><a href="#cb19-186"></a>        plt.xlim(<span class="op">-</span><span class="fl">0.01</span>, <span class="fl">1.01</span>)</span>
<span id="cb19-187"><a href="#cb19-187"></a>        plt.xticks(x_positions, bin_labels, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb19-188"><a href="#cb19-188"></a>        </span>
<span id="cb19-189"><a href="#cb19-189"></a>        <span class="co"># Configure y-axis</span></span>
<span id="cb19-190"><a href="#cb19-190"></a>        y_ticks <span class="op">=</span> np.arange(<span class="dv">0</span>, <span class="dv">90</span>, <span class="dv">20</span>)</span>
<span id="cb19-191"><a href="#cb19-191"></a>        plt.yticks(y_ticks, [<span class="ss">f'</span><span class="sc">{</span>x<span class="sc">:.2f}</span><span class="ss"> %'</span> <span class="cf">for</span> x <span class="kw">in</span> y_ticks])</span>
<span id="cb19-192"><a href="#cb19-192"></a>        plt.ylim(<span class="dv">0</span>, <span class="dv">90</span>)</span>
<span id="cb19-193"><a href="#cb19-193"></a>        </span>
<span id="cb19-194"><a href="#cb19-194"></a>        <span class="co"># Configure grid</span></span>
<span id="cb19-195"><a href="#cb19-195"></a>        plt.grid(<span class="va">True</span>, axis<span class="op">=</span><span class="st">'y'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, color<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb19-196"><a href="#cb19-196"></a>        plt.grid(<span class="va">False</span>, axis<span class="op">=</span><span class="st">'x'</span>)</span>
<span id="cb19-197"><a href="#cb19-197"></a>        </span>
<span id="cb19-198"><a href="#cb19-198"></a>        <span class="co"># Ensure all text is black and properly sized</span></span>
<span id="cb19-199"><a href="#cb19-199"></a>        plt.tick_params(colors<span class="op">=</span><span class="st">'black'</span>, which<span class="op">=</span><span class="st">'both'</span>)</span>
<span id="cb19-200"><a href="#cb19-200"></a>        <span class="cf">for</span> label <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb19-201"><a href="#cb19-201"></a>            label.set_color(<span class="st">'black'</span>)</span>
<span id="cb19-202"><a href="#cb19-202"></a>        </span>
<span id="cb19-203"><a href="#cb19-203"></a>        plt.legend(lines, labels)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<p>We provide the following plots and tables:</p>
<ul>
<li><strong>Kinematic Distribution Plot</strong>: Visualizes the distribution of kinematic values for each pose estimator.</li>
<li><strong>Per Keypoint Plot</strong>: Displays the median kinematic metric values or Euclidean distance for each COCO keypoint. This plot requires keypoints to be stored in COCO format.</li>
<li><strong>Inference Time Plot</strong>: Visualizes the average inference time associated with each pose estimator.</li>
<li><strong>Result Table</strong>: Aggregates results per metric and pose estimator across all videos, presenting the data in tabular form.</li>
</ul>
</section>
</section>
<section id="sec-architecture-rendering" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="sec-architecture-rendering"><span class="header-section-number">4.5</span> Rendering</h2>
<p>Video rendering is handled by the <code>Renderer</code> class. For each video in the dataset, the renderer creates a dedicated folder within the <code>renderings</code> directory of the checkpoint folder. Inside each video folder, it generates one video per pose estimator, displaying the rendered keypoints. Special attention was given to maintaining consistent colors for each pose estimator across all videos and plots, using a predefined, color-blind–friendly palette.</p>
</section>
</section>
<section id="sec-datasets" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Datasets</h1>
<p>This study uses four video-based datasets, each representing a different level of complexity, from simple, controlled settings to more dynamic and interactive scenarios. To capture this range, we selected or created four distinct datasets: TED Kid Video <span class="citation" data-cites="ted-kid">(<a href="#ref-ted-kid" role="doc-biblioref">Allen 2017</a>)</span>, TED Talks <span class="citation" data-cites="ted">(<a href="#ref-ted" role="doc-biblioref"><span>“TED”</span> 2025</a>)</span>, Tragic Talkers <span class="citation" data-cites="tragic-talkers">(<a href="#ref-tragic-talkers" role="doc-biblioref">Berghi, Volino, and Jackson 2022</a>)</span>, and a masked video dataset. Each dataset was chosen based on specific criteria to evaluate pose estimation models under varying degrees of difficulty.</p>
<section id="sec-datasets-ted-kid" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="sec-datasets-ted-kid"><span class="header-section-number">5.1</span> TED Kid Video</h2>
<p>The TED kid video is a short, 10-second clip featuring a child in a well-lit environment from the TEDx talk “Education for all” <span class="citation" data-cites="ted-kid">(<a href="#ref-ted-kid" role="doc-biblioref">Allen 2017</a>)</span>. Throughout the sequence, all body parts remain clearly visible, with no occlusion or obstruction of the subject. This video represents a controlled scenario designed to evaluate pose estimation methods under ideal conditions, serving as a baseline for comparison with more complex datasets.</p>
<p>We first tested our models’ performance and evaluation metrics on this video to verify that our metrics function correctly in an ideal setting and that the implementation is accurate. This initial validation ensures that subsequent experiments on more challenging datasets can be interpreted with confidence in the correctness of our evaluation pipeline.</p>
</section>
<section id="sec-datasets-ted-talks" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="sec-datasets-ted-talks"><span class="header-section-number">5.2</span> TED Talks</h2>
<p>For the TED talks dataset <span class="citation" data-cites="ted">(<a href="#ref-ted" role="doc-biblioref"><span>“TED”</span> 2025</a>)</span>, we selected ten videos featuring diverse speakers to capture a wide range of conditions. The selection criteria included speaker gender, skin tone, clothing types (e.g., long dresses versus short garments), partial occlusion, and videos where only specific body parts—such as hands, upper body, or lower body—are visible. We also considered variations in movement style and speed.</p>
<p>Our focus was on evaluating model performance under more complex conditions, such as scene changes, background noise (e.g., audience sounds), partial visibility of the body, and situations where body parts are difficult to distinguish (e.g., due to long dresses). We also accounted for visual distractions like images or patterns on the speaker’s clothing. In each TED talk video, our analysis concentrates solely on the primary speaker.</p>
</section>
<section id="sec-datasets-tragic-talkers" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="sec-datasets-tragic-talkers"><span class="header-section-number">5.3</span> Tragic Talkers</h2>
<p>We aimed to evaluate model performance in scenarios involving multiple people interacting. The Tragic Talkers dataset <span class="citation" data-cites="tragic-talkers">(<a href="#ref-tragic-talkers" role="doc-biblioref">Berghi, Volino, and Jackson 2022</a>)</span> was chosen because it provides 2D pseudo-ground truth annotations generated by the OpenPose AI model, allowing us to test metrics such as PCK or RMSE.</p>
<p>The dataset features a man in regular clothing and a woman wearing a long dress. It contains four distinct video scenarios, each originally recorded from twenty-two different camera angles. For our analysis, we used only four angles, as many viewpoints were too similar.</p>
<ul>
<li><strong>Monologue (Male and Female):</strong> Individual speakers deliver monologues with relatively simple and slow movements.</li>
<li><strong>Conversation:</strong> A male and female speaker engage in dialogue with limited movement.</li>
<li><strong>Interactive 1:</strong> A conversation between a male and female speaker that includes physical interaction (e.g., hand contact), with the man sitting close to the woman.</li>
<li><strong>Interactive 4:</strong> A more dynamic dialogue featuring faster movements, partial occlusion, and moments of full occlusion.</li>
</ul>
<p>These scenarios were chosen to reflect a variety of real-world human interactions, allowing us to test how well pose estimation models perform under conditions such as occlusion, multi-person scenes, and varied movement patterns.</p>
<p>However, the ground truth pose estimations were produced by an AI model rather than human annotators, which is why they are imperfect and should not be considered a true ground truth baseline (we refer to it as pseudo-ground truth). Because the original study does not specify which OpenPose variant, parameters, or post-processing steps were used, the PCK and RMSE accuracy values should be interpreted as a measure of how closely pose estimators replicate the OpenPose output rather than as an absolute indicator of pose estimation quality. In this context, a PCK accuracy above 80% is considered a good result, indicating that poses are generally well estimated.</p>
</section>
<section id="sec-datasets-masked-video" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="sec-datasets-masked-video"><span class="header-section-number">5.4</span> Masked Video Dataset</h2>
<p>The masked video dataset is a collection of three videos. It includes the TED kid video, a segment from the TED talk “Let curiosity lead” <span class="citation" data-cites="ted-curiosity">(<a href="#ref-ted-curiosity" role="doc-biblioref">Shahidi 2023</a>)</span>, and the video “interactive1_t1-cam06” from the Tragic Talkers dataset. This dataset was created to evaluate the performance of pose estimators on masked videos, addressing the challenge of sharing datasets containing sensitive information among researchers.</p>
<p>For the dataset creation, we used MaskAnyoneUI to manually mask the persons of interest in each video using four different hiding strategies: blurring, pixelation, contours, and solid fill. Including the original unmasked videos, this resulted in a total of 15 videos for evaluation.</p>
</section>
<section id="data-preprocessing" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="data-preprocessing"><span class="header-section-number">5.5</span> Data Preprocessing</h2>
<p>Data preprocessing was carried out on the TED talks to remove unnecessary parts of the videos and to split them into shorter segments compatible with MaskAnyone. Since MaskAnyone cannot process videos longer than 2.5 minutes, and is already resource-intensive even at that limit, we divided the TED Talk videos into chunks of 30 or 50 seconds, depending on the content.</p>
<p>TED talks also showed some inconsistency in structure. Some videos were straightforward, with only the speaker and audience visible, making them easy to segment at any point. However, others included additional visual content such as slides, pictures, or unrelated scenes, which made it more difficult to determine clean chunking points.</p>
<p>For these more complex videos, we carefully selected segment boundaries to ensure that each chunk started with frames where a human was clearly visible. When necessary, we manually trimmed the beginning of chunks to avoid starting with empty or unrelated frames. This step was critical because if a video starts with non-human content, MaskAnyone may incorrectly classify objects in the first frame as humans and then continue misdetecting them in subsequent frames.</p>
<p>No preprocessing was required for the Tragic Talkers dataset, as the videos were already clean and free of noise or unrelated visual content.</p>
</section>
</section>
<section id="sec-metrics" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Evaluation Metrics</h1>
<p>In the following sections, we outline the metrics used for evaluating accuracy, smoothness and jitter of different pose estimators.</p>
<section id="ground-truth-metrics" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="ground-truth-metrics"><span class="header-section-number">6.1</span> Ground-Truth Metrics</h2>
<p>The metrics in this section are based on ground truth data provided by the dataset and primarily evaluate the accuracy of the pose estimation compared to the reference ground truth.</p>
<section id="sec-metrics-euclidean-distance" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="sec-metrics-euclidean-distance"><span class="header-section-number">6.1.1</span> Euclidean Distance</h3>
<p>The Euclidean distance metric measures the spatial accuracy of pose estimation by calculating the normalized distance between predicted and ground truth keypoint positions. For each keypoint of a person in a frame, it computes the L2 norm (Euclidean distance) between the predicted position <span class="math inline">\((x_p, y_p)\)</span> and the ground truth position <span class="math inline">\((x_{gt}, y_{gt})\)</span>:</p>
<p><span class="math display">\[ d = \frac{\sqrt{(x_p - x_{gt})^2 + (y_p - y_{gt})^2}}{s} \]</span></p>
<p>where <span class="math inline">\(s\)</span> is a normalization factor. Normalization is essential to make the metric scale-invariant and comparable across persons of different sizes.</p>
<p>The metric is set up to support three normalization strategies, out of which we only implemented the bounding box normalization. We outline future work for the implementation of head and torso normalization in section <a href="#sec-future-work" class="quarto-xref">Section&nbsp;9</a>.</p>
<ol type="1">
<li><strong>Bounding Box Size</strong>: The distance is normalized by the maximum of the width and height of the person’s bounding box, computed from the ground truth keypoints. This approach adapts to varying person sizes but may introduce minor pose-dependent scaling variance.</li>
<li><strong>Head Size</strong>: Normalization by the head bone link size (not implemented).</li>
<li><strong>Torso Size</strong>: Normalization by the torso diameter (not implemented).</li>
</ol>
<p>Head and torso normalization address the pose-dependent scaling variance of the bounding box normalization. The metric also accounts for several edge cases to ensure robust evaluation:</p>
<ul>
<li><strong>Different Order of Persons</strong>: The metric uses the Hungarian algorithm as described in section <a href="#sec-architecture-evaluation-metric" class="quarto-xref">Section&nbsp;4.3.1</a> to match person indices between ground truth and predictions, ensuring that distances are calculated between corresponding persons even if they appear in different orders.</li>
<li><strong>Keypoint Missing in Ground Truth but not in Prediction</strong>: When a keypoint is absent in the ground truth (coordinates <code>(0,0)</code>) but detected in the prediction, the distance is set to <code>NaN</code> and excluded from aggregation, as no valid ground truth reference exists.</li>
<li><strong>Keypoint Missing in Prediction but Present in Ground Truth</strong>: When a keypoint exists in the ground truth but is missing in the prediction, the distance is assigned a predetermined large fill value (here, 1). This penalizes missing detections while preventing disproportionate impact on aggregated results.</li>
<li><strong>Undetected Persons</strong>: If a person in the ground truth is completely undetected in the prediction, all their keypoint distances are set to the same fill value to penalize the failure.</li>
</ul>
<p>Euclidean distance forms the basis for computing the Percentage of Correct Keypoints (PCK) and Root Mean Square Error (RMSE) metrics.</p>
</section>
<section id="sec-metrics-pck" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="sec-metrics-pck"><span class="header-section-number">6.1.2</span> Percentage of Keypoints (PCK)</h3>
<p>The Percentage of Correct Keypoints (PCK) metric evaluates pose estimation accuracy by calculating the proportion of predicted keypoints whose normalized Euclidean distance to the ground truth falls within a specified threshold. A keypoint is considered “correct” if its distance is below this threshold, allowing PCK to quantify the reliability of pose predictions at the chosen precision level.</p>
<p>For each frame, PCK is calculated as:</p>
<p><span class="math display">\[
PCK = \frac{\text{number of keypoints with distance &lt; threshold}}{\text{total number of valid keypoints}}
\]</span></p>
<p>PCK values range from zero to one, where one indicates perfect predictions (all keypoints are within the threshold) and zero indicates complete failure (no keypoints within the threshold).</p>
</section>
<section id="sec-metrics-rmse" class="level3" data-number="6.1.3">
<h3 data-number="6.1.3" class="anchored" data-anchor-id="sec-metrics-rmse"><span class="header-section-number">6.1.3</span> Root Mean Square Error (RMSE)</h3>
<p>The Root Mean Square Error (RMSE) provides a single aggregated measure of pose estimation accuracy by calculating the root mean square of normalized Euclidean distances across all valid keypoints and persons in a frame. RMSE is defined as:</p>
<p><span class="math display">\[
RMSE = \sqrt{\frac{1}{N} \sum_{i=1}^{N} d_i^2}
\]</span></p>
<p>where <span class="math inline">\(N\)</span> is the total number of valid keypoints in the frame, and <span class="math inline">\(d_i\)</span> is the normalized Euclidean distance of keypoint <span class="math inline">\(i\)</span>. By squaring the distances before averaging, RMSE penalizes larger errors more heavily, making it particularly sensitive to outliers.</p>
</section>
</section>
<section id="kinematic-metrics" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="kinematic-metrics"><span class="header-section-number">6.2</span> Kinematic Metrics</h2>
<p>Velocity, acceleration, and jerk are key kinematic metrics that help identify unnatural or erratic movements in pose estimations by highlighting rapid changes in motion.</p>
<section id="sec-metrics-velocity" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="sec-metrics-velocity"><span class="header-section-number">6.2.1</span> Velocity</h3>
<p>The velocity metric measures the rate of change in keypoint positions between consecutive frames. For each keypoint of a person, it quantifies how quickly the keypoint moves in pixels per frame, providing insight into the smoothness and temporal consistency of the pose estimation.</p>
<p>The velocity calculation proceeds in three steps:</p>
<ol type="1">
<li>Person indices are matched between consecutive frames (as described in <a href="#sec-architecture-evaluation-metric" class="quarto-xref">Section&nbsp;4.3.1</a>) to ensure tracking of the same individual over time.</li>
<li>The velocity is then computed with <span class="math inline">\(v_t = p_{t+1} - p_t\)</span> as the difference between keypoint positions in consecutive frames, where <span class="math inline">\(p_t\)</span> represents the keypoint position at frame <span class="math inline">\(t\)</span>, and <span class="math inline">\(v_t\)</span> is the resulting velocity vector.</li>
<li>Finally, the metric can be configured to report velocities in either pixels per frame or pixels per second. In the latter case, the frame-based velocity is divided by the time delta between frames (1/fps).</li>
</ol>
<p>The metric robustly handles several edge cases:</p>
<ul>
<li>For videos with fewer than two frames, velocity cannot be computed, and the metric returns <code>NaN</code> values.</li>
<li>If a keypoint is missing in either of two consecutive frames, the corresponding velocity is set to <code>NaN</code>.</li>
<li>Since velocity is derived from frame-to-frame differences, the output contains one fewer frame than the input video.</li>
<li>The output includes a coordinate axis (x and y) representing the velocity vector, which serves as a basis for the computation of the acceleration and jerk metrics. For evaluation and visualization, aggregate along this axis using the <code>vector_magnitude</code> method to obtain scalar velocity values.</li>
</ul>
</section>
<section id="sec-metrics-acceleration" class="level3" data-number="6.2.2">
<h3 data-number="6.2.2" class="anchored" data-anchor-id="sec-metrics-acceleration"><span class="header-section-number">6.2.2</span> Acceleration</h3>
<p>The acceleration metric measures the rate of change in velocity over time, representing how quickly the movement speed of keypoints changes. It is computed by <span class="math inline">\(a_t = v_{t+1} - v_t\)</span>, where <span class="math inline">\(a_t\)</span> is the acceleration at time <span class="math inline">\(t\)</span>, <span class="math inline">\(v_t\)</span> represents the velocity, and <span class="math inline">\(p_t\)</span> the keypoint position. Acceleration values can be reported in either pixels per frame squared or pixels per second squared, with the latter requiring normalization by the squared time delta between frames (1/fps²).</p>
</section>
<section id="sec-metrics-jerk" class="level3" data-number="6.2.3">
<h3 data-number="6.2.3" class="anchored" data-anchor-id="sec-metrics-jerk"><span class="header-section-number">6.2.3</span> Jerk</h3>
<p>Jerk measures the rate of change of acceleration, offering insights into the smoothness and abruptness of motion by quantifying how quickly acceleration varies. It is calculated with <span class="math inline">\(j_t = a_{t+1} - a_t\)</span> as the difference between consecutive acceleration values, where <span class="math inline">\(j_t\)</span> is the jerk at time <span class="math inline">\(t\)</span> and <span class="math inline">\(a_t\)</span> represents the acceleration. The metric supports reporting in pixels per frame cubed or pixels per second cubed, with the latter normalized by the cubed time delta between frames (1/fps³).</p>
</section>
</section>
</section>
<section id="sec-experiments" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Experimental Setup</h1>
<p>In this section, we describe the experimental setup used to evaluate pose estimators across four datasets.</p>
<p><strong>General Setup</strong></p>
<p>We evaluated seven pose estimators on the four datasets: TED Kid Video, TED Talks, Tragic Talkers, and the Masked Video Dataset. The pose estimators are: YoloPose (v11-l), MediaPipePose (pose_landmarker_heavy), OpenPose (body_25), MaskAnyoneAPI-MediaPipe, MaskAnyoneAPI-OpenPose, MaskAnyoneUI-MediaPipe, and MaskAnyoneUI-OpenPose. Confidence thresholds were visually determined on a subset of videos and set to 0.3 for YoloPose and MediaPipePose, and 0.15 for OpenPose. Since MaskAnyone-based estimators do not output confidence scores, a threshold of zero was used for them. All keypoints were stored in COCO format to enable per-keypoint comparisons across models.</p>
<p><strong>TED Kid Video and TED Talks</strong></p>
<p>For both datasets, we evaluated the kinematic metrics velocity, acceleration, and jerk for each pose estimator. Due to the absence of ground truth annotations for TED Talks, accuracy metrics could not be computed.</p>
<p><strong>Tragic Talkers</strong></p>
<p>For the Tragic Talkers dataset, we evaluated both accuracy metrics (Euclidean distance, PCK, RMSE) and kinematic metrics (velocity, acceleration, jerk) for each pose estimator.</p>
<p><strong>Inference on Raw vs.&nbsp;Masked Videos</strong></p>
<p>For the Masked Video dataset, inference was first performed on the raw videos with all pose estimators to establish a baseline. Subsequently, inference was repeated on videos masked with different hiding strategies. Performance was compared against the baseline to assess the impact of masking, using PCK and RMSE metrics to quantify accuracy relative to raw videos.</p>
<p>Note that this masking evaluation pipeline is a preliminary implementation outside of MaskBench’s native capabilities, serving as a proof of concept. We intend to integrate full support for this workflow in future MaskBench releases (see section <a href="#sec-future-work-pipelining" class="quarto-xref">Section&nbsp;9.2.1</a>).</p>
</section>
<section id="sec-results" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> Results</h1>
<p>We present the experimental results below. To improve readability, kinematic metrics are reported without units in the text: velocity is in pixels/frame, acceleration in pixels/frame², and jerk in pixels/frame³. Our analysis focuses primarily on acceleration and jerk, instead of velocity, as these metrics are more suited to detecting instability and unnatural motion in pose estimation.</p>
<section id="sec-results-ted-kid" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="sec-results-ted-kid"><span class="header-section-number">8.1</span> TED Kid Video</h2>
<p><a href="#tbl-results-ted-kid" class="quarto-xref">Table&nbsp;1</a> summarizes the average velocity, acceleration, and jerk for each pose estimator on the TED aid video. Standard pose estimation models like YoloPose, MediaPipePose, and OpenPose exhibit relatively high values across all metrics, indicating more erratic and less stable pose estimations. MediaPipePose has the highest values for velocity (3.36), acceleration (4.10), and jerk (5.56).</p>
<p>In contrast, all evaluated MaskAnyone pose estimators show consistently lower acceleration and jerk, with MaskAnyoneUI-MediaPipe achieving the best results (velocity: 1.97, acceleration: 1.20, jerk: 1.89), representing reductions of 40%, 70%, and 66% respectively, compared to pure MediaPipePose. This indicates substantially smoother and more stable pose tracking over time. The improvements are more pronounced for MediaPipePose than OpenPose: MaskAnyone reduces MediaPipePose’s acceleration and jerk by 2.9 and 5.31, while OpenPose sees smaller decreases of 1.11 and 2.81, demonstrating the greater effectiveness of MaskAnyone with MediaPipePose.</p>
<div id="tbl-results-ted-kid" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-results-ted-kid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="results table" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Pose Estimator</th>
<th data-quarto-table-cell-role="th">Velocity</th>
<th data-quarto-table-cell-role="th">Acceleration</th>
<th data-quarto-table-cell-role="th">Jerk</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>YoloPose</td>
<td>2.81</td>
<td>2.95</td>
<td>5.04</td>
</tr>
<tr class="even">
<td>MediaPipePose</td>
<td>3.36</td>
<td>4.10</td>
<td>7.18</td>
</tr>
<tr class="odd">
<td>OpenPose</td>
<td>2.71</td>
<td>3.20</td>
<td>5.56</td>
</tr>
<tr class="maskanyone-api border-top even">
<td>MaskAnyoneAPI-MediaPipe</td>
<td class="second">2.00</td>
<td class="second">1.21</td>
<td class="best">1.87</td>
</tr>
<tr class="maskanyone-api border-bottom odd">
<td>MaskAnyoneAPI-OpenPose</td>
<td>2.73</td>
<td>2.46</td>
<td>3.53</td>
</tr>
<tr class="maskanyone-ui border-top even">
<td>MaskAnyoneUI-MediaPipe</td>
<td class="best">1.97</td>
<td class="best">1.20</td>
<td class="second">1.89</td>
</tr>
<tr class="maskanyone-ui border-bottom odd">
<td>MaskAnyoneUI-OpenPose</td>
<td>2.62</td>
<td>2.09</td>
<td>2.75</td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-results-ted-kid-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Average metric results for different pose estimators on the TED kid video.
</figcaption>
</figure>
</div>
<p><a href="#fig-ted-kid-acceleration-distribution" class="quarto-xref">Figure&nbsp;3 (a)</a> and <a href="#fig-ted-kid-jerk-distribution" class="quarto-xref">Figure&nbsp;3 (b)</a> present the distribution of the acceleration and jerk metrics for the different pose estimators. These plots show the percentage of keypoints within fixed value ranges for the acceleration and jerk metrics over all frames. The ideal curve for a stable pose estimation follows an exponential decay curve, with most kinematic values near zero and a few large values. Both plots confirm the results from <a href="#tbl-results-ted-kid" class="quarto-xref">Table&nbsp;1</a>, showing that the MaskAnyone pose estimators have a very high concentration of low acceleration and jerk values. The UI and API variants of MaskAnyone-MediaPipe most closely resemble the ideal curve, with over 80% of keypoints having acceleration below 1 pixel/frame². MaskAnyone-OpenPose estimators rank third and fourth, with around 57% of keypoints below this threshold. YoloPose ranks fifth with 52%, followed by OpenPose at 40%. MediaPipePose is the most unstable, with only 30% of keypoints below one pixel/frame² and a relatively flat distribution curve. Similar patterns can be observed for the jerk distribution.</p>
<p><a href="#fig-ted-kid-acceleration-per-keypoint" class="quarto-xref">Figure&nbsp;4 (c)</a> shows the median acceleration per keypoint for the different pose estimators. Each keypoint contains a set of seven bars, one for each pose estimator, indicating the median acceleration value for that keypoint and pose estimator. The first notable finding is that keypoints like the wrists, elbows, hips, and ankles exhibit consistently higher median acceleration compared to more stable points like the eyes, ears, and nose. This aligns with expectations since these joints undergo more frequent and pronounced movement. Secondly, MaskAnyoneAPI-MediaPipe and MaskAnyoneUI-MediaPipe consistently achieve the lowest acceleration values across all keypoints. Both MaskAnyoneUI variants improve upon their default counterparts, MediaPipePose and OpenPose, for every keypoint. The most pronounced gains appear at the hips, knees, and ankles, where MaskAnyoneUI-MediaPipe reduces median acceleration from about six pixels/frame² down to less than one.</p>
<div id="fig-ted-kid-plots" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ted-kid-plots-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-ted-kid-plots" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-ted-kid-acceleration-distribution" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-ted-kid-acceleration-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="plots/TED-kid/acceleration_distribution.png" class="img-fluid figure-img" data-ref-parent="fig-ted-kid-plots">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-ted-kid-acceleration-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Acceleration Distribution
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-ted-kid-plots" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-ted-kid-jerk-distribution" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-ted-kid-jerk-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="plots/TED-kid/jerk_distribution.png" class="img-fluid figure-img" data-ref-parent="fig-ted-kid-plots">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-ted-kid-jerk-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Jerk Distribution
</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-ted-kid-plots" style="flex-basis: 100.0%;justify-content: flex-start;">
<div id="fig-ted-kid-acceleration-per-keypoint" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-ted-kid-acceleration-per-keypoint-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="plots/TED-kid/keypoint_plot_acceleration.png" class="img-fluid figure-img" data-ref-parent="fig-ted-kid-plots">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-ted-kid-acceleration-per-keypoint-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) Median Acceleration per Keypoint
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ted-kid-plots-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: <strong>Comparison of pose estimation models on the TED-kid video.</strong> (a, b) Acceleration and jerk distribution showing the proportion of keypoints within a fixed value range. A larger concentration at lower values indicates a smoother and more stable pose, whereas a high number of keypoints with large values indicates more jittery movements. (c) Median acceleration per keypoint, indicating stability across individual body parts. Keypoints like the wrist, elbow, and ankle are expected to have a higher median acceleration than other body parts, which tend to be more stable during movements, like the eyes, ears, and nose.
</figcaption>
</figure>
</div>
<p>Last but not least, it is important to not only evaluate pose estimation results analytically but also to visually inspect pose quality. <a href="#tbl-results-ted-kid-videos" class="quarto-xref">Table&nbsp;2</a> shows rendered videos for the seven pose estimators on the TED kid video.</p>
<p>In the MediaPipePose video, the pose estimation appears unstable, showing more jitter and sudden pose changes. At the beginning, the model fails to detect the right elbow joint, which all other estimators detect correctly. Additionally, the hips, ankles, and elbows display rapid, jerky movements throughout the video.</p>
<p>Comparing MaskAnyoneUI-MediaPipe and MaskAnyoneAPI-MediaPipe rendered videos reveals that both are considerably more stable and smoother than pure MediaPipePose. Aside from the person’s natural movement, key points generally remain fixed and steady.</p>
<p>Observing the other pose estimators shows that none are as stable as MaskAnyoneUI-MediaPipe, but most outperform pure MediaPipePose in stability. This visual evidence supports the quantitative results in <a href="#tbl-results-ted-kid" class="quarto-xref">Table&nbsp;1</a> and <a href="#fig-ted-kid-plots" class="quarto-xref">Figure&nbsp;3</a>. It also confirms that our kinematic metrics effectively indicate pose estimation stability.</p>
<div id="tbl-results-ted-kid-videos" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-results-ted-kid-videos-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="video-table table" data-quarto-postprocess="true">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<tbody>
<tr class="odd">
<td><div class="video-zoom-wrapper ted-kid-video">
<video class="video-zoom ted-kid-sync" autoplay="" muted="" playsinline="">
<source src="videos/TED-kid/TED-kid.mp4" type="video/mp4">
</video>
</div>
<div class="video-caption">
Raw Video
</div></td>
<td><div class="video-zoom-wrapper ted-kid-video">
<video class="video-zoom ted-kid-sync" autoplay="" muted="" playsinline="">
<source src="videos/TED-kid/TED-kid_YoloPose.mp4" type="video/mp4">
</video>
</div>
<div class="video-caption">
YOLOPose
</div></td>
</tr>
<tr class="even">
<td><div class="video-zoom-wrapper ted-kid-video">
<video class="video-zoom ted-kid-sync" autoplay="" muted="" playsinline="">
<source src="videos/TED-kid/TED-kid_MediaPipePose.mp4" type="video/mp4">
</video>
</div>
<div class="video-caption">
MediaPipe Pose
</div></td>
<td><div class="video-zoom-wrapper ted-kid-video">
<video class="video-zoom ted-kid-sync" autoplay="" muted="" playsinline="">
<source src="videos/TED-kid/TED-kid_OpenPose.mp4" type="video/mp4">
</video>
</div>
<div class="video-caption">
OpenPose
</div></td>
</tr>
<tr class="odd">
<td><div class="video-zoom-wrapper ted-kid-video">
<video class="video-zoom ted-kid-sync" autoplay="" muted="" playsinline="">
<source src="videos/TED-kid/TED-kid_MaskAnyoneAPI-MediaPipe.mp4" type="video/mp4">
</video>
</div>
<div class="video-caption">
MaskAnyoneAPI-MediaPipe
</div></td>
<td><div class="video-zoom-wrapper ted-kid-video">
<video class="video-zoom ted-kid-sync" autoplay="" muted="" playsinline="">
<source src="videos/TED-kid/TED-kid_MaskAnyoneAPI-OpenPose.mp4" type="video/mp4">
</video>
</div>
<div class="video-caption">
MaskAnyoneAPI-OpenPose
</div></td>
</tr>
<tr class="even">
<td><div class="video-zoom-wrapper ted-kid-video">
<video class="video-zoom ted-kid-sync" autoplay="" muted="" playsinline="">
<source src="videos/TED-kid/TED-kid_MaskAnyoneUI-MediaPipe.mp4" type="video/mp4">
</video>
</div>
<div class="video-caption">
MaskAnyoneUI-MediaPipe
</div></td>
<td><div class="video-zoom-wrapper ted-kid-video">
<video class="video-zoom ted-kid-sync" autoplay="" muted="" playsinline="">
<source src="videos/TED-kid/TED-kid_MaskAnyoneUI-OpenPose.mp4" type="video/mp4">
</video>
</div>
<div class="video-caption">
MaskAnyoneUI-OpenPose
</div></td>
</tr>
</tbody>
</table>


<script>
document.addEventListener('DOMContentLoaded', function() {
    const videos = document.querySelectorAll('.ted-kid-sync');
    let endedCount = 0;

    function restartAllVideos() {
        videos.forEach(video => {
            video.currentTime = 0;
            video.play();
        });
        endedCount = 0;
    }

    videos.forEach(video => {
        video.addEventListener('ended', () => {
            endedCount++;
            if (endedCount === videos.length) {
                restartAllVideos();
            }
        });
    });
});
</script>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-results-ted-kid-videos-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: Rendered result videos of different pose estimators on the TED-kid video.
</figcaption>
</figure>
</div>
</section>
<section id="sec-results-ted-talks" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="sec-results-ted-talks"><span class="header-section-number">8.2</span> TED Talks</h2>
<p>The results on ten full TED talks closely mirror those on the single TED kid video, as shown in <a href="#tbl-results-ted-talks" class="quarto-xref">Table&nbsp;3</a>. Among the evaluated pose estimators, MaskAnyoneUI-MediaPipe consistently achieved the best stability, with the lowest average velocity, acceleration, and jerk values of 1.25, 1.08, and 1.83, respectively. MaskAnyoneAPI-MediaPipe followed, showing the second-best performance in acceleration and jerk, closely trailed by YoloPose. OpenPose ranked next, while both MaskAnyone-OpenPose variants exhibited greater instability than the pure OpenPose model. Consistent with earlier findings, MediaPipePose was the least stable estimator, with the highest values across all metrics: 3.29 for velocity, 4.52 for acceleration, and 7.94 for jerk. An additional observation is the clear trend that MediaPipe-based MaskAnyone variants generally outperform OpenPose-based ones in stability, as reflected by their consistently lower velocity, acceleration, and jerk values.</p>
<div id="tbl-results-ted-talks" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-results-ted-talks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="results table" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Pose Estimator</th>
<th data-quarto-table-cell-role="th">Velocity</th>
<th data-quarto-table-cell-role="th">Acceleration</th>
<th data-quarto-table-cell-role="th">Jerk</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>YoloPose</td>
<td class="second">1.35</td>
<td>1.46</td>
<td>2.44</td>
</tr>
<tr class="even">
<td>MediaPipePose</td>
<td>3.29</td>
<td>4.52</td>
<td>7.94</td>
</tr>
<tr class="odd">
<td>OpenPose</td>
<td>1.58</td>
<td>2.22</td>
<td>3.44</td>
</tr>
<tr class="maskanyone-api border-top even">
<td>MaskAnyoneAPI-MediaPipe</td>
<td>1.44</td>
<td class="second">1.25</td>
<td class="second">2.04</td>
</tr>
<tr class="maskanyone-api border-bottom odd">
<td>MaskAnyoneAPI-OpenPose</td>
<td>2.30</td>
<td>2.42</td>
<td>4.07</td>
</tr>
<tr class="maskanyone-ui border-top even">
<td>MaskAnyoneUI-MediaPipe</td>
<td class="best">1.25</td>
<td class="best">1.08</td>
<td class="best">1.83</td>
</tr>
<tr class="maskanyone-ui border-bottom odd">
<td>MaskAnyoneUI-OpenPose</td>
<td>2.07</td>
<td>2.29</td>
<td>3.72</td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-results-ted-talks-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: Average metric results for different pose estimators aggregated over all TED talk videos.
</figcaption>
</figure>
</div>
<div id="fig-ted-talks-plots" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ted-talks-plots-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-ted-talks-plots" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-ted-talks-acceleration-distribution" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-ted-talks-acceleration-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="plots/TED-talks/acceleration_distribution.png" class="img-fluid figure-img" data-ref-parent="fig-ted-talks-plots">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-ted-talks-acceleration-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Acceleration Distribution
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-ted-talks-plots" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-ted-talks-jerk-distribution" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-ted-talks-jerk-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="plots/TED-talks/jerk_distribution.png" class="img-fluid figure-img" data-ref-parent="fig-ted-talks-plots">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-ted-talks-jerk-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Jerk Distribution
</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-ted-talks-plots" style="flex-basis: 100.0%;justify-content: flex-start;">
<div id="fig-ted-kid-acceleration-per-keypoint" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-ted-kid-acceleration-per-keypoint-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="plots/TED-talks/keypoint_plot_acceleration.png" class="img-fluid figure-img" data-ref-parent="fig-ted-talks-plots">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-ted-kid-acceleration-per-keypoint-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) Median Acceleration per Keypoint
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ted-talks-plots-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: <strong>Comparison of pose estimation models on the TED talks dataset.</strong> (a, b) Acceleration and jerk distribution showing the proportion of keypoints within a fixed value range. A larger concentration at lower values indicates a smoother and more stable pose, whereas a high number of keypoints with large values indicates more jittery movements. (c) Median acceleration per keypoint, indicating stability across individual body parts.
</figcaption>
</figure>
</div>
<p><a href="#fig-ted-talks-acceleration-distribution" class="quarto-xref">Figure&nbsp;4 (a)</a> and <a href="#fig-ted-talks-jerk-distribution" class="quarto-xref">Figure&nbsp;4 (b)</a> indicate that pose estimators are less stable in TED talks than in the TED kid video, with high acceleration and jerk values occurring more frequently. This is likely because TED talks include camera movements, scene changes, segments without visible people, and audience views, none of which appear in the TED kid video. We included two particularly challenging video chunks in <a href="#tbl-results-ted-videos" class="quarto-xref">Table&nbsp;4</a>. The first column shows results for the qualitatively worst-performing pose estimator, MediaPipePose, while the second column presents the best performer, MaskAnyoneUI-MediaPipe.</p>
<p>In the first scene <span class="citation" data-cites="ted-tarana">(<a href="#ref-ted-tarana" role="doc-biblioref">Burke 2018</a>)</span> from the TED talk “Me Too is a Movement, Not a Moment”, the woman wears a long dress, and the video contains multiple scene changes, audience views with the speaker in the background, and parts where the speaker is not visible. MaskAnyone substantially improves the stability and visual accuracy of pose estimation in all these scenarios.</p>
<p>In the second scene <span class="citation" data-cites="ted-song">(<a href="#ref-ted-song" role="doc-biblioref">Buzz 2023</a>)</span> from the TED talk “Universe / Statues / Liberation”, the main challenges are rapid camera view changes and close-up shots of the singing woman. Both MaskAnyoneUI-MediaPipe and raw MediaPipe struggle with close-ups of the hips and arms. The model attempts to fit a full human pose into the small visible area of an arm or hip, leading to incorrect pose estimation and unstable motion. It appears that once the model detects one joint, it tries to estimate the entire pose, which can cause errors in these conditions. This issue was primarily observed with MediaPipe models, including MaskAnyone-MediaPipe variants, and not with other pose estimators. Despite this, MaskAnyoneUI-MediaPipe still provides more stable and accurate pose estimations than pure MediaPipePose for most frames in this video.</p>
<div id="tbl-results-ted-videos" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-results-ted-videos-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="video-table table" data-quarto-postprocess="true">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<tbody>
<tr class="odd">
<td><div class="video-zoom-wrapper">
<video class="video-zoom tarana-sync" autoplay="" muted="" playsinline="">
<source src="videos/ted-talks/tarana_chunk17_MediaPipePose.mp4" type="video/mp4">
</video>
</div></td>
<td><div class="video-zoom-wrapper">
<video class="video-zoom tarana-sync" autoplay="" muted="" playsinline="">
<source src="videos/ted-talks/tarana_chunk17_MaskAnyoneUI-MediaPipe.mp4" type="video/mp4">
</video>
</div></td>
</tr>
<tr class="even">
<td><div class="video-zoom-wrapper">
<video class="video-zoom song-sync" autoplay="" muted="" playsinline="">
<source src="videos/ted-talks/song_chunk1_MediaPipePose.mp4" type="video/mp4">
</video>
</div>
<div class="video-caption">
MediaPipePose
</div></td>
<td><div class="video-zoom-wrapper">
<video class="video-zoom song-sync" autoplay="" muted="" playsinline="">
<source src="videos/ted-talks/song_chunk1_MaskAnyoneUI-MediaPipe.mp4" type="video/mp4">
</video>
</div>
<div class="video-caption">
MaskAnyoneUI-MediaPipe
</div></td>
</tr>
</tbody>
</table>


<script>
document.addEventListener('DOMContentLoaded', function() {
    const videos = document.querySelectorAll('.tarana-sync');
    let endedCount = 0;

    function restartAllVideos() {
        videos.forEach(video => {
            video.currentTime = 0;
            video.play();
        });
        endedCount = 0;
    }

    videos.forEach(video => {
        video.addEventListener('ended', () => {
            endedCount++;
            if (endedCount === videos.length) {
                restartAllVideos();
            }
        });
    });
});
document.addEventListener('DOMContentLoaded', function() {
    const videos = document.querySelectorAll('.song-sync');
    let endedCount = 0;

    function restartAllVideos() {
        videos.forEach(video => {
            video.currentTime = 0;
            video.play();
        });
        endedCount = 0;
    }

    videos.forEach(video => {
        video.addEventListener('ended', () => {
            endedCount++;
            if (endedCount === videos.length) {
                restartAllVideos();
            }
        });
    });
});
</script>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-results-ted-videos-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4: Two TED Talk chunks overlaid with pose estimations from MediaPipePose and MaskAnyoneUI-MediaPipe, featuring challenging segments with scene changes, camera movements, and periods without visible persons. The first row shows a clip from the TED talk “Me Too is a Movement, Not a Moment” <span class="citation" data-cites="ted-tarana">(<a href="#ref-ted-tarana" role="doc-biblioref">Burke 2018</a>)</span>. The second row shows a clip from the TED talk “Universe / Statues / Liberation” <span class="citation" data-cites="ted-song">(<a href="#ref-ted-song" role="doc-biblioref">Buzz 2023</a>)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="sec-results-tragic-talkers" class="level2" data-number="8.3">
<h2 data-number="8.3" class="anchored" data-anchor-id="sec-results-tragic-talkers"><span class="header-section-number">8.3</span> Tragic Talkers</h2>
<p><a href="#tbl-results-tragic-talkers" class="quarto-xref">Table&nbsp;5</a> presents the average metric results of various pose estimators on the Tragic Talkers dataset. Regarding accuracy against the pseudo-ground truth, YoloPose achieves the highest PCK at 96%, followed by OpenPose at 87%. All pose estimators except MediaPipePose exceed a PCK of 83%, with MediaPipePose detecting only 69% of keypoints correctly. However, these PCK and RMSE values should be interpreted cautiously, as the pseudo-ground truth was generated by an AI model rather than human annotators, making it inherently imperfect. Thus, the results reflect how closely each pose estimator matches the OpenPose output, rather than absolute pose estimation quality.</p>
<p>The kinematic metrics, especially acceleration and jerk, provide clearer results. MaskAnyoneAPI-MediaPipe performs best, with the lowest acceleration and jerk values of approximately 2.9 and 5.0, respectively. MaskAnyoneUI-MediaPipe follows closely behind MaskAnyoneAPI-MediaPipe with slightly increased acceleration and jerk, while YoloPose shows similar acceleration but a somewhat higher jerk. Although MaskAnyone-OpenPose variants outperform standard OpenPose, they still exhibit noticeably greater acceleration and jerk, reflecting less smooth motion. Pure MediaPipePose remains the least stable estimator, with average acceleration and jerk values of approximately 9.8 and 17.6, respectively.</p>
<div id="tbl-results-tragic-talkers" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-results-tragic-talkers-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="results table" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Pose Estimator</th>
<th data-quarto-table-cell-role="th">PCK</th>
<th data-quarto-table-cell-role="th">RMSE</th>
<th data-quarto-table-cell-role="th">Velocity</th>
<th data-quarto-table-cell-role="th">Acceleration</th>
<th data-quarto-table-cell-role="th">Jerk</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>YoloPose</td>
<td class="best">0.96</td>
<td class="second">0.11</td>
<td>4.36</td>
<td>3.27</td>
<td>5.57</td>
</tr>
<tr class="even">
<td>MediaPipePose</td>
<td>0.69</td>
<td>0.47</td>
<td>6.48</td>
<td>9.80</td>
<td>17.58</td>
</tr>
<tr class="odd">
<td>OpenPose</td>
<td class="second">0.87</td>
<td>0.33</td>
<td>4.63</td>
<td>6.38</td>
<td>10.00</td>
</tr>
<tr class="maskanyone-api border-top even">
<td>MaskAnyoneAPI-MediaPipe</td>
<td>0.78</td>
<td>0.12</td>
<td class="second">3.46</td>
<td class="best">2.86</td>
<td class="best">5.01</td>
</tr>
<tr class="maskanyone-api border-bottom odd">
<td>MaskAnyoneAPI-OpenPose</td>
<td>0.85</td>
<td>0.36</td>
<td>5.69</td>
<td>6.08</td>
<td>10.22</td>
</tr>
<tr class="maskanyone-ui border-top even">
<td>MaskAnyoneUI-MediaPipe</td>
<td>0.83</td>
<td class="best">0.07</td>
<td class="best">3.26</td>
<td class="second">2.91</td>
<td class="second">5.10</td>
</tr>
<tr class="maskanyone-ui border-bottom odd">
<td>MaskAnyoneUI-OpenPose</td>
<td>0.85</td>
<td>0.36</td>
<td>5.53</td>
<td>5.12</td>
<td>9.06</td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-results-tragic-talkers-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;5: Average metric results for different pose estimators aggregated over four camera angles of five Tragic Talkers sequences with pseudo-ground truth.
</figcaption>
</figure>
</div>
<p><a href="#fig-tragic-talkers-acceleration-distribution" class="quarto-xref">Figure&nbsp;5 (a)</a> and <a href="#fig-tragic-talkers-jerk-distribution" class="quarto-xref">Figure&nbsp;5 (b)</a> confirm the results from <a href="#tbl-results-tragic-talkers" class="quarto-xref">Table&nbsp;5</a>. Both plots show that the MaskAnyone-MediaPipe pose estimators achieve the highest proportion of low acceleration and jerk values, followed by YoloPose, the MaskAnyone-OpenPose pose estimators, and OpenPose. MediaPipePose once again has a very flat curve, indicating a lot of large acceleration and jerk values.</p>
<p><a href="#fig-tragic-talkers-acceleration-per-keypoint" class="quarto-xref">Figure&nbsp;5 (c)</a> shows that MediaPipePose is among the pose estimators with the highest median acceleration values for all keypoints. YoloPose, MaskAnyoneAPI-MediaPipe, and MaskAnyoneUI-MediaPipe achieve consistently low median acceleration values for all keypoints.</p>
<div id="fig-tragic-talkers-plots" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tragic-talkers-plots-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-tragic-talkers-plots" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-tragic-talkers-acceleration-distribution" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-tragic-talkers-acceleration-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="plots/TragicTalkers/acceleration_distribution.png" class="img-fluid figure-img" data-ref-parent="fig-tragic-talkers-plots">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-tragic-talkers-acceleration-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Acceleration Distribution
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-tragic-talkers-plots" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-tragic-talkers-jerk-distribution" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-tragic-talkers-jerk-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="plots/TragicTalkers/jerk_distribution.png" class="img-fluid figure-img" data-ref-parent="fig-tragic-talkers-plots">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-tragic-talkers-jerk-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Jerk Distribution
</figcaption>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-tragic-talkers-plots" style="flex-basis: 100.0%;justify-content: flex-start;">
<div id="fig-tragic-talkers-acceleration-per-keypoint" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-tragic-talkers-acceleration-per-keypoint-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="plots/TragicTalkers/keypoint_plot_Acceleration.png" class="img-fluid figure-img" data-ref-parent="fig-tragic-talkers-plots">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-tragic-talkers-acceleration-per-keypoint-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c) Median Acceleration per Keypoint
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tragic-talkers-plots-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: <strong>Comparison of pose estimation models on the Tragic Talkers dataset.</strong> (a, b) Acceleration and jerk distribution showing the proportion of keypoints within a fixed value range. A larger concentration at lower values indicates a smoother and more stable pose, whereas a high number of keypoints with large values indicates more jittery movements. (c) Median acceleration per keypoint, indicating stability across individual body parts.
</figcaption>
</figure>
</div>
<p>Interestingly, although the MaskAnyone-OpenPose pose estimators achieve lower acceleration values for nose, eye, ear, shoulder, and ankle keypoints than pure OpenPose, they perform worse for the elbow, hip, and knee keypoints. A potential reason for this could be that MaskAnyone uses a higher confidence threshold for keypoints than our OpenPose implementation, which leads to the elbow, hip, and knee keypoints not being detected or rendered. As an example, consider <a href="#tbl-results-tragic-talkers-bad-legs" class="quarto-xref">Table&nbsp;6</a>, which shows the first seconds of the rendered video for OpenPose, MaskAnyoneAPI-OpenPose, and MaskAnyoneUI-OpenPose for the “conversation1_t3-cam08” sequence. In this scene, both MaskAnyone-OpenPose pose estimators fail to detect the legs of the woman, while OpenPose correctly detects them.</p>
<div id="tbl-results-tragic-talkers-bad-legs" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-results-tragic-talkers-bad-legs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="video-table table" data-quarto-postprocess="true">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<tbody>
<tr class="odd">
<td><div class="video-zoom-wrapper">
<video class="video-zoom tt-legs-sync" autoplay="" muted="" playsinline="">
<source src="videos/TragicTalkers/conversation1_t3-cam08/conversation1_t3-cam08_OpenPose.mp4#t=0,10" type="video/mp4">
</video>
</div>
<div class="video-caption">
OpenPose
</div></td>
<td><div class="video-zoom-wrapper">
<video class="video-zoom tt-legs-sync" autoplay="" muted="" playsinline="">
<source src="videos/TragicTalkers/conversation1_t3-cam08/conversation1_t3-cam08_MaskAnyoneAPI-OpenPose.mp4#t=0,10" type="video/mp4">
</video>
</div>
<div class="video-caption">
MaskAnyoneAPI-OpenPose
</div></td>
<td><div class="video-zoom-wrapper">
<video class="video-zoom tt-legs-sync" autoplay="" muted="" playsinline="">
<source src="videos/TragicTalkers/conversation1_t3-cam08/conversation1_t3-cam08_MaskAnyoneUI-OpenPose.mp4#t=0,10" type="video/mp4">
</video>
</div>
<div class="video-caption">
MaskAnyoneUI-OpenPose
</div></td>
</tr>
</tbody>
</table>


<script>
document.addEventListener('DOMContentLoaded', function() {
    const videos = document.querySelectorAll('.tt-legs-sync');
    let videosReady = 0;
    
    function checkAllVideosEnded(video) {
        if (video.currentTime >= 10) {  // Check if we've reached the fragment end time
            videosReady++;
            if (videosReady === videos.length) {
                videos.forEach(v => {
                    v.currentTime = 0;
                    v.play();
                });
                videosReady = 0;
            }
        }
    }

    videos.forEach(video => {
        video.addEventListener('timeupdate', () => checkAllVideosEnded(video));
    });
});
</script>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-results-tragic-talkers-bad-legs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6: First 10 seconds of the rendered Tragic Talkers videos for OpenPose, MaskAnyoneAPI-OpenPose, and MaskAnyoneUI-OpenPose for the “conversation1_t3-cam08” sequence.
</figcaption>
</figure>
</div>
<p>Last but not least, we qualitatively compare MaskAnyoneAPI-MediaPipe, MaskAnyoneUI-MediaPipe, and YoloPose on the “interactive4_t3-cam08” sequence (<a href="#tbl-results-tragic-talkers-best-models" class="quarto-xref">Table&nbsp;7</a>). These three pose estimators have the lowest overall average acceleration values, as shown in <a href="#tbl-results-tragic-talkers" class="quarto-xref">Table&nbsp;5</a>. Two important observations were made:</p>
<ol type="1">
<li>YoloPose is the only estimator that correctly identifies the woman when she turns around, facing away from the camera. Both MaskAnyone variants fail in this scenario.</li>
<li>At the start of the sequence, where both actors stand with hands stretched out, only YoloPose correctly captures the lower part of the woman’s body. Both MaskAnyone estimators produce an incorrect upper-body pose initially, which improves as the woman lowers her arms, eventually stabilizing in the correct position.</li>
</ol>
<p>Qualitatively, YoloPose performs best on this sequence.</p>
<div id="tbl-results-tragic-talkers-best-models" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-results-tragic-talkers-best-models-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="video-table table" data-quarto-postprocess="true">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<tbody>
<tr class="odd">
<td><div class="video-zoom-wrapper">
<video class="video-zoom tt-best-models-sync" autoplay="" muted="" playsinline="">
<source src="videos/TragicTalkers/interactive4_t3-cam08/interactive4_t3-cam08_YoloPose.mp4" type="video/mp4">
</video>
</div>
<div class="video-caption">
YoloPose
</div></td>
<td><div class="video-zoom-wrapper">
<video class="video-zoom tt-best-models-sync" autoplay="" muted="" playsinline="">
<source src="videos/TragicTalkers/interactive4_t3-cam08/interactive4_t3-cam08_MaskAnyoneAPI-MediaPipe.mp4" type="video/mp4">
</video>
</div>
<div class="video-caption">
MaskAnyoneAPI-MediaPipe
</div></td>
<td><div class="video-zoom-wrapper">
<video class="video-zoom tt-best-models-sync" autoplay="" muted="" playsinline="">
<source src="videos/TragicTalkers/interactive4_t3-cam08/interactive4_t3-cam08_MaskAnyoneUI-MediaPipe.mp4" type="video/mp4">
</video>
</div>
<div class="video-caption">
MaskAnyoneUI-MediaPipe
</div></td>
</tr>
</tbody>
</table>


<script>
document.addEventListener('DOMContentLoaded', function() {
    const videos = document.querySelectorAll('.tt-best-models-sync');
    let endedCount = 0;

    function restartAllVideos() {
        videos.forEach(video => {
            video.currentTime = 0;
            video.play();
        });
        endedCount = 0;
    }

    videos.forEach(video => {
        video.addEventListener('ended', () => {
            endedCount++;
            if (endedCount === videos.length) {
                restartAllVideos();
            }
        });
    });
});
</script>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-results-tragic-talkers-best-models-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;7: The most stable pose estimators on the Tragic Talkers “interactive4_t3-cam08” sequence.
</figcaption>
</figure>
</div>
</section>
<section id="sec-results-inference-raw-masked" class="level2" data-number="8.4">
<h2 data-number="8.4" class="anchored" data-anchor-id="sec-results-inference-raw-masked"><span class="header-section-number">8.4</span> Inference on Raw vs.&nbsp;Masked Videos</h2>
<div id="tbl-pck-raw-masked" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-pck-raw-masked-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="results raw-masked-table table" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Pose Estimator</th>
<th data-quarto-table-cell-role="th">Blurring</th>
<th data-quarto-table-cell-role="th">Pixelation</th>
<th data-quarto-table-cell-role="th">Contours</th>
<th data-quarto-table-cell-role="th">Solid Fill</th>
<th data-quarto-table-cell-role="th">Average</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>YoloPose</td>
<td class="best">0.95</td>
<td>0.09</td>
<td class="best">0.93</td>
<td class="second">0.32</td>
<td class="second">0.57</td>
</tr>
<tr class="even">
<td>MediaPipePose</td>
<td class="best">0.95</td>
<td class="best">0.81</td>
<td>0.56</td>
<td class="best">0.34</td>
<td class="best">0.67</td>
</tr>
<tr class="odd">
<td>OpenPose</td>
<td class="second">0.88</td>
<td>0.10</td>
<td class="second">0.62</td>
<td>0.01</td>
<td>0.40</td>
</tr>
<tr class="maskanyone-api border-top even">
<td>MaskAnyoneAPI-MediaPipe</td>
<td>0.85</td>
<td>0.30</td>
<td>0.00</td>
<td>0.00</td>
<td>0.29</td>
</tr>
<tr class="maskanyone-api border-bottom odd">
<td>MaskAnyoneAPI-OpenPose</td>
<td>0.75</td>
<td>0.00</td>
<td>0.36</td>
<td>0.00</td>
<td>0.28</td>
</tr>
<tr class="maskanyone-ui border-top even">
<td>MaskAnyoneUI-MediaPipe</td>
<td class="best">0.95</td>
<td class="second">0.63</td>
<td>0.00</td>
<td>0.07</td>
<td>0.41</td>
</tr>
<tr class="maskanyone-ui border-bottom odd">
<td>MaskAnyoneUI-OpenPose</td>
<td>0.87</td>
<td>0.03</td>
<td>0.58</td>
<td>0.00</td>
<td>0.37</td>
</tr>
<tr class="even">
<td>Average</td>
<td>0.86</td>
<td>0.23</td>
<td>0.44</td>
<td>0.11</td>
<td>/</td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-pck-raw-masked-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;8: Percentage of correct keypoints (PCK) for different pose estimators on videos masked by different hiding strategies.
</figcaption>
</figure>
</div>
<div id="tbl-rmse-raw-masked" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-rmse-raw-masked-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="results raw-masked-table table" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Pose Estimator</th>
<th data-quarto-table-cell-role="th">Blurring</th>
<th data-quarto-table-cell-role="th">Pixelation</th>
<th data-quarto-table-cell-role="th">Contours</th>
<th data-quarto-table-cell-role="th">Solid Fill</th>
<th data-quarto-table-cell-role="th">Average</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>YoloPose</td>
<td class="second">0.12</td>
<td>0.92</td>
<td class="best">0.13</td>
<td class="second">0.74</td>
<td class="second">0.48</td>
</tr>
<tr class="even">
<td>MediaPipePose</td>
<td class="second">0.12</td>
<td class="best">0.26</td>
<td>0.49</td>
<td class="best">0.64</td>
<td class="best">0.38</td>
</tr>
<tr class="odd">
<td>OpenPose</td>
<td>0.25</td>
<td>0.94</td>
<td class="second">0.47</td>
<td>1.0</td>
<td>0.67</td>
</tr>
<tr class="maskanyone-api border-top even">
<td>MaskAnyoneAPI-MediaPipe</td>
<td>0.27</td>
<td>0.74</td>
<td>1.00</td>
<td>0.99</td>
<td>0.75</td>
</tr>
<tr class="maskanyone-api border-bottom odd">
<td>MaskAnyoneAPI-OpenPose</td>
<td>0.43</td>
<td>0.99</td>
<td>0.75</td>
<td>1.00</td>
<td>0.79</td>
</tr>
<tr class="maskanyone-ui border-top even">
<td>MaskAnyoneUI-MediaPipe</td>
<td class="best">0.07</td>
<td class="second">0.41</td>
<td>1.00</td>
<td>0.94</td>
<td>0.60</td>
</tr>
<tr class="maskanyone-ui border-bottom odd">
<td>MaskAnyoneUI-OpenPose</td>
<td>0.24</td>
<td>0.98</td>
<td>0.52</td>
<td>1.00</td>
<td>0.69</td>
</tr>
<tr class="even">
<td>Average</td>
<td>0.21</td>
<td>0.78</td>
<td>0.62</td>
<td>0.90</td>
<td>/</td>
</tr>
</tbody>
</table>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-rmse-raw-masked-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;9: Root mean square error (RMSE) for different pose estimators on videos masked by different hiding strategies.
</figcaption>
</figure>
</div>
<p>As described in <a href="#sec-datasets-masked-video" class="quarto-xref">Section&nbsp;5.4</a>, three videos were masked using four different hiding strategies. <a href="#tbl-pck-raw-masked" class="quarto-xref">Table&nbsp;8</a> and <a href="#tbl-rmse-raw-masked" class="quarto-xref">Table&nbsp;9</a> present the percentage of correct keypoints (PCK) and root mean square error (RMSE) for various pose estimators on the masked videos, compared to the original videos.</p>
<p><strong>Comparison of pose estimators</strong></p>
<p>MediaPipePose achieves the highest average PCK of 67% and the lowest average RMSE of 0.38, indicating robustness across all hiding strategies. YoloPose also performs well with an average PCK of 57%, particularly on the blurring and contours strategies, where it detects 95% and 93% of the original keypoints, respectively. In contrast, OpenPose performs weaker, with an average PCK of only 40% and a high RMSE of 0.67.</p>
<p>Among the MaskAnyone variants, UI-based models generally outperform API-based ones. MaskAnyoneUI-MediaPipe achieves a moderate average PCK of 41% and RMSE of 0.6. The API variants perform poorly, with average PCKs around 28% to 29%, indicating that human input improves performance on masked videos.</p>
<p>However, unlike in other datasets, MaskAnyone UI variants do not improve but rather degrade performance compared to the pure AI models. Masking the videos makes keypoint detection more challenging, often lowering the confidence scores assigned by the models. Because MaskAnyone applies higher confidence thresholds than the base AI models, many keypoints with reduced confidence may be discarded, leading to more undetected keypoints. Additionally, if the first stage of MaskAnyone, where YoloPose detects the person, performs poorly, the second stage, which uses SAM2 <span class="citation" data-cites="sam2">(<a href="#ref-sam2" role="doc-biblioref">Ravi et al. 2024</a>)</span> to segment and crop the person, also suffers. This cascades to low-quality input for the final pose estimation stage, degrading overall performance.</p>
<p><strong>Comparison of hiding strategies</strong></p>
<p>Last, we compare the hiding strategies in terms of balancing privacy and pose estimation performance on masked videos.</p>
<p>Blurring produced the highest average PCK of 86% across all pose estimators. This shows that models can still recognize and track people accurately, even when the image is partially obscured. The result highlights that pose estimation does not depend solely on the visibility of individual joints. Instead, the models appear to rely on the overall shape and structure of the body, using contextual cues to fill in missing details. They likely infer joint positions by applying spatial relationships and body priors learned during training, such as limb proportions, symmetry, and common human poses. For instance, even when specific features like eyes or hands are hidden, the surrounding geometry, such as head position, shoulder width, or arm direction, provides sufficient context for pose estimation. This suggests that these models depend more on learned pose patterns than on fine-grained pixel information.</p>
<p>The results for other hiding strategies are more mixed. YoloPose achieves an impressive 93% PCK on videos masked with contours, indicating its ability to utilize edge and shape information rather than texture or color. In contrast, other pose estimators perform poorly on this strategy. Both MaskAnyone API variants detect almost no keypoints, and the remaining models achieve between 36% and 62% PCK.</p>
<p>For pixelation, only MediaPipePose detects persons reasonably well, with 81% PCK. YoloPose, OpenPose, and both MaskAnyone-OpenPose variants detect fewer than 10% of keypoints. This suggests that the pixelation level used drastically reduces usable information for pose estimation and that this method is currently unsuitable for such tasks.</p>
<p>The solid fill hiding strategy is the most challenging, removing nearly all information about the person except the outline. As a result, it yields the lowest average PCK of 11%. MediaPipePose performs best here but reaches only 34% PCK.</p>
<p>In conclusion, blurring offers the best trade-off between privacy and pose estimation performance on masked videos. While it may not fully de-identify individuals, it retains sufficient information for accurate pose predictions. The contour hiding strategy can be considered when stronger privacy is required, though it reduces accuracy for all but YoloPose.</p>
<p><a href="#tbl-results-raw-masked-rendered" class="quarto-xref">Table&nbsp;10</a> presents qualitative results of the best-performing pose estimators on masked videos from the TED talk “Let curiosity lead” and the Tragic Talkers sequence “interactive1_t1-cam06.”</p>
<div id="tbl-results-raw-masked-rendered" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<div aria-describedby="tbl-results-raw-masked-rendered-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<style>
/* Style for raw-masked videos table */
.video-table {
    border-collapse: collapse; /* Remove spacing between cells */
}
.video-table td {
    text-align: center;
    vertical-align: middle;
    padding: 4px; /* Minimal padding around cells */
}
.video-table .video-zoom-wrapper {
    display: flex;
    justify-content: center;
    align-items: center;
}
.video-table .video-zoom-wrapper video {
    height: 200px; /* Fixed height for all videos in this table */
    width: auto;   /* Maintain aspect ratio */
}
</style>


<table class="video-table table" data-quarto-postprocess="true">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<tbody>
<tr class="odd">
<td><div class="video-zoom-wrapper">
<video class="video-zoom ted-raw-masked-sync" autoplay="" muted="" playsinline="">
<source src="videos/raw-masked/Blurring-TED-curiosity-chunk_YoloPose.mp4" type="video/mp4">
</video>
<!-- <div class="video-caption">YoloPose</div> -->
</div></td>
<td><div class="video-zoom-wrapper">
<video class="video-zoom tt-raw-masked-sync" autoplay="" muted="" playsinline="">
<source src="videos/raw-masked/Blurring-TT-interactive1_t1-cam06_YoloPose.mp4" type="video/mp4">
</video>
<!-- <div class="video-caption">YoloPose</div> -->
</div></td>
</tr>
<tr class="even">
<td><div class="video-zoom-wrapper">
<video class="video-zoom ted-raw-masked-sync" autoplay="" muted="" playsinline="">
<source src="videos/raw-masked/Pixelation-TED-curiosity-chunk_MaskAnyoneUI-MediaPipe.mp4" type="video/mp4">
</video>
<!-- <div class="video-caption">MaskAnyoneUI-MediaPipe</div> -->
</div></td>
<td><div class="video-zoom-wrapper">
<video class="video-zoom tt-raw-masked-sync" autoplay="" muted="" playsinline="">
<source src="videos/raw-masked/Pixelation-TT-interactive1_t1-cam06_MediaPipePose.mp4" type="video/mp4">
</video>
<!-- <div class="video-caption">MediaPipePose</div> -->
</div></td>
</tr>
<tr class="odd">
<td><div class="video-zoom-wrapper">
<video class="video-zoom ted-raw-masked-sync" autoplay="" muted="" playsinline="">
<source src="videos/raw-masked/Contours-TED-curiosity-chunk_YoloPose.mp4" type="video/mp4">
</video>
<!-- <div class="video-caption">YoloPose</div> -->
</div></td>
<td><div class="video-zoom-wrapper">
<video class="video-zoom tt-raw-masked-sync" autoplay="" muted="" playsinline="">
<source src="videos/raw-masked/Contours-TT-interactive1_t1-cam06_YoloPose.mp4" type="video/mp4">
</video>
<!-- <div class="video-caption">YoloPose</div> -->
</div></td>
</tr>
<tr class="even">
<td><div class="video-zoom-wrapper">
<video class="video-zoom ted-raw-masked-sync" autoplay="" muted="" playsinline="">
<source src="videos/raw-masked/SolidFill-TED-curiosity-chunk_YoloPose.mp4" type="video/mp4">
</video>
<!-- <div class="video-caption">YoloPose</div> -->
</div></td>
<td><div class="video-zoom-wrapper">
<video class="video-zoom tt-raw-masked-sync" autoplay="" muted="" playsinline="">
<source src="videos/raw-masked/SolidFill-TT-interactive1_t1-cam06_MaskAnyoneUI-MediaPipe.mp4" type="video/mp4">
</video>
<!-- <div class="video-caption">MaskAnyoneUI-MediaPipe</div> -->
</div></td>
</tr>
</tbody>
</table>


<script>
document.addEventListener('DOMContentLoaded', function() {
    const videos = document.querySelectorAll('.ted-raw-masked-sync');
    let endedCount = 0;

    function restartAllVideos() {
        videos.forEach(video => {
            video.currentTime = 0;
            video.play();
        });
        endedCount = 0;
    }

    videos.forEach(video => {
        video.addEventListener('ended', () => {
            endedCount++;
            if (endedCount === videos.length) {
                restartAllVideos();
            }
        });
    });
});

document.addEventListener('DOMContentLoaded', function() {
    const videos = document.querySelectorAll('.tt-raw-masked-sync');
    let endedCount = 0;

    function restartAllVideos() {
        videos.forEach(video => {
            video.currentTime = 0;
            video.play();
        });
        endedCount = 0;
    }

    videos.forEach(video => {
        video.addEventListener('ended', () => {
            endedCount++;
            if (endedCount === videos.length) {
                restartAllVideos();
            }
        });
    });
});
</script>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-tbl" id="tbl-results-raw-masked-rendered-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;10: The best-performing pose estimators on the masked videos are shown for the TED sequence “Let curiosity lead” and the Tragic Talkers sequence “interactive1_t1-cam06”. Each row corresponds to a different hiding strategy, in the order: Blurring, Pixelation, Contours, and Solid Fill. For the TED sequence (first column), the pose estimators are YoloPose, MaskAnyoneUI-MediaPipe, YoloPose, and YoloPose. For the Tragic Talkers sequence (second column), the pose estimators are YoloPose, MediaPipePose, YoloPose, and MaskAnyoneUI-MediaPipe.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="sec-future-work" class="level1" data-number="9">
<h1 data-number="9"><span class="header-section-number">9</span> Future Work &amp; Limitations</h1>
<p>In this section, we outline future work and limitations of MaskBench and MaskAnyone.</p>
<section id="maskanyone-limitations" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="maskanyone-limitations"><span class="header-section-number">9.1</span> MaskAnyone Limitations</h2>
<p>Both MaskAnyone-API and MaskAnyone-UI have introduced improvements in detection accuracy and user interaction. However, they still exhibit notable limitations in complex and long video scenarios such as TED talks, which often feature background noise, occlusions, scene transitions, and unrelated visual elements.</p>
<p>The first and most important challenge of MaskAnyone-API and MaskAnyone-UI is that it does not support long videos, such as TED talks. It requires manual chunking of the video; however, chunking introduces another issue. After chunking, some videos start with frames where no human is visible. When a video begins with such frames, MaskAnyone falsely predicts objects in the scene as humans and then continues to the end of the video with that false prediction. For better detection and pose estimation, we would need to remove the start of chunks without visible humans, but this results in a loss of content.</p>
<p>Another major challenge lies in handling abrupt scene changes or shifts in camera perspective. For example, when a video cuts from a close-up to a full-body shot (or vice versa), MaskAnyone fails to maintain consistent detection and tracking, resulting in missed detections or inaccurate pose estimation. MaskAnyone-UI addresses this issue through a human-in-the-loop mechanism that allows users to manually select key frames, ensuring more reliable tracking throughout the video.</p>
<p>Another issue, observed primarily in MaskAnyone-API, is the double overlaying of pose skeletons on the same person. This results in duplicate or misaligned pose renderings. This problem has not been observed in the UI version, as manual frame selection allows users to avoid such misdetections.</p>
<p>Finally, false positive predictions remain a common problem in MaskAnyone-API. Not only in scenes where no human is present, where the system interprets non-human objects such as buildings, cigarettes, or images as people, but it also occurs in scenarios where a human is actually present, yet MaskAnyone-API segments the background instead of the person. False positive predictions occur in MaskAnyone-UI as well, but only on rare occasions.</p>
</section>
<section id="maskbench-outlook" class="level2" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="maskbench-outlook"><span class="header-section-number">9.2</span> MaskBench Outlook</h2>
<p>With our promising results, we are laying the groundwork for a more versatile benchmarking framework for pose estimation on masked videos. There are several directions in which we plan to extend our work.</p>
<section id="sec-future-work-pipelining" class="level3" data-number="9.2.1">
<h3 data-number="9.2.1" class="anchored" data-anchor-id="sec-future-work-pipelining"><span class="header-section-number">9.2.1</span> Pipelining</h3>
<p>Currently, MaskBench supports a single workflow: running inference on a set of videos, evaluating results using metrics, visualizing them, and rendering the videos with overlaid poses. As demonstrated with the masked video dataset experiment, there are many more potential workflows that could be integrated. We aim to introduce an extensible and customizable pipeline class to MaskBench. Each pipeline would define a specific workflow (i.e., our current workflow, the masked video dataset workflow, or other, yet to be defined workflows) by chaining MaskBench components in a particular order, reusing existing modules and adding new ones where necessary.</p>
<p>For example, the masked video dataset workflow could be structured as follows:</p>
<ol type="1">
<li>Run inference on the raw videos with all pose estimators.</li>
<li>Evaluate results with metrics.</li>
<li>Visualize pose estimator performance on raw videos using plots or tables.</li>
<li>Render the raw videos with overlaid poses.</li>
<li>Reuse the SAM2 masks from MaskAnyone to apply different hiding strategies to the videos. Masking parameters could be adjusted by the user to explore not only different strategies but also varying degrees of masking, helping determine the optimal balance between privacy and performance.</li>
<li>For each pose estimator, run inference on all videos across all hiding strategies.</li>
<li>Evaluate results with metrics.</li>
<li>Visualize performance on masked videos using plots or tables.</li>
<li>Render the masked videos with overlaid poses.</li>
</ol>
</section>
<section id="sec-future-work-downstream-tasks" class="level3" data-number="9.2.2">
<h3 data-number="9.2.2" class="anchored" data-anchor-id="sec-future-work-downstream-tasks"><span class="header-section-number">9.2.2</span> Evaluation of downstream tasks</h3>
<p>Estimating a person’s pose can serve as a preliminary step for many downstream tasks, such as gesture recognition <span class="citation" data-cites="gesture-recognition gesture-recognition-3d">(<a href="#ref-gesture-recognition" role="doc-biblioref">Köpüklü et al. 2019</a>; <a href="#ref-gesture-recognition-3d" role="doc-biblioref">Molchanov et al. 2016</a>)</span>, 3D human reconstruction <span class="citation" data-cites="sith pose2mesh">(<a href="#ref-sith" role="doc-biblioref">Ho, Song, and Hilliges 2024</a>; <a href="#ref-pose2mesh" role="doc-biblioref">Choi, Moon, and Lee 2020</a>)</span>, and action classification. MaskBench could be extended to evaluate how different upstream pose estimators affect performance on these downstream tasks for both raw and masked videos. This extension would give researchers practical guidance on which pose estimator to choose for a given downstream application. Furthermore, researchers could use MaskBench to mask sensitive datasets and publish the masked videos together with pose outputs derived from the original raw videos. Other researchers could then use the masked videos plus the provided pose outputs as input for downstream tasks without accessing the original raw data. MaskBench should include an evaluation framework that quantifies the potential performance loss in downstream tasks when using masked videos or alternative upstream estimators.</p>
</section>
<section id="user-interface" class="level3" data-number="9.2.3">
<h3 data-number="9.2.3" class="anchored" data-anchor-id="user-interface"><span class="header-section-number">9.2.3</span> User interface</h3>
<p>Adding a web-based user interface to MaskBench would make the framework significantly more accessible. At present, running MaskBench requires technical expertise, such as working with Docker containers, setting environment variables, and editing configuration files. A dedicated interface could replace these steps with an intuitive, visual workflow for configuring and running pipelines. It could also provide built-in visualization panels for metrics, interactive plots, and side-by-side video comparisons, making it easier to explore results without leaving the application. Ultimately, this would lower the entry barrier for non-technical users while speeding up experimentation for advanced users.</p>
</section>
<section id="additional-improvements" class="level3" data-number="9.2.4">
<h3 data-number="9.2.4" class="anchored" data-anchor-id="additional-improvements"><span class="header-section-number">9.2.4</span> Additional improvements</h3>
<p>In addition to the major extensions outlined above, several smaller improvements could further enhance MaskBench in the future:</p>
<ul>
<li><strong>Expanded normalization options for the Euclidean distance metric</strong>. Currently, normalization is only possible using bounding boxes, but can be extended to support head and torso normalization as described in <a href="#sec-metrics-euclidean-distance" class="quarto-xref">Section&nbsp;6.1.1</a>. This requires identifying the relevant head and torso keypoints while keeping the system flexible enough to support multiple keypoint formats beyond COCO.</li>
<li><strong>Integrated logging system</strong>. A built-in logger could provide cleaner, more structured terminal output. For debugging, an option to display all logs from the underlying Docker containers would make error tracing during development much easier.</li>
<li><strong>Support for face and hand keypoints</strong>. This would enable evaluation of a broader set of downstream tasks where fine-grained keypoint data is important.</li>
<li><strong>Additional ground-truth–independent metrics and plots</strong>. Beyond velocity, acceleration, and jerk, metrics could assess the physical plausibility of a pose given human body constraints. This would shift part of the evaluation focus from pure numerical quality to biomechanical realism.</li>
<li><strong>3D pose estimation support</strong> as a long-term goal. This could include both evaluating 3D models directly and projecting 3D ground truth keypoints onto the 2D image plane using camera calibration data. Leveraging marker-based motion capture datasets—such as BioCV from the University of Bath <span class="citation" data-cites="bio-cv">(<a href="#ref-bio-cv" role="doc-biblioref">Evans et al. 2024</a>)</span>—would allow for more precise real-world benchmarking than 2D pseudo-ground truth data.</li>
<li><strong>Integrate Samurai <span class="citation" data-cites="samurai">(<a href="#ref-samurai" role="doc-biblioref">Yang et al. 2024</a>)</span> into MaskAnyone</strong>. This would allow for more stable tracking of persons over time by using adapted memory modules that more consistently maintain the identity of a person across frames.</li>
</ul>
</section>
</section>
</section>
<section id="sec-conclusion" class="level1" data-number="10">
<h1 data-number="10"><span class="header-section-number">10</span> Conclusion</h1>
<p>This work introduced MaskBench, a modular and extensible benchmarking framework for evaluating pose estimation models under diverse conditions, including privacy-preserving masking strategies. We evaluated four datasets of increasing complexity, including real-world TED talk recordings to examine how models perform in unconstrained, natural scenarios rather than under controlled laboratory conditions. The study includes popular pose estimators such as YoloPose <span class="citation" data-cites="yolo">(<a href="#ref-yolo" role="doc-biblioref">Redmon et al. 2016</a>)</span>, MediaPipe <span class="citation" data-cites="mediapipe">(<a href="#ref-mediapipe" role="doc-biblioref">Lugaresi et al. 2019</a>)</span>, and OpenPose <span class="citation" data-cites="openpose-1">(<a href="#ref-openpose-1" role="doc-biblioref">Z. Cao et al. 2019</a>)</span>, alongside the mixture-of-expert-model pipeline MaskAnyone <span class="citation" data-cites="schilling2023maskanyone">(<a href="#ref-schilling2023maskanyone" role="doc-biblioref">Schilling 2024b</a>)</span>, to assess their performance across these varied settings.</p>
<p>Our quantitative evaluation, using acceleration and jerk metrics to measure temporal stability, showed that the MaskAnyone pipeline, particularly the human-in-the-loop MediaPipe variant, substantially improves stability by reducing acceleration and jerk compared to standard models. YoloPose was the most robust standalone estimator, while MediaPipePose consistently exhibited the highest instability. Visual inspection of the output poses confirmed these findings, with noticeably smoother and more consistent motion in cases where the metrics indicated high stability.</p>
<p>Our small study on masked videos revealed that blurring offers the best trade-off between privacy and accuracy, maintaining high PCK values across models, whereas pixelation and solid fills significantly degraded performance. Model-specific responses to masking strategies highlighted that pose estimation often relies more on overall body structure than on pixel-level detail.</p>
<p>While results are promising, limitations remain, including reliance on pseudo-ground truth in some datasets and the preliminary implementation of masked-video workflows. Future work will focus on extending MaskBench with flexible pipelining, downstream task evaluation, and user-friendly interfaces, enabling systematic exploration of how privacy-preserving transformations affect pose estimation and subsequent applications.</p>
</section>
<section id="references" class="level1" data-number="11">

<!-- -->


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">11 References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-ted-kid" class="csl-entry" role="listitem">
Allen, Cameron. 2017. <span>“Education for All.”</span> <em>Youtube.com</em>. TEDxKids@ElCajon. <a href="https://www.youtube.com/watch?v=OMbNoo4mCcI">https://www.youtube.com/watch?v=OMbNoo4mCcI</a>.
</div>
<div id="ref-tragic-talkers" class="csl-entry" role="listitem">
Berghi, Davide, Marco Volino, and Philip J. B. Jackson. 2022. <span>“Tragic Talkers: A Shakespearean Sound- and Light-Field Dataset for Audio-Visual Machine Learning Research.”</span> In <em>Proceedings of the 19th ACM SIGGRAPH European Conference on Visual Media Production</em>. CVMP ’22. New York, NY, USA: Association for Computing Machinery. <a href="https://doi.org/10.1145/3565516.3565522">https://doi.org/10.1145/3565516.3565522</a>.
</div>
<div id="ref-ted-tarana" class="csl-entry" role="listitem">
Burke, Tarana. 2018. <span>“Me Too Is a Movement, Not a Moment.”</span> <em>Ted.com</em>. TED Talks. <a href="https://www.ted.com/talks/tarana_burke_me_too_is_a_movement_not_a_moment">https://www.ted.com/talks/tarana_burke_me_too_is_a_movement_not_a_moment</a>.
</div>
<div id="ref-ted-song" class="csl-entry" role="listitem">
Buzz. 2023. <span>“<span>‘Universe’</span> / <span>‘Statues’</span> / <span>‘Liberation’</span>.”</span> <em>Ted.com</em>. TED Talks. <a href="https://www.ted.com/talks/buzz_universe_statues_liberation">https://www.ted.com/talks/buzz_universe_statues_liberation</a>.
</div>
<div id="ref-openpose-3" class="csl-entry" role="listitem">
Cao, Zhe, Tomas Simon, Shih-En Wei, and Yaser Sheikh. 2017. <span>“Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields.”</span> In <em>CVPR</em>.
</div>
<div id="ref-openpose-1" class="csl-entry" role="listitem">
Cao, Z., G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A. Sheikh. 2019. <span>“OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>.
</div>
<div id="ref-pose2mesh" class="csl-entry" role="listitem">
Choi, Hongsuk, Gyeongsik Moon, and Kyoung Mu Lee. 2020. <span>“Pose2Mesh: Graph Convolutional Network for 3D Human Pose and Mesh Recovery from a 2D Human Pose.”</span> In <em>European Conference on Computer Vision (ECCV)</em>.
</div>
<div id="ref-bio-cv" class="csl-entry" role="listitem">
Evans, Murray, Laurie Needham, Logan Wade, Martin Parsons, Steffi Colyer, Polly McGuigan, James Bilzon, and Darren Cosker. 2024. <span>“Synchronised Video, Motion Capture and Force Plate Dataset for Validating Markerless Human Movement Analysis.”</span> <em>Scientific Data</em> 11 (1): 1300. <a href="https://doi.org/10.1038/s41597-024-04077-3">https://doi.org/10.1038/s41597-024-04077-3</a>.
</div>
<div id="ref-sith" class="csl-entry" role="listitem">
Ho, Hsuan-I, Jie Song, and Otmar Hilliges. 2024. <span>“SiTH: Single-View Textured Human Reconstruction with Image-Conditioned Diffusion.”</span> In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>.
</div>
<div id="ref-yolo11_ultralytics" class="csl-entry" role="listitem">
Jocher, Glenn, and Jing Qiu. 2024. <span>“Ultralytics YOLO11.”</span> <a href="https://github.com/ultralytics/ultralytics">https://github.com/ultralytics/ultralytics</a>.
</div>
<div id="ref-kaim2024comparison" class="csl-entry" role="listitem">
Kaim, Utsav, Aryan Jaiswal, Vyoum Khare, and Manish M Parmar. 2024. <span>“Comparison of ML Models for Posture.”</span> <em>International Journal of Creative Research Thoughts (IJCRT)</em> 12 (8). <a href="https://www.ijcrt.org/papers/IJCRT2408135.pdf">https://www.ijcrt.org/papers/IJCRT2408135.pdf</a>.
</div>
<div id="ref-gesture-recognition" class="csl-entry" role="listitem">
Köpüklü, Okan, Ahmet Gunduz, Neslihan Kose, and Gerhard Rigoll. 2019. <span>“Real-Time Hand Gesture Detection and Classification Using Convolutional Neural Networks.”</span> In <em>2019 14th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2019)</em>, 1–8. <a href="https://doi.org/10.1109/FG.2019.8756576">https://doi.org/10.1109/FG.2019.8756576</a>.
</div>
<div id="ref-mediapipe" class="csl-entry" role="listitem">
Lugaresi, Camillo, Jiuqiang Tang, Hadon Nash, Chris McClanahan, Esha Uboweja, Michael Hays, Fan Zhang, et al. 2019. <span>“MediaPipe: A Framework for Perceiving and Processing Reality.”</span> In <em>Third Workshop on Computer Vision for AR/VR at IEEE Computer Vision and Pattern Recognition (CVPR) 2019</em>. <a href="https://mixedreality.cs.cornell.edu/s/NewTitle_May1_MediaPipe_CVPR_CV4ARVR_Workshop_2019.pdf">https://mixedreality.cs.cornell.edu/s/NewTitle_May1_MediaPipe_CVPR_CV4ARVR_Workshop_2019.pdf</a>.
</div>
<div id="ref-gesture-recognition-3d" class="csl-entry" role="listitem">
Molchanov, Pavlo, Xiaodong Yang, Shalini Gupta, Kihwan Kim, Stephen Tyree, and Jan Kautz. 2016. <span>“Online Detection and Classification of Dynamic Hand Gestures with Recurrent 3D Convolutional Neural Networks.”</span> In <em>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 4207–15. <a href="https://doi.org/10.1109/CVPR.2016.456">https://doi.org/10.1109/CVPR.2016.456</a>.
</div>
<div id="ref-sam2" class="csl-entry" role="listitem">
Ravi, Nikhila, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, et al. 2024. <span>“SAM 2: Segment Anything in Images and Videos.”</span> <em>arXiv Preprint arXiv:2408.00714</em>. <a href="https://arxiv.org/abs/2408.00714">https://arxiv.org/abs/2408.00714</a>.
</div>
<div id="ref-yolo" class="csl-entry" role="listitem">
Redmon, Joseph, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. <span>“You Only Look Once: Unified, Real-Time Object Detection.”</span> In <em>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 779–88. <a href="https://doi.org/10.1109/CVPR.2016.91">https://doi.org/10.1109/CVPR.2016.91</a>.
</div>
<div id="ref-saiwa2025openpose" class="csl-entry" role="listitem">
Saiva. 2025. <span>“OpenPose Vs MediaPipe: Comprehensive Comparison &amp; Analysis.”</span> <a href="https://saiwa.ai/blog/openpose-vs-mediapipe">https://saiwa.ai/blog/openpose-vs-mediapipe</a>.
</div>
<div id="ref-maskanyone-github" class="csl-entry" role="listitem">
Schilling, Martin. 2024a. <span>“MaskAnyone - the de-Identification Toolbox for Video Data.”</span> <em>GitHub Repository</em>. <a href="https://github.com/MaskAnyone/MaskAnyone" class="uri">https://github.com/MaskAnyone/MaskAnyone</a>; GitHub.
</div>
<div id="ref-schilling2023maskanyone" class="csl-entry" role="listitem">
———. 2024b. <span>“MaskAnyone: A Human Segmentation Pipeline for Pose Estimation and de-Identification in Videos.”</span> Master's Thesis, Potsdam, Germany: Hasso Plattner Institute, University of Potsdam.
</div>
<div id="ref-ted-curiosity" class="csl-entry" role="listitem">
Shahidi, Yara. 2023. <span>“Let Curiosity Lead.”</span> TED Talks. <a href="https://www.ted.com/talks/yara_shahidi_let_curiosity_lead">https://www.ted.com/talks/yara_shahidi_let_curiosity_lead</a>.
</div>
<div id="ref-ted" class="csl-entry" role="listitem">
<span>“TED.”</span> 2025. TED Talks. <a href="https://www.ted.com/">https://www.ted.com/</a>.
</div>
<div id="ref-samurai" class="csl-entry" role="listitem">
Yang, Cheng-Yen, Hsiang-Wei Huang, Wenhao Chai, Zhongyu Jiang, and Jenq-Neng Hwang. 2024. <span>“SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory.”</span> <a href="https://arxiv.org/abs/2411.11922">https://arxiv.org/abs/2411.11922</a>.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb20" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "MaskBench - A Comprehensive Benchmark Framework for 2D Pose Estimation and Video De-Identification"</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2025 08 08"</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="co">    - name: Tim Riedel</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="co">      affiliation: Hasso Plattner Institute, University of Potsdam, Germany</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="co">    - name: Zainab Zafari</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co">      affiliation: Hasso Plattner Institute, University of Potsdam, Germany</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a><span class="co">    - name: Sharjeel Shaik</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="co">      affiliation: University of Potsdam, Germany</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="co">    - name: Babajide Alamu Owoyele</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a><span class="co">      affiliation: Hasso Plattner Institute, University of Potsdam, Germany</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a><span class="co">    - name: Wim Pouw</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a><span class="co">      affiliation: Tilburg University, Netherlands</span></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a><span class="an">contact:</span></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a><span class="co">    - name: Tim Riedel</span></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a><span class="co">      email: tim.riedel@student.hpi.de</span></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a><span class="co">    - name: Zainab Zafari</span></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a><span class="co">      email: zainab.zafari@student.hpi.de</span></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a><span class="co">    - name: Babajide Alamu Owoyele</span></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a><span class="co">      email: babajide.owoyele@.hpi.de</span></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a><span class="co">    - name: Wim Pouw</span></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a><span class="co">      email: w.pouw@tilburguniversity.edu</span></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> dependencies/refs.bib</span></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a><span class="an">css:</span><span class="co"> dependencies/styles.css</span></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a><span class="an">theme:</span></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a><span class="co">    - journal</span></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a><span class="co">    - dependencies/theme.scss</span></span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a><span class="co">    html:</span></span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a><span class="co">        tbl-cap-location: bottom</span></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a><span class="co">        toc: true</span></span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a><span class="co">        toc-location: left</span></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a><span class="co">        toc-title: "Contents"</span></span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a><span class="co">        toc-depth: 3</span></span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a><span class="co">        number-sections: true</span></span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a><span class="co">        code-fold: true</span></span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a><span class="co">        code-tools: true</span></span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a><span class="co">        output-file: index.html</span></span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a><span class="co">        grid:</span></span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a><span class="co">            margin-width: 100px</span></span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a><span class="an">filters:</span></span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a><span class="co">    - include-code-files</span></span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a><span class="co"># engine: knitr</span></span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a><span class="fu"># Abstract {#sec-abstract}</span></span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a>Pose estimation plays a critical role in numerous computer vision applications but remains challenging in scenarios involving privacy-sensitive data and in real-world, unconstrained videos like TED Talks, that are not recorded under controlled laboratory conditions.</span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a>To address the issue of sharing datasets across academic institutions without compromising privacy, we explore how masking strategies like blurring, pixelation, contour overlays, and solid fills impact pose estimation performance.</span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a>We introduce MaskBench, a modular and extensible benchmarking framework designed to evaluate pose estimators under varying conditions, including masked video inputs.</span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a>MaskBench integrates a total of seven pose estimators, including YoloPose, MediaPipePose, OpenPose, and both automated and human-in-the-loop variants of MaskAnyone, a multi-stage pipeline combining segmentation and pose estimation through a mixture-of-expert approach.</span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a>Our evaluation was done on four datasets with increasing scene complexity, and uses both kinematic metrics like velocity, acceleration, and jerk as well as accuracy-based metrics like Percentage of Correct Keypoints (PCK) and Root Mean Square Error (RMSE).</span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a>Results show that MaskAnyone variants significantly improve the visual quality of the pose estimation by reducing jitter and improving keypoint stability, especially the human-in-the-loop variants of MaskAnyone-MediaPipe.</span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a>These visual results are supported by quantitative metrics, with the aforementioned models achieving the lowest acceleration and jerk values across all datasets.</span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a>YoloPose consistently ranks as the most robust standalone model.</span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a>Regarding masking techniques, preliminary results suggest that blurring offers a promising balance between privacy and pose estimation quality.</span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a>However, since this experiment was conducted on a limited set of videos, further investigation is needed to draw general conclusions.</span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a>These findings highlight the potential of pipelines like MaskAnyone and the extensibility of MaskBench for future research on pose estimation under privacy-preserving constraints.</span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-62"><a href="#cb20-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-63"><a href="#cb20-63" aria-hidden="true" tabindex="-1"></a><span class="fu"># Getting Started {#sec-installation}</span></span>
<span id="cb20-64"><a href="#cb20-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-65"><a href="#cb20-65" aria-hidden="true" tabindex="-1"></a><span class="fu">## 🛠️ Installation {#sec-installation-setup}</span></span>
<span id="cb20-66"><a href="#cb20-66" aria-hidden="true" tabindex="-1"></a>**System Requirements**</span>
<span id="cb20-67"><a href="#cb20-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-68"><a href="#cb20-68" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>🐳 **Docker**: Latest stable version  </span>
<span id="cb20-69"><a href="#cb20-69" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>🎮 **GPU**: NVIDIA (CUDA-enabled)  </span>
<span id="cb20-70"><a href="#cb20-70" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>💾 **Memory**: 20 GB or more</span>
<span id="cb20-71"><a href="#cb20-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-72"><a href="#cb20-72" aria-hidden="true" tabindex="-1"></a>Follow the instructions below to install and run experiments with MaskBench:</span>
<span id="cb20-73"><a href="#cb20-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-74"><a href="#cb20-74" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Install Docker** and ensure the daemon is running.</span>
<span id="cb20-75"><a href="#cb20-75" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Clone this repo**:</span>
<span id="cb20-76"><a href="#cb20-76" aria-hidden="true" tabindex="-1"></a>   <span class="in">```bash</span></span>
<span id="cb20-77"><a href="#cb20-77" aria-hidden="true" tabindex="-1"></a>   <span class="fu">git</span> clone https://github.com/maskbench/maskbench.git</span>
<span id="cb20-78"><a href="#cb20-78" aria-hidden="true" tabindex="-1"></a>   <span class="in">```</span></span>
<span id="cb20-79"><a href="#cb20-79" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Setup the folder structure**. You can store the datasets, outputs or weights in any location you want (for example, if your dataset is large and stored on a separate disk), however, to minimize setup overhead, we suggest the following folder structure:</span>
<span id="cb20-80"><a href="#cb20-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-81"><a href="#cb20-81" aria-hidden="true" tabindex="-1"></a>    <span class="in">```bash</span></span>
<span id="cb20-82"><a href="#cb20-82" aria-hidden="true" tabindex="-1"></a>    <span class="ex">maskbench/</span></span>
<span id="cb20-83"><a href="#cb20-83" aria-hidden="true" tabindex="-1"></a>    <span class="ex">├──</span> src</span>
<span id="cb20-84"><a href="#cb20-84" aria-hidden="true" tabindex="-1"></a>    <span class="ex">└──</span> config/</span>
<span id="cb20-85"><a href="#cb20-85" aria-hidden="true" tabindex="-1"></a>        <span class="ex">└──</span> your-experiment-config.yml</span>
<span id="cb20-86"><a href="#cb20-86" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-87"><a href="#cb20-87" aria-hidden="true" tabindex="-1"></a>    <span class="ex">maskbench_assets/</span></span>
<span id="cb20-88"><a href="#cb20-88" aria-hidden="true" tabindex="-1"></a>    <span class="ex">├──</span> weights</span>
<span id="cb20-89"><a href="#cb20-89" aria-hidden="true" tabindex="-1"></a>    <span class="ex">├──</span> output</span>
<span id="cb20-90"><a href="#cb20-90" aria-hidden="true" tabindex="-1"></a>    <span class="ex">└──</span> datasets/</span>
<span id="cb20-91"><a href="#cb20-91" aria-hidden="true" tabindex="-1"></a>        <span class="ex">└──</span> your-dataset/</span>
<span id="cb20-92"><a href="#cb20-92" aria-hidden="true" tabindex="-1"></a>            <span class="ex">├──</span> videos/</span>
<span id="cb20-93"><a href="#cb20-93" aria-hidden="true" tabindex="-1"></a>            <span class="ex">│</span>   └── video_name1.mp4</span>
<span id="cb20-94"><a href="#cb20-94" aria-hidden="true" tabindex="-1"></a>            <span class="ex">├──</span> labels/</span>
<span id="cb20-95"><a href="#cb20-95" aria-hidden="true" tabindex="-1"></a>            <span class="ex">│</span>   └── video_name1.json</span>
<span id="cb20-96"><a href="#cb20-96" aria-hidden="true" tabindex="-1"></a>            <span class="ex">├──</span> maskanyone_ui_mediapipe/</span>
<span id="cb20-97"><a href="#cb20-97" aria-hidden="true" tabindex="-1"></a>            <span class="ex">│</span>   └── video_name1.json</span>
<span id="cb20-98"><a href="#cb20-98" aria-hidden="true" tabindex="-1"></a>            <span class="ex">└──</span> maskanyone_ui_openpose/</span>
<span id="cb20-99"><a href="#cb20-99" aria-hidden="true" tabindex="-1"></a>                <span class="ex">└──</span> video_name1.json</span>
<span id="cb20-100"><a href="#cb20-100" aria-hidden="true" tabindex="-1"></a>    <span class="in">```</span></span>
<span id="cb20-101"><a href="#cb20-101" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Switch to the git repository**</span>
<span id="cb20-102"><a href="#cb20-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-103"><a href="#cb20-103" aria-hidden="true" tabindex="-1"></a>    <span class="in">```bash</span></span>
<span id="cb20-104"><a href="#cb20-104" aria-hidden="true" tabindex="-1"></a>    <span class="bu">cd</span> maskbench</span>
<span id="cb20-105"><a href="#cb20-105" aria-hidden="true" tabindex="-1"></a>    <span class="in">```</span></span>
<span id="cb20-106"><a href="#cb20-106" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Create the environment file**. This file is used to tell MaskBench about your dataset, output and weights directory, as well as the configuration file to use for an experiment. Copy the .env file using:</span>
<span id="cb20-107"><a href="#cb20-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-108"><a href="#cb20-108" aria-hidden="true" tabindex="-1"></a>    <span class="in">```bash</span></span>
<span id="cb20-109"><a href="#cb20-109" aria-hidden="true" tabindex="-1"></a>    <span class="fu">cp</span> .env.dist .env</span>
<span id="cb20-110"><a href="#cb20-110" aria-hidden="true" tabindex="-1"></a>    <span class="in">```</span></span>
<span id="cb20-111"><a href="#cb20-111" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>**Edit the .env file**. Open it using <span class="in">`vim .env`</span> or <span class="in">`nano .env.`</span>. Adjust the following variables:</span>
<span id="cb20-112"><a href="#cb20-112" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span><span class="in">`MASKBENCH_GPU_ID:`</span> If you are on a multi-GPU setup, tell MaskBench which GPU to use. Either specify a number (0, 1, ...) or "all" in which case all available GPUs on the system are used. Currently, MaskBench only supports inference on a single GPU or on all GPUs.</span>
<span id="cb20-113"><a href="#cb20-113" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span><span class="in">`MASKBENCH_CONFIG_FILE:`</span> The configuration file used to define your experiment setup.</span>
<span id="cb20-114"><a href="#cb20-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-115"><a href="#cb20-115" aria-hidden="true" tabindex="-1"></a>    The following variables only need to be adjusted, if you use a different folder structure than the one proposed above:</span>
<span id="cb20-116"><a href="#cb20-116" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span><span class="in">`MASKBENCH_DATASET_DIR:`</span> The directory where video files are located. MaskBench supports video files with .mp4 and .avi extensions.</span>
<span id="cb20-117"><a href="#cb20-117" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span><span class="in">`MASKBENCH_OUTPUT_DIR:`</span> The directory where experiment results will be saved.</span>
<span id="cb20-118"><a href="#cb20-118" aria-hidden="true" tabindex="-1"></a><span class="ss">    * </span><span class="in">`MASKBENCH_WEIGHTS_DIR:`</span> Directory for storing model weights (e.g., MediaPipe, YOLOv11, OpenPose).</span>
<span id="cb20-119"><a href="#cb20-119" aria-hidden="true" tabindex="-1"></a><span class="ss">7. </span>**Run the MaskBench Docker container**.</span>
<span id="cb20-120"><a href="#cb20-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-121"><a href="#cb20-121" aria-hidden="true" tabindex="-1"></a>    <span class="in">```bash</span></span>
<span id="cb20-122"><a href="#cb20-122" aria-hidden="true" tabindex="-1"></a>    <span class="ex">docker</span> compose up</span>
<span id="cb20-123"><a href="#cb20-123" aria-hidden="true" tabindex="-1"></a>    <span class="in">```</span></span>
<span id="cb20-124"><a href="#cb20-124" aria-hidden="true" tabindex="-1"></a>    If multiple users run MaskBench simultaneously, use <span class="in">`docker compose -p $USER up`</span>.</span>
<span id="cb20-125"><a href="#cb20-125" aria-hidden="true" tabindex="-1"></a><span class="ss">8. </span>**Install MaskAnyone**. If you plan on using the UI version of MaskAnyone to create smooth poses, masked videos and improve raw pose estimation models, follow the installation instructions <span class="co">[</span><span class="ot">here</span><span class="co">](https://github.com/MaskAnyone/MaskAnyone)</span>.</span>
<span id="cb20-126"><a href="#cb20-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-127"><a href="#cb20-127" aria-hidden="true" tabindex="-1"></a><span class="fu">## 🚀 Usage {#sec-installation-usage}</span></span>
<span id="cb20-128"><a href="#cb20-128" aria-hidden="true" tabindex="-1"></a>The following paragraphs describe how to structure your dataset, configure the application, and understand the output of MaskBench. Following these guidelines ensures the application runs smoothly and recognizes your data correctly.</span>
<span id="cb20-129"><a href="#cb20-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-130"><a href="#cb20-130" aria-hidden="true" tabindex="-1"></a><span class="fu">### 📂 Dataset structure</span></span>
<span id="cb20-131"><a href="#cb20-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-132"><a href="#cb20-132" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Videos**: Place all videos you want to evaluate in the <span class="in">`videos`</span> folder.</span>
<span id="cb20-133"><a href="#cb20-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-134"><a href="#cb20-134" aria-hidden="true" tabindex="-1"></a>    <span class="in">```bash</span></span>
<span id="cb20-135"><a href="#cb20-135" aria-hidden="true" tabindex="-1"></a>    <span class="ex">your-dataset/</span></span>
<span id="cb20-136"><a href="#cb20-136" aria-hidden="true" tabindex="-1"></a>    <span class="ex">├──</span> videos/</span>
<span id="cb20-137"><a href="#cb20-137" aria-hidden="true" tabindex="-1"></a>    <span class="ex">│</span>    ├── video_name1.mp4</span>
<span id="cb20-138"><a href="#cb20-138" aria-hidden="true" tabindex="-1"></a>    <span class="ex">│</span>    ├── video_name2.mp4</span>
<span id="cb20-139"><a href="#cb20-139" aria-hidden="true" tabindex="-1"></a>    <span class="in">```</span></span>
<span id="cb20-140"><a href="#cb20-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-141"><a href="#cb20-141" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Labels** (Optional): If you provide labels, there must be exactly one label file for each video, with the same file name. Example:</span>
<span id="cb20-142"><a href="#cb20-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-143"><a href="#cb20-143" aria-hidden="true" tabindex="-1"></a>    <span class="in">```bash</span></span>
<span id="cb20-144"><a href="#cb20-144" aria-hidden="true" tabindex="-1"></a>    <span class="ex">your-dataset/</span></span>
<span id="cb20-145"><a href="#cb20-145" aria-hidden="true" tabindex="-1"></a>    <span class="ex">├──</span> videos/</span>
<span id="cb20-146"><a href="#cb20-146" aria-hidden="true" tabindex="-1"></a>    <span class="ex">│</span>    └── video_name1.mp4</span>
<span id="cb20-147"><a href="#cb20-147" aria-hidden="true" tabindex="-1"></a>    <span class="ex">├──</span> labels/</span>
<span id="cb20-148"><a href="#cb20-148" aria-hidden="true" tabindex="-1"></a>    <span class="ex">│</span>    └── video_name2.json</span>
<span id="cb20-149"><a href="#cb20-149" aria-hidden="true" tabindex="-1"></a>    <span class="in">```</span></span>
<span id="cb20-150"><a href="#cb20-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-151"><a href="#cb20-151" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**MaskAnyoneUI Output**: If you use MaskAnyoneUI, run the application, download the resulting pose file, store it in either the  <span class="in">`maskanyone_ui_openpose`</span> or <span class="in">`maskanyone_ui_mediapipe`</span> folder and once again name it exactly like the corresponding video file.</span>
<span id="cb20-152"><a href="#cb20-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-153"><a href="#cb20-153" aria-hidden="true" tabindex="-1"></a><span class="fu">### ⚙️ Configuration Files</span></span>
<span id="cb20-154"><a href="#cb20-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-155"><a href="#cb20-155" aria-hidden="true" tabindex="-1"></a>We provide four sample configuration files from our experiments. Feel free to copy and adapt them to your needs. The following note explains some parameters in more detail.</span>
<span id="cb20-156"><a href="#cb20-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-157"><a href="#cb20-157" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb20-158"><a href="#cb20-158" aria-hidden="true" tabindex="-1"></a><span class="fu">## Explanation of config parameters</span></span>
<span id="cb20-159"><a href="#cb20-159" aria-hidden="true" tabindex="-1"></a><span class="in">```{.yaml}</span></span>
<span id="cb20-160"><a href="#cb20-160" aria-hidden="true" tabindex="-1"></a><span class="co"># A directory name (MaskBench run name) inside the output directory from which to load existing results.</span></span>
<span id="cb20-161"><a href="#cb20-161" aria-hidden="true" tabindex="-1"></a><span class="co"># If set, inference is skipped and results are loaded. To run inference from scratch, comment out or set to "None".</span></span>
<span id="cb20-162"><a href="#cb20-162" aria-hidden="true" tabindex="-1"></a><span class="fu">inference_checkpoint_name</span><span class="kw">:</span><span class="at"> None</span></span>
<span id="cb20-163"><a href="#cb20-163" aria-hidden="true" tabindex="-1"></a><span class="fu">execute_evaluation</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span><span class="co">                    # Set to false to skip calculating evaluation metrics and plotting.</span></span>
<span id="cb20-164"><a href="#cb20-164" aria-hidden="true" tabindex="-1"></a><span class="fu">execute_rendering</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span><span class="co">                     # Set to false to skip rendering the videos.</span></span>
<span id="cb20-165"><a href="#cb20-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-166"><a href="#cb20-166" aria-hidden="true" tabindex="-1"></a><span class="fu">dataset</span><span class="kw">:</span></span>
<span id="cb20-167"><a href="#cb20-167" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> TragicTalkers</span><span class="co">                       # User-definable name of the dataset</span></span>
<span id="cb20-168"><a href="#cb20-168" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">module</span><span class="kw">:</span><span class="at"> datasets.tragic_talkers_dataset</span></span>
<span id="cb20-169"><a href="#cb20-169" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">class</span><span class="kw">:</span><span class="at"> TragicTalkersDataset</span></span>
<span id="cb20-170"><a href="#cb20-170" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">dataset_folder</span><span class="kw">:</span><span class="at"> /datasets/tragic-talkers</span><span class="co">  # Location of the dataset folder (always starts with /datasets, because this refers to the mounted folder in the docker container). You only need to adjust the name of the folder.</span></span>
<span id="cb20-171"><a href="#cb20-171" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">config</span><span class="kw">:</span></span>
<span id="cb20-172"><a href="#cb20-172" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">video_folder</span><span class="kw">:</span><span class="at"> videos</span></span>
<span id="cb20-173"><a href="#cb20-173" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">ground_truth_folder</span><span class="kw">:</span><span class="at"> labels</span></span>
<span id="cb20-174"><a href="#cb20-174" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">convert_gt_keypoints_to_coco</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb20-175"><a href="#cb20-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-176"><a href="#cb20-176" aria-hidden="true" tabindex="-1"></a><span class="fu">pose_estimators</span><span class="kw">:</span><span class="co">                            # List of pose estimators (specificy as many as needed)</span></span>
<span id="cb20-177"><a href="#cb20-177" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> YoloPose</span><span class="co">                          # User-definable name of the pose estimator. </span></span>
<span id="cb20-178"><a href="#cb20-178" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">enabled</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span><span class="co">                           # Enable or disable this pose estimator.</span></span>
<span id="cb20-179"><a href="#cb20-179" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">module</span><span class="kw">:</span><span class="at"> models.yolo_pose_estimator</span></span>
<span id="cb20-180"><a href="#cb20-180" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">class</span><span class="kw">:</span><span class="at"> YoloPoseEstimator</span></span>
<span id="cb20-181"><a href="#cb20-181" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">config</span><span class="kw">:</span><span class="co">                                 # Pose estimator specific configuration variables.</span></span>
<span id="cb20-182"><a href="#cb20-182" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">weights</span><span class="kw">:</span><span class="at"> yolo11l-pose.pt</span><span class="co">              # Weights file name inside the specified weights directory.</span></span>
<span id="cb20-183"><a href="#cb20-183" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">save_keypoints_in_coco_format</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span><span class="co">   # Whether to store keypoints in COCO format (18 keypoints) or not)</span></span>
<span id="cb20-184"><a href="#cb20-184" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">confidence_threshold</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.3</span><span class="co">             # Confidence threshold below which keyopints are considered undetected.</span></span>
<span id="cb20-185"><a href="#cb20-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-186"><a href="#cb20-186" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> MaskAnyoneUI-MediaPipe</span></span>
<span id="cb20-187"><a href="#cb20-187" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">enabled</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb20-188"><a href="#cb20-188" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">module</span><span class="kw">:</span><span class="at"> models.maskanyone_ui_pose_estimator</span></span>
<span id="cb20-189"><a href="#cb20-189" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">class</span><span class="kw">:</span><span class="at"> MaskAnyoneUiPoseEstimator  </span></span>
<span id="cb20-190"><a href="#cb20-190" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">config</span><span class="kw">:</span></span>
<span id="cb20-191"><a href="#cb20-191" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">dataset_folder_path</span><span class="kw">:</span><span class="at"> /datasets/tragic-talkers/maskanyone_ui_mediapipe</span><span class="co"> # Folder of MaskAnyone poses</span></span>
<span id="cb20-192"><a href="#cb20-192" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">overlay_strategy</span><span class="kw">:</span><span class="at"> mp_pose</span><span class="co">             # Mediapipe: mp_pose, OpenPose: openpose, openpose_body25b</span></span>
<span id="cb20-193"><a href="#cb20-193" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">save_keypoints_in_coco_format</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb20-194"><a href="#cb20-194" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">confidence_threshold</span><span class="kw">:</span><span class="at"> </span><span class="dv">0</span><span class="co">               # Confidence thresholds not supported by MaskAnyone</span></span>
<span id="cb20-195"><a href="#cb20-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-196"><a href="#cb20-196" aria-hidden="true" tabindex="-1"></a><span class="fu">metrics</span><span class="kw">:</span><span class="co">                                    # List of metrics  (specificy as many as needed)</span></span>
<span id="cb20-197"><a href="#cb20-197" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> PCK</span><span class="co">                               # User-definable name of the metric</span></span>
<span id="cb20-198"><a href="#cb20-198" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">module</span><span class="kw">:</span><span class="at"> evaluation.metrics.pck</span></span>
<span id="cb20-199"><a href="#cb20-199" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">class</span><span class="kw">:</span><span class="at"> PCKMetric</span></span>
<span id="cb20-200"><a href="#cb20-200" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">config</span><span class="kw">:</span><span class="co">                                 # Metric specific configuration variables.</span></span>
<span id="cb20-201"><a href="#cb20-201" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">normalize_by</span><span class="kw">:</span><span class="at"> bbox</span></span>
<span id="cb20-202"><a href="#cb20-202" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">threshold</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.2</span></span>
<span id="cb20-203"><a href="#cb20-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-204"><a href="#cb20-204" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> Velocity</span></span>
<span id="cb20-205"><a href="#cb20-205" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">module</span><span class="kw">:</span><span class="at"> evaluation.metrics.velocity</span></span>
<span id="cb20-206"><a href="#cb20-206" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">class</span><span class="kw">:</span><span class="at"> VelocityMetric</span></span>
<span id="cb20-207"><a href="#cb20-207" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">config</span><span class="kw">:</span></span>
<span id="cb20-208"><a href="#cb20-208" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">time_unit</span><span class="kw">:</span><span class="at"> frame</span></span>
<span id="cb20-209"><a href="#cb20-209" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-210"><a href="#cb20-210" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-211"><a href="#cb20-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-212"><a href="#cb20-212" aria-hidden="true" tabindex="-1"></a><span class="fu">### 🏋️ Model Variants and Weights</span></span>
<span id="cb20-213"><a href="#cb20-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-214"><a href="#cb20-214" aria-hidden="true" tabindex="-1"></a>You only need to modify the weights if you are adding a new pose estimator to MaskBench.</span>
<span id="cb20-215"><a href="#cb20-215" aria-hidden="true" tabindex="-1"></a>In that case, place your weights in the <span class="in">`weights/`</span> folder.</span>
<span id="cb20-216"><a href="#cb20-216" aria-hidden="true" tabindex="-1"></a>By default, MaskBench automatically downloads the following weights:</span>
<span id="cb20-217"><a href="#cb20-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-218"><a href="#cb20-218" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>MediaPipe: <span class="in">`pose_landmarker_{lite, full, heavy}.task`</span></span>
<span id="cb20-219"><a href="#cb20-219" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Yolo11: <span class="in">`yolo11{n, s, m, l, x}-pose`</span></span>
<span id="cb20-220"><a href="#cb20-220" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>OpenPose overlay_strategy: <span class="in">`BODY_25, BODY_25B, COCO`</span></span>
<span id="cb20-221"><a href="#cb20-221" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>MaskAnyone overlay_strategy: <span class="in">`mp_pose, openpose, openpose_body25b`</span></span>
<span id="cb20-222"><a href="#cb20-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-223"><a href="#cb20-223" aria-hidden="true" tabindex="-1"></a><span class="fu">### 📊 Output</span></span>
<span id="cb20-224"><a href="#cb20-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-225"><a href="#cb20-225" aria-hidden="true" tabindex="-1"></a>All results, including plots, pose files, inference times and renderings, will be saved in the output directory.</span>
<span id="cb20-226"><a href="#cb20-226" aria-hidden="true" tabindex="-1"></a>For each run of MaskBench a folder is created with the name of the dataset and a timestamp. Example:</span>
<span id="cb20-227"><a href="#cb20-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-228"><a href="#cb20-228" aria-hidden="true" tabindex="-1"></a><span class="in">```bash</span></span>
<span id="cb20-229"><a href="#cb20-229" aria-hidden="true" tabindex="-1"></a><span class="ex">output/</span></span>
<span id="cb20-230"><a href="#cb20-230" aria-hidden="true" tabindex="-1"></a> <span class="ex">├──</span> TedTalks_2025-08-11_15-42-10/</span>
<span id="cb20-231"><a href="#cb20-231" aria-hidden="true" tabindex="-1"></a> <span class="ex">│</span>    ├── plots/</span>
<span id="cb20-232"><a href="#cb20-232" aria-hidden="true" tabindex="-1"></a> <span class="ex">│</span>    ├── poses/</span>
<span id="cb20-233"><a href="#cb20-233" aria-hidden="true" tabindex="-1"></a> <span class="ex">│</span>    ├── renderings/</span>
<span id="cb20-234"><a href="#cb20-234" aria-hidden="true" tabindex="-1"></a> <span class="ex">│</span>    ├── inference_times.json</span>
<span id="cb20-235"><a href="#cb20-235" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-236"><a href="#cb20-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-237"><a href="#cb20-237" aria-hidden="true" tabindex="-1"></a><span class="fu"># Related Work {#sec-related-work}</span></span>
<span id="cb20-238"><a href="#cb20-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-239"><a href="#cb20-239" aria-hidden="true" tabindex="-1"></a>Human pose estimation is a core task in computer vision, concerned with identifying the spatial positions of body joints—such as shoulders, elbows, and knees—from images or video sequences. Existing approaches are typically classified into two broad strategies: top-down and bottom-up. Top-down methods begin by detecting individual persons within an image, after which a separate pose estimation model is applied to each detected instance. In contrast, bottom-up approaches first detect all keypoints across the image and then group them to form full-body poses for each individual <span class="co">[</span><span class="ot">@saiwa2025openpose; @kaim2024comparison</span><span class="co">]</span>.</span>
<span id="cb20-240"><a href="#cb20-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-241"><a href="#cb20-241" aria-hidden="true" tabindex="-1"></a>Among widely used frameworks, **OpenPose** <span class="co">[</span><span class="ot">@openpose-3</span><span class="co">]</span> is a prominent example of a bottom-up pose estimation method. It first identifies keypoints across the entire image and then assembles them into person-wise skeletons using Part Affinity Fields (PAFs). OpenPose supports multiple configurations, including 18- and 25-keypoint body models @fig-openpose, and offers full-body tracking, hand and facial landmark estimation. The framework is optimized for GPU execution and is widely used in applications requiring multi-person pose estimation.</span>
<span id="cb20-242"><a href="#cb20-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-243"><a href="#cb20-243" aria-hidden="true" tabindex="-1"></a>**YOLO11** <span class="co">[</span><span class="ot">@yolo; @yolo11_ultralytics</span><span class="co">]</span>, on the other hand, follows a top-down approach. It extends the YOLO family of real-time object detectors by incorporating pose estimation capabilities. After detecting bounding boxes for each person, YOLO11 predicts 17 body keypoints @fig-yolo per individual, using a topology aligned with the COCO keypoint format. It is designed for high-performance scenarios and is optimized for GPU usage, making it suitable for real-time, multi-person tracking in high-resolution video streams.</span>
<span id="cb20-244"><a href="#cb20-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-245"><a href="#cb20-245" aria-hidden="true" tabindex="-1"></a>**MediaPipe Pose** <span class="co">[</span><span class="ot">@mediapipe</span><span class="co">]</span> is a lightweight, top-down framework designed specifically for real-time pose estimation on CPU-only devices. Built upon BlazePose, it employs a 33-landmark skeleton @fig-mediapipe that extends the standard COCO format with additional joints to improve anatomical precision. The pipeline consists of an initial detection and tracking stage, followed by landmark prediction and overlay. MediaPipe is particularly suited for single-person applications in mobile and browser environments, where computational efficiency and low latency are critical.</span>
<span id="cb20-246"><a href="#cb20-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-247"><a href="#cb20-247" aria-hidden="true" tabindex="-1"></a>::: {#fig-keypoints layout-ncol=3}</span>
<span id="cb20-248"><a href="#cb20-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-249"><a href="#cb20-249" aria-hidden="true" tabindex="-1"></a><span class="al">![OpenPose](images/openpose-keypointbody25.png)</span>{#fig-openpose height=200px}</span>
<span id="cb20-250"><a href="#cb20-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-251"><a href="#cb20-251" aria-hidden="true" tabindex="-1"></a><span class="al">![YOLO](images/yolo-keypoints.png)</span>{#fig-yolo height=200px}</span>
<span id="cb20-252"><a href="#cb20-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-253"><a href="#cb20-253" aria-hidden="true" tabindex="-1"></a><span class="al">![MediaPipe](images/mediapipe-landmarks.png)</span>{#fig-mediapipe height=200px}</span>
<span id="cb20-254"><a href="#cb20-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-255"><a href="#cb20-255" aria-hidden="true" tabindex="-1"></a>Some of the most common pose models and their keypoints. **YOLOv11** uses the COCO format with 17 keypoints. **OpenPose** supports COCO (18) and BODY-25 (25). **MediaPipe** uses a 33-landmark skeleton.</span>
<span id="cb20-256"><a href="#cb20-256" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-257"><a href="#cb20-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-258"><a href="#cb20-258" aria-hidden="true" tabindex="-1"></a>Several studies have benchmarked popular pose estimation models across different datasets, conditions, and use cases. The following works are particularly relevant to our benchmarking framework:</span>
<span id="cb20-259"><a href="#cb20-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-260"><a href="#cb20-260" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**Comparision Of ML Models For Posture** <span class="co">[</span><span class="ot">@kaim2024comparison</span><span class="co">]</span></span>
<span id="cb20-261"><a href="#cb20-261" aria-hidden="true" tabindex="-1"></a>    Compared YOLOv7 Pose and MediaPipe Pose. YOLOv7 achieved a slightly higher accuracy score of 87.8\% versus MediaPipe’s 84.1\%. However, MediaPipe demonstrated superior real-time performance on CPU-only devices, achieving 16-18 frames per second (FPS) compared to YOLOv7’s 4-5 FPS. In low-light environments, MediaPipe maintained detection consistency, whereas YOLOv7 performed better in occluded scenarios, successfully recognizing hidden body parts. </span>
<span id="cb20-262"><a href="#cb20-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-263"><a href="#cb20-263" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**OpenPose vs MediaPipe: A Practical and Architectural Comparison** <span class="co">[</span><span class="ot">@saiwa2025openpose</span><span class="co">]</span></span>
<span id="cb20-264"><a href="#cb20-264" aria-hidden="true" tabindex="-1"></a>    A recent blog post by Saiwa presents a detailed comparison between OpenPose and MediaPipe, discussing their architectural differences, device compatibility, and practical applications. OpenPose uses a bottom-up approach with Part Affinity Fields and is optimized for multi-person full-body tracking, whereas MediaPipe follows a top-down strategy focusing on speed and cross-platform deployment.</span>
<span id="cb20-265"><a href="#cb20-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-266"><a href="#cb20-266" aria-hidden="true" tabindex="-1"></a>In addition to standalone pose estimation models, **MaskAnyone** <span class="co">[</span><span class="ot">@maskanyone-github; @schilling2023maskanyone</span><span class="co">]</span> is a multi-stage framework developed at the Hasso Plattner Institute (HPI) that combines object detection, segmentation, de-identification, and pose estimation within a unified pipeline. The system begins by detecting human instances in a video using YOLO. For each detected bounding box, SAM2 <span class="co">[</span><span class="ot">@sam2</span><span class="co">]</span> is applied to segment the individual subject. Depending on the configuration, pose estimation is then performed using either OpenPose or MediaPipe. Last but not least, it produces a de-identified video using the SAM2 segmentation masks. The framework supports both fully automatic processing (we refer to it as MaskAnyoneAPI) and a human-in-the-loop approach (referred to as MaskAnyoneUI), where users can manually select specific frames, refine the segmentation output of SAM2 and thereafter start the pose estimation. This combination of automated and user-guided steps allows for finer control in scenarios where automatic segmentation may be insufficient or require correction.</span>
<span id="cb20-267"><a href="#cb20-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-268"><a href="#cb20-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-269"><a href="#cb20-269" aria-hidden="true" tabindex="-1"></a><span class="fu"># MaskBench Architecture {#sec-architecture}</span></span>
<span id="cb20-270"><a href="#cb20-270" aria-hidden="true" tabindex="-1"></a><span class="al">![MaskBench Architecture](./images/maskbench-workflow.png)</span>{#fig-architecture}</span>
<span id="cb20-271"><a href="#cb20-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-272"><a href="#cb20-272" aria-hidden="true" tabindex="-1"></a>The general workflow of MaskBench is shown in Figure @fig-architecture.</span>
<span id="cb20-273"><a href="#cb20-273" aria-hidden="true" tabindex="-1"></a>It begins with loading the dataset, pose estimators, and evaluation metrics.</span>
<span id="cb20-274"><a href="#cb20-274" aria-hidden="true" tabindex="-1"></a>The application then creates a checkpoint folder in the specified output directory, named according to the dataset and a timestamp (e.g., <span class="in">`/output/TedTalks-20250724-121127`</span>).</span>
<span id="cb20-275"><a href="#cb20-275" aria-hidden="true" tabindex="-1"></a>Subsequently, inference is performed on all videos in the dataset using the pose estimators specified in the configuration file.</span>
<span id="cb20-276"><a href="#cb20-276" aria-hidden="true" tabindex="-1"></a>For the MaskAnyoneUI pose estimators, the user is required to perform semi-automatic annotation of the videos using MaskAnyone before starting the MaskBench run.</span>
<span id="cb20-277"><a href="#cb20-277" aria-hidden="true" tabindex="-1"></a>A poses folder is created within the checkpoint, containing a subfolder for each pose estimator and a single JSON file for each video.</span>
<span id="cb20-278"><a href="#cb20-278" aria-hidden="true" tabindex="-1"></a>The application then evaluates all specified metrics and generates plots, which are stored in the plots folder within the checkpoint.</span>
<span id="cb20-279"><a href="#cb20-279" aria-hidden="true" tabindex="-1"></a>Finally, for each video, the application produces a set of rendered videos—one for each pose estimator—which are stored in the renderings folder in the checkpoint.</span>
<span id="cb20-280"><a href="#cb20-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-281"><a href="#cb20-281" aria-hidden="true" tabindex="-1"></a>Each component of MaskBench is implemented in a modular way, so it can be easily extended and modified.</span>
<span id="cb20-282"><a href="#cb20-282" aria-hidden="true" tabindex="-1"></a>We will discuss this in the following sections.</span>
<span id="cb20-283"><a href="#cb20-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-284"><a href="#cb20-284" aria-hidden="true" tabindex="-1"></a><span class="fu">## Dataset {#sec-architecture-dataset}</span></span>
<span id="cb20-285"><a href="#cb20-285" aria-hidden="true" tabindex="-1"></a>The dataset provides video data for pose estimation and, if available, ground truth data for evaluation.</span>
<span id="cb20-286"><a href="#cb20-286" aria-hidden="true" tabindex="-1"></a>When adding a new dataset, the user must create a new class that inherits from the Dataset class and override the <span class="in">`_load_samples`</span> method, which generates one VideoSample object for each video in the dataset.</span>
<span id="cb20-287"><a href="#cb20-287" aria-hidden="true" tabindex="-1"></a>If the dataset provides ground truth data, the user must also override the <span class="in">`get_gt_pose_results`</span> and <span class="in">`get_gt_keypoint_pairs`</span> methods.</span>
<span id="cb20-288"><a href="#cb20-288" aria-hidden="true" tabindex="-1"></a>For each video, the <span class="in">`get_gt_pose_results`</span> method should return a <span class="in">`VideoPoseResult`</span> object.</span>
<span id="cb20-289"><a href="#cb20-289" aria-hidden="true" tabindex="-1"></a>The <span class="in">`get_gt_keypoint_pairs`</span> method is used to render the ground truth keypoints and contains a list of tuples, each specifying the indices of two keypoints to be connected in the rendered video.</span>
<span id="cb20-290"><a href="#cb20-290" aria-hidden="true" tabindex="-1"></a>Default keypoint pairs for YoloPose, MediaPipePose, and various implementations of OpenPose models are provided in the <span class="in">`keypoint_pairs.py`</span> file.</span>
<span id="cb20-291"><a href="#cb20-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-292"><a href="#cb20-292" aria-hidden="true" tabindex="-1"></a>Below is the code implementation for the abstract dataset class, the simple TED Talks dataset (without ground truth), and the more complex TragicTalkers dataset (with pseudo-ground truth data).</span>
<span id="cb20-293"><a href="#cb20-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-294"><a href="#cb20-294" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb20-295"><a href="#cb20-295" aria-hidden="true" tabindex="-1"></a><span class="fu">## Dataset Class</span></span>
<span id="cb20-296"><a href="#cb20-296" aria-hidden="true" tabindex="-1"></a><span class="in">```{.python include="../src/datasets/dataset.py" code-line-numbers="true" filename="src/datasets/dataset.py"}</span></span>
<span id="cb20-297"><a href="#cb20-297" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-298"><a href="#cb20-298" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-299"><a href="#cb20-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-300"><a href="#cb20-300" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb20-301"><a href="#cb20-301" aria-hidden="true" tabindex="-1"></a><span class="fu">## TED Talks Dataset</span></span>
<span id="cb20-302"><a href="#cb20-302" aria-hidden="true" tabindex="-1"></a><span class="in">```{.python include="../src/datasets/ted_dataset.py" code-line-numbers="true" filename="src/datasets/ted_dataset.py"}</span></span>
<span id="cb20-303"><a href="#cb20-303" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-304"><a href="#cb20-304" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-305"><a href="#cb20-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-306"><a href="#cb20-306" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb20-307"><a href="#cb20-307" aria-hidden="true" tabindex="-1"></a><span class="fu">## Tragic Talkers Dataset</span></span>
<span id="cb20-308"><a href="#cb20-308" aria-hidden="true" tabindex="-1"></a><span class="in">```{.python include="../src/datasets/tragic_talkers_dataset.py" code-line-numbers="true" filename="src/datasets/tragic_talkers_dataset.py"}</span></span>
<span id="cb20-309"><a href="#cb20-309" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-310"><a href="#cb20-310" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-311"><a href="#cb20-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-312"><a href="#cb20-312" aria-hidden="true" tabindex="-1"></a><span class="fu">## Inference {#sec-architecture-inference}</span></span>
<span id="cb20-313"><a href="#cb20-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-314"><a href="#cb20-314" aria-hidden="true" tabindex="-1"></a><span class="fu">### Video Pose Result {#sec-architecture-video-pose-result}</span></span>
<span id="cb20-315"><a href="#cb20-315" aria-hidden="true" tabindex="-1"></a>The VideoPoseResult object represents the standardized output of a pose prediction model.</span>
<span id="cb20-316"><a href="#cb20-316" aria-hidden="true" tabindex="-1"></a>It is a nested structure containing a <span class="in">`FramePoseResult`</span> object for each frame in the video.</span>
<span id="cb20-317"><a href="#cb20-317" aria-hidden="true" tabindex="-1"></a>Each frame pose result includes a list of <span class="in">`PersonPoseResult`</span> objects, one for each person detected in the frame.</span>
<span id="cb20-318"><a href="#cb20-318" aria-hidden="true" tabindex="-1"></a>Every person’s result contains a list of <span class="in">`PoseKeypoint`</span> objects, one for each keypoint in the model’s output format, providing x and y coordinates along with an optional confidence score.</span>
<span id="cb20-319"><a href="#cb20-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-320"><a href="#cb20-320" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb20-321"><a href="#cb20-321" aria-hidden="true" tabindex="-1"></a><span class="fu">## Video Pose Result Class</span></span>
<span id="cb20-322"><a href="#cb20-322" aria-hidden="true" tabindex="-1"></a><span class="in">```{.python include="../src/inference/pose_result.py" code-line-numbers="true" filename="src/inference/pose_result.py"}</span></span>
<span id="cb20-323"><a href="#cb20-323" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-324"><a href="#cb20-324" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-325"><a href="#cb20-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-326"><a href="#cb20-326" aria-hidden="true" tabindex="-1"></a><span class="fu">### Pose Estimator {#sec-architecture-pose-estimators}</span></span>
<span id="cb20-327"><a href="#cb20-327" aria-hidden="true" tabindex="-1"></a>Pose estimators are responsible for predicting the poses of persons in a video by wrapping calls to specific AI models or pose estimation pipelines.</span>
<span id="cb20-328"><a href="#cb20-328" aria-hidden="true" tabindex="-1"></a>Each model is implemented in a separate class that inherits from the abstract <span class="in">`PoseEstimator`</span> class.</span>
<span id="cb20-329"><a href="#cb20-329" aria-hidden="true" tabindex="-1"></a>The output of each estimator is a standardized <span class="in">`VideoPoseResult`</span> object.</span>
<span id="cb20-330"><a href="#cb20-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-331"><a href="#cb20-331" aria-hidden="true" tabindex="-1"></a>To add a new pose estimator, users must implement methods for pose estimation and for retrieving keypoint pairs.</span>
<span id="cb20-332"><a href="#cb20-332" aria-hidden="true" tabindex="-1"></a>Special care must be taken to ensure that the output meets the following constraints:</span>
<span id="cb20-333"><a href="#cb20-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-334"><a href="#cb20-334" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>The number of frames in the pose results matches the number of frames in the video.</span>
<span id="cb20-335"><a href="#cb20-335" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>If no persons are detected in a frame, the persons list should be empty.</span>
<span id="cb20-336"><a href="#cb20-336" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>For detected persons with missing keypoints, those keypoints should have values <span class="in">`x=0, y=0, confidence=None`</span>.</span>
<span id="cb20-337"><a href="#cb20-337" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>The number of keypoints per person remains constant across all frames.</span>
<span id="cb20-338"><a href="#cb20-338" aria-hidden="true" tabindex="-1"></a><span class="ss">5.  </span>Keypoints with low confidence should be masked out using the <span class="in">`confidence_threshold`</span> configuration parameter.</span>
<span id="cb20-339"><a href="#cb20-339" aria-hidden="true" tabindex="-1"></a><span class="ss">6.  </span>Keypoints must be mapped to the COCO format if the <span class="in">`save_keypoints_in_coco_format`</span> configuration parameter is set to true.</span>
<span id="cb20-340"><a href="#cb20-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-341"><a href="#cb20-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-342"><a href="#cb20-342" aria-hidden="true" tabindex="-1"></a>As an example, we provide the implementation of the abstract pose estimator class and the implementation of the YOLO model.</span>
<span id="cb20-343"><a href="#cb20-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-344"><a href="#cb20-344" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb20-345"><a href="#cb20-345" aria-hidden="true" tabindex="-1"></a><span class="fu">## Pose Estimator Class</span></span>
<span id="cb20-346"><a href="#cb20-346" aria-hidden="true" tabindex="-1"></a><span class="in">```{.python include="../src/models/pose_estimator.py" code-line-numbers="true" filename="src/models/pose_estimator.py"}</span></span>
<span id="cb20-347"><a href="#cb20-347" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-348"><a href="#cb20-348" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-349"><a href="#cb20-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-350"><a href="#cb20-350" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb20-351"><a href="#cb20-351" aria-hidden="true" tabindex="-1"></a><span class="fu">## YOLO Model</span></span>
<span id="cb20-352"><a href="#cb20-352" aria-hidden="true" tabindex="-1"></a><span class="in">```{.python include="../src/models/yolo_pose_estimator.py" code-line-numbers="true" filename="src/models/yolo_pose_estimator.py"}</span></span>
<span id="cb20-353"><a href="#cb20-353" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-354"><a href="#cb20-354" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-355"><a href="#cb20-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-356"><a href="#cb20-356" aria-hidden="true" tabindex="-1"></a>MaskBench supports seven pose estimators, including pure AI models such as YOLOv11-Pose, MediaPipePose, and OpenPose.</span>
<span id="cb20-357"><a href="#cb20-357" aria-hidden="true" tabindex="-1"></a>Additionally, it incorporates MaskAnyone as a pose estimator, which combines multiple expert models.</span>
<span id="cb20-358"><a href="#cb20-358" aria-hidden="true" tabindex="-1"></a>We distinguish between two variants of the MaskAnyone estimator: the MaskAnyoneAPI pose estimator, which runs fully automatically during inference, and the MaskAnyoneUI pose estimator, which employs a human-in-the-loop approach allowing manual adjustment of the mask for the persons of interest.</span>
<span id="cb20-359"><a href="#cb20-359" aria-hidden="true" tabindex="-1"></a>The latter requires manual execution by the user prior to running MaskBench, with the resulting pose files provided as one file per video.</span>
<span id="cb20-360"><a href="#cb20-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-361"><a href="#cb20-361" aria-hidden="true" tabindex="-1"></a><span class="fu">### Inference Engine {#sec-architecture-inference-engine}</span></span>
<span id="cb20-362"><a href="#cb20-362" aria-hidden="true" tabindex="-1"></a>The inference engine is responsible for running pose estimators on videos and saving the results as JSON files in the poses folder.</span>
<span id="cb20-363"><a href="#cb20-363" aria-hidden="true" tabindex="-1"></a>If a checkpoint name is specified in the configuration file, the inference engine will load existing results from the checkpoint and skip inference for videos that already have corresponding outputs.</span>
<span id="cb20-364"><a href="#cb20-364" aria-hidden="true" tabindex="-1"></a>This feature allows the user to resume an already started inference process or to bypass the time-consuming inference entirely and perform only metric evaluation and rendering.</span>
<span id="cb20-365"><a href="#cb20-365" aria-hidden="true" tabindex="-1"></a>The inference engine returns a nested dictionary that maps pose estimator names to video names and their corresponding <span class="in">`VideoPoseResult`</span> objects.</span>
<span id="cb20-366"><a href="#cb20-366" aria-hidden="true" tabindex="-1"></a>Additionally, it records the inference times for each pose estimator and video, saving this information as a JSON file within the checkpoint folder.</span>
<span id="cb20-367"><a href="#cb20-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-368"><a href="#cb20-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-369"><a href="#cb20-369" aria-hidden="true" tabindex="-1"></a><span class="fu">## Evaluation {#sec-architecture-evaluation}</span></span>
<span id="cb20-370"><a href="#cb20-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-371"><a href="#cb20-371" aria-hidden="true" tabindex="-1"></a><span class="fu">### Metric {#sec-architecture-evaluation-metric}</span></span>
<span id="cb20-372"><a href="#cb20-372" aria-hidden="true" tabindex="-1"></a>Metrics play a crucial role in quantitatively evaluating the accuracy and quality of pose predictions.</span>
<span id="cb20-373"><a href="#cb20-373" aria-hidden="true" tabindex="-1"></a>Each metric inherits from the abstract <span class="in">`Metric`</span> class and implements a computation method that takes as input a predicted video pose result, an optional ground truth pose result, and the name of the pose estimator.</span>
<span id="cb20-374"><a href="#cb20-374" aria-hidden="true" tabindex="-1"></a>The <span class="in">`compute`</span> method of a metric outputs a MetricResult object containing the metric values for the video (see section @sec-architecture-evaluation-metric-result).</span>
<span id="cb20-375"><a href="#cb20-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-376"><a href="#cb20-376" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb20-377"><a href="#cb20-377" aria-hidden="true" tabindex="-1"></a><span class="fu">## Metric Class </span></span>
<span id="cb20-378"><a href="#cb20-378" aria-hidden="true" tabindex="-1"></a><span class="in">```{.python include="../src/evaluation/metrics/metric.py" code-line-numbers="true" filename="src/evaluation/metrics/metric.py"}</span></span>
<span id="cb20-379"><a href="#cb20-379" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-380"><a href="#cb20-380" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-381"><a href="#cb20-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-382"><a href="#cb20-382" aria-hidden="true" tabindex="-1"></a>MaskBench currently implements ground truth-based metrics for Euclidean Distance, Percentage of Correct Keypoints (PCK), and Root Mean Square Error (RMSE).</span>
<span id="cb20-383"><a href="#cb20-383" aria-hidden="true" tabindex="-1"></a>Furthermore, we provide kinematic metrics for velocity, acceleration, and jerk.</span>
<span id="cb20-384"><a href="#cb20-384" aria-hidden="true" tabindex="-1"></a>Section @sec-metrics contains a more extensive description of the implemented metrics.</span>
<span id="cb20-385"><a href="#cb20-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-386"><a href="#cb20-386" aria-hidden="true" tabindex="-1"></a>**Matching Person Indices**</span>
<span id="cb20-387"><a href="#cb20-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-388"><a href="#cb20-388" aria-hidden="true" tabindex="-1"></a>For some metrics, it is essential to ensure that the order of persons in the predicted video pose results matches that of the reference.</span>
<span id="cb20-389"><a href="#cb20-389" aria-hidden="true" tabindex="-1"></a>The metric class provides a method called <span class="in">`match_person_indices`</span> to align person indices between ground truth and predicted results.</span>
<span id="cb20-390"><a href="#cb20-390" aria-hidden="true" tabindex="-1"></a>This method is used not only in ground-truth-based metrics but also in kinematic metrics, which require consistent person indices across consecutive frames to compute velocity, acceleration, and other temporal measures.</span>
<span id="cb20-391"><a href="#cb20-391" aria-hidden="true" tabindex="-1"></a>The implementation employs the Hungarian algorithm, using the mean position of a person’s keypoints to find the optimal matching between all persons in the reference and predicted pose results.</span>
<span id="cb20-392"><a href="#cb20-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-393"><a href="#cb20-393" aria-hidden="true" tabindex="-1"></a>Let $N$ denote the number of persons in the reference, $M$ the number in the prediction, and $K$ the number of keypoints per person.</span>
<span id="cb20-394"><a href="#cb20-394" aria-hidden="true" tabindex="-1"></a>The output of the <span class="in">`match_person_indices`</span> method is an array with shape $\text{max}(N, M) \times K \times 2$.</span>
<span id="cb20-395"><a href="#cb20-395" aria-hidden="true" tabindex="-1"></a>The first $N$ entries correspond to persons ordered as in the reference, while the remaining $M - N$ entries (if $M &gt; N$) represent additional persons present only in the prediction.</span>
<span id="cb20-396"><a href="#cb20-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-397"><a href="#cb20-397" aria-hidden="true" tabindex="-1"></a>Edge cases include situations where a person appears in one frame but not in the next.</span>
<span id="cb20-398"><a href="#cb20-398" aria-hidden="true" tabindex="-1"></a>In such cases, the unmatched person is assigned an index with infinite values to indicate absence, while the other persons retain consistent indices.</span>
<span id="cb20-399"><a href="#cb20-399" aria-hidden="true" tabindex="-1"></a>This also applies when the prediction contains fewer persons than the reference (M &lt; N).</span>
<span id="cb20-400"><a href="#cb20-400" aria-hidden="true" tabindex="-1"></a>Each metric can then handle these infinite values appropriately, for example, by converting them to <span class="in">`NaN`</span> in kinematic metrics or assigning predefined values in Euclidean distance and ground truth–based metrics.</span>
<span id="cb20-401"><a href="#cb20-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-402"><a href="#cb20-402" aria-hidden="true" tabindex="-1"></a>**Unit Testing**</span>
<span id="cb20-403"><a href="#cb20-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-404"><a href="#cb20-404" aria-hidden="true" tabindex="-1"></a>Implementing unit tests for metric classes is essential to ensure that their outputs are accurate and consistent.</span>
<span id="cb20-405"><a href="#cb20-405" aria-hidden="true" tabindex="-1"></a>We provide unit tests for all metrics in the <span class="in">`src/tests`</span> folder, which can be executed using the <span class="in">`pytest`</span> command.</span>
<span id="cb20-406"><a href="#cb20-406" aria-hidden="true" tabindex="-1"></a>Running these tests after any modifications to the metric classes helps guarantee that existing functionality remains intact.</span>
<span id="cb20-407"><a href="#cb20-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-408"><a href="#cb20-408" aria-hidden="true" tabindex="-1"></a><span class="fu">### Metric Result {#sec-architecture-evaluation-metric-result}</span></span>
<span id="cb20-409"><a href="#cb20-409" aria-hidden="true" tabindex="-1"></a>The output of a metric’s compute method is a <span class="in">`MetricResult`</span> object.</span>
<span id="cb20-410"><a href="#cb20-410" aria-hidden="true" tabindex="-1"></a>This object contains metric values stored in a multi-dimensional array, where each axis is labeled with descriptive names such as “frame,” “person,” and “keypoint”.</span>
<span id="cb20-411"><a href="#cb20-411" aria-hidden="true" tabindex="-1"></a>The class provides an <span class="in">`aggregate`</span> function that reduces these values using a specified method along selected axes only.</span>
<span id="cb20-412"><a href="#cb20-412" aria-hidden="true" tabindex="-1"></a>Currently, MaskBench supports aggregation methods including mean, median, Root Mean Square Error (RMSE), vector magnitude, sum, minimum, and maximum.</span>
<span id="cb20-413"><a href="#cb20-413" aria-hidden="true" tabindex="-1"></a>The result of the aggregation is another MetricResult object with reduced dimensionality, retaining only the axes that were not aggregated.</span>
<span id="cb20-414"><a href="#cb20-414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-415"><a href="#cb20-415" aria-hidden="true" tabindex="-1"></a>This flexible approach of storing the results with their axes names and using the names in the aggregation method allows for the visualization of the results in a variety of ways, for example, as a per-keypoint plot, distribution plot, or as a single scalar value. Furthermore, it allows extending the framework with new metrics (possibly containing different axis names) and also different visualizations.</span>
<span id="cb20-416"><a href="#cb20-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-417"><a href="#cb20-417" aria-hidden="true" tabindex="-1"></a><span class="fu">### Evaluator {#sec-architecture-evaluation-evaluator}</span></span>
<span id="cb20-418"><a href="#cb20-418" aria-hidden="true" tabindex="-1"></a>Given a list of metrics, the evaluator executes each configured metric on the pose estimation results for all pose estimators and videos.</span>
<span id="cb20-419"><a href="#cb20-419" aria-hidden="true" tabindex="-1"></a>It returns a nested dictionary that maps metric names to pose estimator names, then to video names, and finally to their corresponding <span class="in">`MetricResult`</span> objects.</span>
<span id="cb20-420"><a href="#cb20-420" aria-hidden="true" tabindex="-1"></a>It does not perform aggregation over the videos or pose estimators in order to allow for more flexibility in the visualization of the results.</span>
<span id="cb20-421"><a href="#cb20-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-422"><a href="#cb20-422" aria-hidden="true" tabindex="-1"></a><span class="fu">## Visualization {#sec-architecture-visualization}</span></span>
<span id="cb20-423"><a href="#cb20-423" aria-hidden="true" tabindex="-1"></a>After evaluation, the results are visualized in plots and tables.</span>
<span id="cb20-424"><a href="#cb20-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-425"><a href="#cb20-425" aria-hidden="true" tabindex="-1"></a><span class="fu">### Visualizer</span></span>
<span id="cb20-426"><a href="#cb20-426" aria-hidden="true" tabindex="-1"></a>An abstract <span class="in">`BaseVisualizer`</span> class defines the interface for all visualization components.</span>
<span id="cb20-427"><a href="#cb20-427" aria-hidden="true" tabindex="-1"></a>We implemented a MaskBench-specific visualizer class tailored to our experiments, which can be reused for other studies or extended to accommodate new types of visualizations.</span>
<span id="cb20-428"><a href="#cb20-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-429"><a href="#cb20-429" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb20-430"><a href="#cb20-430" aria-hidden="true" tabindex="-1"></a><span class="fu">## MaskBench Visualizer</span></span>
<span id="cb20-431"><a href="#cb20-431" aria-hidden="true" tabindex="-1"></a><span class="in">```{.python include="../src/evaluation/visualizer/maskbench_visualizer.py" code-line-numbers="true" filename="src/visualization/maskbench_visualizer.py"}</span></span>
<span id="cb20-432"><a href="#cb20-432" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-433"><a href="#cb20-433" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-434"><a href="#cb20-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-435"><a href="#cb20-435" aria-hidden="true" tabindex="-1"></a>The visualizer saves the plots and tables in the <span class="in">`plots`</span> folder in the checkpoint.</span>
<span id="cb20-436"><a href="#cb20-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-437"><a href="#cb20-437" aria-hidden="true" tabindex="-1"></a><span class="fu">### Plots</span></span>
<span id="cb20-438"><a href="#cb20-438" aria-hidden="true" tabindex="-1"></a>Each plot inherits from the abstract <span class="in">`Plot`</span> class and implements the <span class="in">`draw`</span> method.</span>
<span id="cb20-439"><a href="#cb20-439" aria-hidden="true" tabindex="-1"></a>This method accepts various forms of input data, most commonly the results produced by the evaluator.</span>
<span id="cb20-440"><a href="#cb20-440" aria-hidden="true" tabindex="-1"></a>Each plot can define a specific approach to aggregating and organizing the data, such as computing the median over all videos for a given pose estimator.</span>
<span id="cb20-441"><a href="#cb20-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-442"><a href="#cb20-442" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb20-443"><a href="#cb20-443" aria-hidden="true" tabindex="-1"></a><span class="fu">## Plot Class</span></span>
<span id="cb20-444"><a href="#cb20-444" aria-hidden="true" tabindex="-1"></a><span class="in">```{.python include="../src/evaluation/plots/plot.py" code-line-numbers="true" filename="src/evaluation/plots/plot.py"}</span></span>
<span id="cb20-445"><a href="#cb20-445" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-446"><a href="#cb20-446" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-447"><a href="#cb20-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-448"><a href="#cb20-448" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb20-449"><a href="#cb20-449" aria-hidden="true" tabindex="-1"></a><span class="fu">## Kinematic Distribution Plot</span></span>
<span id="cb20-450"><a href="#cb20-450" aria-hidden="true" tabindex="-1"></a><span class="in">```{.python include="../src/evaluation/plots/kinematic_distribution_plot.py" code-line-numbers="true" filename="src/evaluation/plots/kinematic_distribution_plot.py"}</span></span>
<span id="cb20-451"><a href="#cb20-451" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-452"><a href="#cb20-452" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-453"><a href="#cb20-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-454"><a href="#cb20-454" aria-hidden="true" tabindex="-1"></a>We provide the following plots and tables:</span>
<span id="cb20-455"><a href="#cb20-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-456"><a href="#cb20-456" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Kinematic Distribution Plot**: Visualizes the distribution of kinematic values for each pose estimator.</span>
<span id="cb20-457"><a href="#cb20-457" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Per Keypoint Plot**:  Displays the median kinematic metric values or Euclidean distance for each COCO keypoint. This plot requires keypoints to be stored in COCO format.</span>
<span id="cb20-458"><a href="#cb20-458" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Inference Time Plot**: Visualizes the average inference time associated with each pose estimator.</span>
<span id="cb20-459"><a href="#cb20-459" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Result Table**: Aggregates results per metric and pose estimator across all videos, presenting the data in tabular form.</span>
<span id="cb20-460"><a href="#cb20-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-461"><a href="#cb20-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-462"><a href="#cb20-462" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rendering {#sec-architecture-rendering}</span></span>
<span id="cb20-463"><a href="#cb20-463" aria-hidden="true" tabindex="-1"></a>Video rendering is handled by the <span class="in">`Renderer`</span> class.</span>
<span id="cb20-464"><a href="#cb20-464" aria-hidden="true" tabindex="-1"></a>For each video in the dataset, the renderer creates a dedicated folder within the <span class="in">`renderings`</span> directory of the checkpoint folder.</span>
<span id="cb20-465"><a href="#cb20-465" aria-hidden="true" tabindex="-1"></a>Inside each video folder, it generates one video per pose estimator, displaying the rendered keypoints.</span>
<span id="cb20-466"><a href="#cb20-466" aria-hidden="true" tabindex="-1"></a>Special attention was given to maintaining consistent colors for each pose estimator across all videos and plots, using a predefined, color-blind–friendly palette.</span>
<span id="cb20-467"><a href="#cb20-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-468"><a href="#cb20-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-469"><a href="#cb20-469" aria-hidden="true" tabindex="-1"></a><span class="fu"># Datasets {#sec-datasets}</span></span>
<span id="cb20-470"><a href="#cb20-470" aria-hidden="true" tabindex="-1"></a>This study uses four video-based datasets, each representing a different level of complexity, from simple, controlled settings to more dynamic and interactive scenarios. To capture this range, we selected or created four distinct datasets: TED Kid Video <span class="co">[</span><span class="ot">@ted-kid</span><span class="co">]</span>, TED Talks <span class="co">[</span><span class="ot">@ted</span><span class="co">]</span>, Tragic Talkers <span class="co">[</span><span class="ot">@tragic-talkers</span><span class="co">]</span>, and a masked video dataset. Each dataset was chosen based on specific criteria to evaluate pose estimation models under varying degrees of difficulty.</span>
<span id="cb20-471"><a href="#cb20-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-472"><a href="#cb20-472" aria-hidden="true" tabindex="-1"></a><span class="fu">## TED Kid Video {#sec-datasets-ted-kid}</span></span>
<span id="cb20-473"><a href="#cb20-473" aria-hidden="true" tabindex="-1"></a>The TED kid video is a short, 10-second clip featuring a child in a well-lit environment from the TEDx talk "Education for all" <span class="co">[</span><span class="ot">@ted-kid</span><span class="co">]</span>.</span>
<span id="cb20-474"><a href="#cb20-474" aria-hidden="true" tabindex="-1"></a>Throughout the sequence, all body parts remain clearly visible, with no occlusion or obstruction of the subject.</span>
<span id="cb20-475"><a href="#cb20-475" aria-hidden="true" tabindex="-1"></a>This video represents a controlled scenario designed to evaluate pose estimation methods under ideal conditions, serving as a baseline for comparison with more complex datasets.</span>
<span id="cb20-476"><a href="#cb20-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-477"><a href="#cb20-477" aria-hidden="true" tabindex="-1"></a>We first tested our models' performance and evaluation metrics on this video to verify that our metrics function correctly in an ideal setting and that the implementation is accurate. This initial validation ensures that subsequent experiments on more challenging datasets can be interpreted with confidence in the correctness of our evaluation pipeline.</span>
<span id="cb20-478"><a href="#cb20-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-479"><a href="#cb20-479" aria-hidden="true" tabindex="-1"></a><span class="fu">## TED Talks {#sec-datasets-ted-talks}</span></span>
<span id="cb20-480"><a href="#cb20-480" aria-hidden="true" tabindex="-1"></a>For the TED talks dataset <span class="co">[</span><span class="ot">@ted</span><span class="co">]</span>, we selected ten videos featuring diverse speakers to capture a wide range of conditions.</span>
<span id="cb20-481"><a href="#cb20-481" aria-hidden="true" tabindex="-1"></a>The selection criteria included speaker gender, skin tone, clothing types (e.g., long dresses versus short garments), partial occlusion, and videos where only specific body parts—such as hands, upper body, or lower body—are visible.</span>
<span id="cb20-482"><a href="#cb20-482" aria-hidden="true" tabindex="-1"></a>We also considered variations in movement style and speed.</span>
<span id="cb20-483"><a href="#cb20-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-484"><a href="#cb20-484" aria-hidden="true" tabindex="-1"></a>Our focus was on evaluating model performance under more complex conditions, such as scene changes, background noise (e.g., audience sounds), partial visibility of the body, and situations where body parts are difficult to distinguish (e.g., due to long dresses).</span>
<span id="cb20-485"><a href="#cb20-485" aria-hidden="true" tabindex="-1"></a>We also accounted for visual distractions like images or patterns on the speaker’s clothing.</span>
<span id="cb20-486"><a href="#cb20-486" aria-hidden="true" tabindex="-1"></a>In each TED talk video, our analysis concentrates solely on the primary speaker.</span>
<span id="cb20-487"><a href="#cb20-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-488"><a href="#cb20-488" aria-hidden="true" tabindex="-1"></a><span class="fu">## Tragic Talkers {#sec-datasets-tragic-talkers}</span></span>
<span id="cb20-489"><a href="#cb20-489" aria-hidden="true" tabindex="-1"></a>We aimed to evaluate model performance in scenarios involving multiple people interacting.</span>
<span id="cb20-490"><a href="#cb20-490" aria-hidden="true" tabindex="-1"></a>The Tragic Talkers dataset <span class="co">[</span><span class="ot">@tragic-talkers</span><span class="co">]</span> was chosen because it provides 2D pseudo-ground truth annotations generated by the OpenPose AI model, allowing us to test metrics such as PCK or RMSE.</span>
<span id="cb20-491"><a href="#cb20-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-492"><a href="#cb20-492" aria-hidden="true" tabindex="-1"></a>The dataset features a man in regular clothing and a woman wearing a long dress.</span>
<span id="cb20-493"><a href="#cb20-493" aria-hidden="true" tabindex="-1"></a>It contains four distinct video scenarios, each originally recorded from twenty-two different camera angles.</span>
<span id="cb20-494"><a href="#cb20-494" aria-hidden="true" tabindex="-1"></a>For our analysis, we used only four angles, as many viewpoints were too similar.</span>
<span id="cb20-495"><a href="#cb20-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-496"><a href="#cb20-496" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**Monologue (Male and Female):** Individual speakers deliver monologues with relatively simple and slow movements.</span>
<span id="cb20-497"><a href="#cb20-497" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**Conversation:** A male and female speaker engage in dialogue with limited movement.</span>
<span id="cb20-498"><a href="#cb20-498" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**Interactive 1:** A conversation between a male and female speaker that includes physical interaction (e.g., hand contact), with the man sitting close to the woman.</span>
<span id="cb20-499"><a href="#cb20-499" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**Interactive 4:** A more dynamic dialogue featuring faster movements, partial occlusion, and moments of full occlusion.</span>
<span id="cb20-500"><a href="#cb20-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-501"><a href="#cb20-501" aria-hidden="true" tabindex="-1"></a>These scenarios were chosen to reflect a variety of real-world human interactions, allowing us to test how well pose estimation models perform under conditions such as occlusion, multi-person scenes, and varied movement patterns.</span>
<span id="cb20-502"><a href="#cb20-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-503"><a href="#cb20-503" aria-hidden="true" tabindex="-1"></a>However, the ground truth pose estimations were produced by an AI model rather than human annotators, which is why they are imperfect and should not be considered a true ground truth baseline (we refer to it as pseudo-ground truth).</span>
<span id="cb20-504"><a href="#cb20-504" aria-hidden="true" tabindex="-1"></a>Because the original study does not specify which OpenPose variant, parameters, or post-processing steps were used, the PCK and RMSE accuracy values should be interpreted as a measure of how closely pose estimators replicate the OpenPose output rather than as an absolute indicator of pose estimation quality.</span>
<span id="cb20-505"><a href="#cb20-505" aria-hidden="true" tabindex="-1"></a>In this context, a PCK accuracy above 80% is considered a good result, indicating that poses are generally well estimated.</span>
<span id="cb20-506"><a href="#cb20-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-507"><a href="#cb20-507" aria-hidden="true" tabindex="-1"></a><span class="fu">## Masked Video Dataset {#sec-datasets-masked-video}</span></span>
<span id="cb20-508"><a href="#cb20-508" aria-hidden="true" tabindex="-1"></a>The masked video dataset is a collection of three videos.</span>
<span id="cb20-509"><a href="#cb20-509" aria-hidden="true" tabindex="-1"></a>It includes the TED kid video, a segment from the TED talk “Let curiosity lead” <span class="co">[</span><span class="ot">@ted-curiosity</span><span class="co">]</span>, and the video “interactive1_t1-cam06” from the Tragic Talkers dataset.</span>
<span id="cb20-510"><a href="#cb20-510" aria-hidden="true" tabindex="-1"></a>This dataset was created to evaluate the performance of pose estimators on masked videos, addressing the challenge of sharing datasets containing sensitive information among researchers.</span>
<span id="cb20-511"><a href="#cb20-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-512"><a href="#cb20-512" aria-hidden="true" tabindex="-1"></a>For the dataset creation, we used MaskAnyoneUI to manually mask the persons of interest in each video using four different hiding strategies: blurring, pixelation, contours, and solid fill.</span>
<span id="cb20-513"><a href="#cb20-513" aria-hidden="true" tabindex="-1"></a>Including the original unmasked videos, this resulted in a total of 15 videos for evaluation.</span>
<span id="cb20-514"><a href="#cb20-514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-515"><a href="#cb20-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-516"><a href="#cb20-516" aria-hidden="true" tabindex="-1"></a><span class="fu">## Data Preprocessing</span></span>
<span id="cb20-517"><a href="#cb20-517" aria-hidden="true" tabindex="-1"></a>Data preprocessing was carried out on the TED talks to remove unnecessary parts of the videos and to split them into shorter segments compatible with MaskAnyone. Since MaskAnyone cannot process videos longer than 2.5 minutes, and is already resource-intensive even at that limit, we divided the TED Talk videos into chunks of 30 or 50 seconds, depending on the content.</span>
<span id="cb20-518"><a href="#cb20-518" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-519"><a href="#cb20-519" aria-hidden="true" tabindex="-1"></a>TED talks also showed some inconsistency in structure. Some videos were straightforward, with only the speaker and audience visible, making them easy to segment at any point. However, others included additional visual content such as slides, pictures, or unrelated scenes, which made it more difficult to determine clean chunking points.</span>
<span id="cb20-520"><a href="#cb20-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-521"><a href="#cb20-521" aria-hidden="true" tabindex="-1"></a>For these more complex videos, we carefully selected segment boundaries to ensure that each chunk started with frames where a human was clearly visible. When necessary, we manually trimmed the beginning of chunks to avoid starting with empty or unrelated frames. This step was critical because if a video starts with non-human content, MaskAnyone may incorrectly classify objects in the first frame as humans and then continue misdetecting them in subsequent frames.</span>
<span id="cb20-522"><a href="#cb20-522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-523"><a href="#cb20-523" aria-hidden="true" tabindex="-1"></a>No preprocessing was required for the Tragic Talkers dataset, as the videos were already clean and free of noise or unrelated visual content.</span>
<span id="cb20-524"><a href="#cb20-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-525"><a href="#cb20-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-526"><a href="#cb20-526" aria-hidden="true" tabindex="-1"></a><span class="fu"># Evaluation Metrics {#sec-metrics}</span></span>
<span id="cb20-527"><a href="#cb20-527" aria-hidden="true" tabindex="-1"></a>In the following sections, we outline the metrics used for evaluating accuracy, smoothness and jitter of different pose estimators.</span>
<span id="cb20-528"><a href="#cb20-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-529"><a href="#cb20-529" aria-hidden="true" tabindex="-1"></a><span class="fu">## Ground-Truth Metrics</span></span>
<span id="cb20-530"><a href="#cb20-530" aria-hidden="true" tabindex="-1"></a>The metrics in this section are based on ground truth data provided by the dataset and primarily evaluate the accuracy of the pose estimation compared to the reference ground truth.</span>
<span id="cb20-531"><a href="#cb20-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-532"><a href="#cb20-532" aria-hidden="true" tabindex="-1"></a><span class="fu">### Euclidean Distance {#sec-metrics-euclidean-distance}</span></span>
<span id="cb20-533"><a href="#cb20-533" aria-hidden="true" tabindex="-1"></a>The Euclidean distance metric measures the spatial accuracy of pose estimation by calculating the normalized distance between predicted and ground truth keypoint positions. </span>
<span id="cb20-534"><a href="#cb20-534" aria-hidden="true" tabindex="-1"></a>For each keypoint of a person in a frame, it computes the L2 norm (Euclidean distance) between the predicted position $(x_p, y_p)$ and the ground truth position $(x_{gt}, y_{gt})$:</span>
<span id="cb20-535"><a href="#cb20-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-536"><a href="#cb20-536" aria-hidden="true" tabindex="-1"></a>$$ d = \frac{\sqrt{(x_p - x_{gt})^2 + (y_p - y_{gt})^2}}{s} $$</span>
<span id="cb20-537"><a href="#cb20-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-538"><a href="#cb20-538" aria-hidden="true" tabindex="-1"></a>where $s$ is a normalization factor. </span>
<span id="cb20-539"><a href="#cb20-539" aria-hidden="true" tabindex="-1"></a>Normalization is essential to make the metric scale-invariant and comparable across persons of different sizes.</span>
<span id="cb20-540"><a href="#cb20-540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-541"><a href="#cb20-541" aria-hidden="true" tabindex="-1"></a>The metric is set up to support three normalization strategies, out of which we only implemented the bounding box normalization.</span>
<span id="cb20-542"><a href="#cb20-542" aria-hidden="true" tabindex="-1"></a>We outline future work for the implementation of head and torso normalization in section @sec-future-work.</span>
<span id="cb20-543"><a href="#cb20-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-544"><a href="#cb20-544" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Bounding Box Size**: The distance is normalized by the maximum of the width and height of the person’s bounding box, computed from the ground truth keypoints.</span>
<span id="cb20-545"><a href="#cb20-545" aria-hidden="true" tabindex="-1"></a>This approach adapts to varying person sizes but may introduce minor pose-dependent scaling variance.</span>
<span id="cb20-546"><a href="#cb20-546" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Head Size**: Normalization by the head bone link size (not implemented).</span>
<span id="cb20-547"><a href="#cb20-547" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Torso Size**: Normalization by the torso diameter (not implemented).</span>
<span id="cb20-548"><a href="#cb20-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-549"><a href="#cb20-549" aria-hidden="true" tabindex="-1"></a>Head and torso normalization address the pose-dependent scaling variance of the bounding box normalization.</span>
<span id="cb20-550"><a href="#cb20-550" aria-hidden="true" tabindex="-1"></a>The metric also accounts for several edge cases to ensure robust evaluation:</span>
<span id="cb20-551"><a href="#cb20-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-552"><a href="#cb20-552" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Different Order of Persons**: The metric uses the Hungarian algorithm as described in section @sec-architecture-evaluation-metric to match person indices between ground truth and predictions, ensuring that distances are calculated between corresponding persons even if they appear in different orders.</span>
<span id="cb20-553"><a href="#cb20-553" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Keypoint Missing in Ground Truth but not in Prediction**: When a keypoint is absent in the ground truth (coordinates <span class="in">`(0,0)`</span>) but detected in the prediction, the distance is set to <span class="in">`NaN`</span> and excluded from aggregation, as no valid ground truth reference exists.</span>
<span id="cb20-554"><a href="#cb20-554" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Keypoint Missing in Prediction but Present in Ground Truth**: When a keypoint exists in the ground truth but is missing in the prediction, the distance is assigned a predetermined large fill value (here, 1).</span>
<span id="cb20-555"><a href="#cb20-555" aria-hidden="true" tabindex="-1"></a>This penalizes missing detections while preventing disproportionate impact on aggregated results.</span>
<span id="cb20-556"><a href="#cb20-556" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Undetected Persons**: If a person in the ground truth is completely undetected in the prediction, all their keypoint distances are set to the same fill value to penalize the failure.</span>
<span id="cb20-557"><a href="#cb20-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-558"><a href="#cb20-558" aria-hidden="true" tabindex="-1"></a>Euclidean distance forms the basis for computing the Percentage of Correct Keypoints (PCK) and Root Mean Square Error (RMSE) metrics.</span>
<span id="cb20-559"><a href="#cb20-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-560"><a href="#cb20-560" aria-hidden="true" tabindex="-1"></a><span class="fu">### Percentage of Keypoints (PCK) {#sec-metrics-pck}</span></span>
<span id="cb20-561"><a href="#cb20-561" aria-hidden="true" tabindex="-1"></a>The Percentage of Correct Keypoints (PCK) metric evaluates pose estimation accuracy by calculating the proportion of predicted keypoints whose normalized Euclidean distance to the ground truth falls within a specified threshold.</span>
<span id="cb20-562"><a href="#cb20-562" aria-hidden="true" tabindex="-1"></a>A keypoint is considered “correct” if its distance is below this threshold, allowing PCK to quantify the reliability of pose predictions at the chosen precision level.</span>
<span id="cb20-563"><a href="#cb20-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-564"><a href="#cb20-564" aria-hidden="true" tabindex="-1"></a>For each frame, PCK is calculated as:</span>
<span id="cb20-565"><a href="#cb20-565" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-566"><a href="#cb20-566" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-567"><a href="#cb20-567" aria-hidden="true" tabindex="-1"></a>PCK = \frac{\text{number of keypoints with distance &lt; threshold}}{\text{total number of valid keypoints}}</span>
<span id="cb20-568"><a href="#cb20-568" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-569"><a href="#cb20-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-570"><a href="#cb20-570" aria-hidden="true" tabindex="-1"></a>PCK values range from zero to one, where one indicates perfect predictions (all keypoints are within the threshold) and zero indicates complete failure (no keypoints within the threshold).</span>
<span id="cb20-571"><a href="#cb20-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-572"><a href="#cb20-572" aria-hidden="true" tabindex="-1"></a><span class="fu">### Root Mean Square Error (RMSE) {#sec-metrics-rmse}</span></span>
<span id="cb20-573"><a href="#cb20-573" aria-hidden="true" tabindex="-1"></a>The Root Mean Square Error (RMSE) provides a single aggregated measure of pose estimation accuracy by calculating the root mean square of normalized Euclidean distances across all valid keypoints and persons in a frame.</span>
<span id="cb20-574"><a href="#cb20-574" aria-hidden="true" tabindex="-1"></a>RMSE is defined as:</span>
<span id="cb20-575"><a href="#cb20-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-576"><a href="#cb20-576" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-577"><a href="#cb20-577" aria-hidden="true" tabindex="-1"></a>RMSE = \sqrt{\frac{1}{N} \sum_{i=1}^{N} d_i^2}</span>
<span id="cb20-578"><a href="#cb20-578" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-579"><a href="#cb20-579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-580"><a href="#cb20-580" aria-hidden="true" tabindex="-1"></a>where $N$ is the total number of valid keypoints in the frame, and $d_i$ is the normalized Euclidean distance of keypoint $i$. </span>
<span id="cb20-581"><a href="#cb20-581" aria-hidden="true" tabindex="-1"></a>By squaring the distances before averaging, RMSE penalizes larger errors more heavily, making it particularly sensitive to outliers.</span>
<span id="cb20-582"><a href="#cb20-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-583"><a href="#cb20-583" aria-hidden="true" tabindex="-1"></a><span class="fu">## Kinematic Metrics</span></span>
<span id="cb20-584"><a href="#cb20-584" aria-hidden="true" tabindex="-1"></a>Velocity, acceleration, and jerk are key kinematic metrics that help identify unnatural or erratic movements in pose estimations by highlighting rapid changes in motion.</span>
<span id="cb20-585"><a href="#cb20-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-586"><a href="#cb20-586" aria-hidden="true" tabindex="-1"></a><span class="fu">### Velocity {#sec-metrics-velocity}</span></span>
<span id="cb20-587"><a href="#cb20-587" aria-hidden="true" tabindex="-1"></a>The velocity metric measures the rate of change in keypoint positions between consecutive frames.</span>
<span id="cb20-588"><a href="#cb20-588" aria-hidden="true" tabindex="-1"></a>For each keypoint of a person, it quantifies how quickly the keypoint moves in pixels per frame, providing insight into the smoothness and temporal consistency of the pose estimation.</span>
<span id="cb20-589"><a href="#cb20-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-590"><a href="#cb20-590" aria-hidden="true" tabindex="-1"></a>The velocity calculation proceeds in three steps:</span>
<span id="cb20-591"><a href="#cb20-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-592"><a href="#cb20-592" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Person indices are matched between consecutive frames (as described in @sec-architecture-evaluation-metric) to ensure tracking of the same individual over time.</span>
<span id="cb20-593"><a href="#cb20-593" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>The velocity is then computed with $v_t = p_{t+1} - p_t$ as the difference between keypoint positions in consecutive frames, where $p_t$ represents the keypoint position at frame $t$, and $v_t$ is the resulting velocity vector.</span>
<span id="cb20-594"><a href="#cb20-594" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Finally, the metric can be configured to report velocities in either pixels per frame or pixels per second. In the latter case, the frame-based velocity is divided by the time delta between frames (1/fps).</span>
<span id="cb20-595"><a href="#cb20-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-596"><a href="#cb20-596" aria-hidden="true" tabindex="-1"></a>The metric robustly handles several edge cases:</span>
<span id="cb20-597"><a href="#cb20-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-598"><a href="#cb20-598" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>For videos with fewer than two frames, velocity cannot be computed, and the metric returns <span class="in">`NaN`</span> values.</span>
<span id="cb20-599"><a href="#cb20-599" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If a keypoint is missing in either of two consecutive frames, the corresponding velocity is set to <span class="in">`NaN`</span>.</span>
<span id="cb20-600"><a href="#cb20-600" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Since velocity is derived from frame-to-frame differences, the output contains one fewer frame than the input video.</span>
<span id="cb20-601"><a href="#cb20-601" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The output includes a coordinate axis (x and y) representing the velocity vector, which serves as a basis for the computation of the acceleration and jerk metrics.</span>
<span id="cb20-602"><a href="#cb20-602" aria-hidden="true" tabindex="-1"></a>For evaluation and visualization, aggregate along this axis using the <span class="in">`vector_magnitude`</span> method to obtain scalar velocity values.</span>
<span id="cb20-603"><a href="#cb20-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-604"><a href="#cb20-604" aria-hidden="true" tabindex="-1"></a><span class="fu">### Acceleration {#sec-metrics-acceleration}</span></span>
<span id="cb20-605"><a href="#cb20-605" aria-hidden="true" tabindex="-1"></a>The acceleration metric measures the rate of change in velocity over time, representing how quickly the movement speed of keypoints changes. </span>
<span id="cb20-606"><a href="#cb20-606" aria-hidden="true" tabindex="-1"></a>It is computed by $a_t = v_{t+1} - v_t$, where $a_t$ is the acceleration at time $t$, $v_t$ represents the velocity, and $p_t$ the keypoint position. </span>
<span id="cb20-607"><a href="#cb20-607" aria-hidden="true" tabindex="-1"></a>Acceleration values can be reported in either pixels per frame squared or pixels per second squared, with the latter requiring normalization by the squared time delta between frames (1/fps²).</span>
<span id="cb20-608"><a href="#cb20-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-609"><a href="#cb20-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-610"><a href="#cb20-610" aria-hidden="true" tabindex="-1"></a><span class="fu">### Jerk {#sec-metrics-jerk}</span></span>
<span id="cb20-611"><a href="#cb20-611" aria-hidden="true" tabindex="-1"></a>Jerk measures the rate of change of acceleration, offering insights into the smoothness and abruptness of motion by quantifying how quickly acceleration varies.</span>
<span id="cb20-612"><a href="#cb20-612" aria-hidden="true" tabindex="-1"></a>It is calculated with $j_t = a_{t+1} - a_t$ as the difference between consecutive acceleration values, where $j_t$ is the jerk at time $t$ and $a_t$ represents the acceleration. </span>
<span id="cb20-613"><a href="#cb20-613" aria-hidden="true" tabindex="-1"></a>The metric supports reporting in pixels per frame cubed or pixels per second cubed, with the latter normalized by the cubed time delta between frames (1/fps³).</span>
<span id="cb20-614"><a href="#cb20-614" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-615"><a href="#cb20-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-616"><a href="#cb20-616" aria-hidden="true" tabindex="-1"></a><span class="fu"># Experimental Setup {#sec-experiments}</span></span>
<span id="cb20-617"><a href="#cb20-617" aria-hidden="true" tabindex="-1"></a>In this section, we describe the experimental setup used to evaluate pose estimators across four datasets.</span>
<span id="cb20-618"><a href="#cb20-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-619"><a href="#cb20-619" aria-hidden="true" tabindex="-1"></a>**General Setup**</span>
<span id="cb20-620"><a href="#cb20-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-621"><a href="#cb20-621" aria-hidden="true" tabindex="-1"></a>We evaluated seven pose estimators on the four datasets: TED Kid Video, TED Talks, Tragic Talkers, and the Masked Video Dataset.</span>
<span id="cb20-622"><a href="#cb20-622" aria-hidden="true" tabindex="-1"></a>The pose estimators are: YoloPose (v11-l), MediaPipePose (pose_landmarker_heavy), OpenPose (body_25), MaskAnyoneAPI-MediaPipe, MaskAnyoneAPI-OpenPose, MaskAnyoneUI-MediaPipe, and MaskAnyoneUI-OpenPose.</span>
<span id="cb20-623"><a href="#cb20-623" aria-hidden="true" tabindex="-1"></a>Confidence thresholds were visually determined on a subset of videos and set to 0.3 for YoloPose and MediaPipePose, and 0.15 for OpenPose. Since MaskAnyone-based estimators do not output confidence scores, a threshold of zero was used for them.</span>
<span id="cb20-624"><a href="#cb20-624" aria-hidden="true" tabindex="-1"></a>All keypoints were stored in COCO format to enable per-keypoint comparisons across models.</span>
<span id="cb20-625"><a href="#cb20-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-626"><a href="#cb20-626" aria-hidden="true" tabindex="-1"></a>**TED Kid Video and TED Talks**</span>
<span id="cb20-627"><a href="#cb20-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-628"><a href="#cb20-628" aria-hidden="true" tabindex="-1"></a>For both datasets, we evaluated the kinematic metrics velocity, acceleration, and jerk for each pose estimator.</span>
<span id="cb20-629"><a href="#cb20-629" aria-hidden="true" tabindex="-1"></a>Due to the absence of ground truth annotations for TED Talks, accuracy metrics could not be computed.</span>
<span id="cb20-630"><a href="#cb20-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-631"><a href="#cb20-631" aria-hidden="true" tabindex="-1"></a>**Tragic Talkers**</span>
<span id="cb20-632"><a href="#cb20-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-633"><a href="#cb20-633" aria-hidden="true" tabindex="-1"></a>For the Tragic Talkers dataset, we evaluated both accuracy metrics (Euclidean distance, PCK, RMSE) and kinematic metrics (velocity, acceleration, jerk) for each pose estimator.</span>
<span id="cb20-634"><a href="#cb20-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-635"><a href="#cb20-635" aria-hidden="true" tabindex="-1"></a>**Inference on Raw vs. Masked Videos**</span>
<span id="cb20-636"><a href="#cb20-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-637"><a href="#cb20-637" aria-hidden="true" tabindex="-1"></a>For the Masked Video dataset, inference was first performed on the raw videos with all pose estimators to establish a baseline.</span>
<span id="cb20-638"><a href="#cb20-638" aria-hidden="true" tabindex="-1"></a>Subsequently, inference was repeated on videos masked with different hiding strategies.</span>
<span id="cb20-639"><a href="#cb20-639" aria-hidden="true" tabindex="-1"></a>Performance was compared against the baseline to assess the impact of masking, using PCK and RMSE metrics to quantify accuracy relative to raw videos.</span>
<span id="cb20-640"><a href="#cb20-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-641"><a href="#cb20-641" aria-hidden="true" tabindex="-1"></a>Note that this masking evaluation pipeline is a preliminary implementation outside of MaskBench’s native capabilities, serving as a proof of concept.</span>
<span id="cb20-642"><a href="#cb20-642" aria-hidden="true" tabindex="-1"></a>We intend to integrate full support for this workflow in future MaskBench releases (see section @sec-future-work-pipelining).</span>
<span id="cb20-643"><a href="#cb20-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-644"><a href="#cb20-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-645"><a href="#cb20-645" aria-hidden="true" tabindex="-1"></a><span class="fu"># Results {#sec-results}</span></span>
<span id="cb20-646"><a href="#cb20-646" aria-hidden="true" tabindex="-1"></a>We present the experimental results below.</span>
<span id="cb20-647"><a href="#cb20-647" aria-hidden="true" tabindex="-1"></a>To improve readability, kinematic metrics are reported without units in the text: velocity is in pixels/frame, acceleration in pixels/frame², and jerk in pixels/frame³.</span>
<span id="cb20-648"><a href="#cb20-648" aria-hidden="true" tabindex="-1"></a>Our analysis focuses primarily on acceleration and jerk, instead of velocity, as these metrics are more suited to detecting instability and unnatural motion in pose estimation.</span>
<span id="cb20-649"><a href="#cb20-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-650"><a href="#cb20-650" aria-hidden="true" tabindex="-1"></a><span class="fu">## TED Kid Video {#sec-results-ted-kid}</span></span>
<span id="cb20-651"><a href="#cb20-651" aria-hidden="true" tabindex="-1"></a>@tbl-results-ted-kid summarizes the average velocity, acceleration, and jerk for each pose estimator on the TED aid video.</span>
<span id="cb20-652"><a href="#cb20-652" aria-hidden="true" tabindex="-1"></a>Standard pose estimation models like YoloPose, MediaPipePose, and OpenPose exhibit relatively high values across all metrics, indicating more erratic and less stable pose estimations.</span>
<span id="cb20-653"><a href="#cb20-653" aria-hidden="true" tabindex="-1"></a>MediaPipePose has the highest values for velocity (3.36), acceleration (4.10), and jerk (5.56).</span>
<span id="cb20-654"><a href="#cb20-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-655"><a href="#cb20-655" aria-hidden="true" tabindex="-1"></a>In contrast, all evaluated MaskAnyone pose estimators show consistently lower acceleration and jerk, with MaskAnyoneUI-MediaPipe achieving the best results (velocity: 1.97, acceleration: 1.20, jerk: 1.89), representing reductions of 40%, 70%, and 66% respectively, compared to pure MediaPipePose.</span>
<span id="cb20-656"><a href="#cb20-656" aria-hidden="true" tabindex="-1"></a>This indicates substantially smoother and more stable pose tracking over time.</span>
<span id="cb20-657"><a href="#cb20-657" aria-hidden="true" tabindex="-1"></a>The improvements are more pronounced for MediaPipePose than OpenPose: MaskAnyone reduces MediaPipePose’s acceleration and jerk by 2.9 and 5.31, while OpenPose sees smaller decreases of 1.11 and 2.81, demonstrating the greater effectiveness of MaskAnyone with MediaPipePose.</span>
<span id="cb20-658"><a href="#cb20-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-659"><a href="#cb20-659" aria-hidden="true" tabindex="-1"></a>::: {#tbl-results-ted-kid}</span>
<span id="cb20-660"><a href="#cb20-660" aria-hidden="true" tabindex="-1"></a><span class="in">```{=html}</span></span>
<span id="cb20-661"><a href="#cb20-661" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;table class="results"&gt;&lt;thead&gt;</span></span>
<span id="cb20-662"><a href="#cb20-662" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;tr&gt;</span></span>
<span id="cb20-663"><a href="#cb20-663" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;th&gt;Pose Estimator&lt;/th&gt;</span></span>
<span id="cb20-664"><a href="#cb20-664" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;th&gt;Velocity&lt;/th&gt;</span></span>
<span id="cb20-665"><a href="#cb20-665" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;th&gt;Acceleration&lt;/th&gt;</span></span>
<span id="cb20-666"><a href="#cb20-666" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;th&gt;Jerk&lt;/th&gt;</span></span>
<span id="cb20-667"><a href="#cb20-667" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;/tr&gt;&lt;/thead&gt;</span></span>
<span id="cb20-668"><a href="#cb20-668" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;tbody&gt;</span></span>
<span id="cb20-669"><a href="#cb20-669" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;tr&gt;</span></span>
<span id="cb20-670"><a href="#cb20-670" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;YoloPose&lt;/td&gt;</span></span>
<span id="cb20-671"><a href="#cb20-671" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;2.81&lt;/td&gt;</span></span>
<span id="cb20-672"><a href="#cb20-672" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;2.95&lt;/td&gt;</span></span>
<span id="cb20-673"><a href="#cb20-673" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;5.04&lt;/td&gt;</span></span>
<span id="cb20-674"><a href="#cb20-674" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;/tr&gt;</span></span>
<span id="cb20-675"><a href="#cb20-675" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;tr&gt;</span></span>
<span id="cb20-676"><a href="#cb20-676" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;MediaPipePose&lt;/td&gt;</span></span>
<span id="cb20-677"><a href="#cb20-677" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;3.36&lt;/td&gt;</span></span>
<span id="cb20-678"><a href="#cb20-678" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;4.10&lt;/td&gt;</span></span>
<span id="cb20-679"><a href="#cb20-679" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;7.18&lt;/td&gt;</span></span>
<span id="cb20-680"><a href="#cb20-680" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;/tr&gt;</span></span>
<span id="cb20-681"><a href="#cb20-681" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;tr&gt;</span></span>
<span id="cb20-682"><a href="#cb20-682" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;OpenPose&lt;/td&gt;</span></span>
<span id="cb20-683"><a href="#cb20-683" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;2.71&lt;/td&gt;</span></span>
<span id="cb20-684"><a href="#cb20-684" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;3.20&lt;/td&gt;</span></span>
<span id="cb20-685"><a href="#cb20-685" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;5.56&lt;/td&gt;</span></span>
<span id="cb20-686"><a href="#cb20-686" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;/tr&gt;</span></span>
<span id="cb20-687"><a href="#cb20-687" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;tr class="maskanyone-api border-top"&gt;</span></span>
<span id="cb20-688"><a href="#cb20-688" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;MaskAnyoneAPI-MediaPipe&lt;/td&gt;</span></span>
<span id="cb20-689"><a href="#cb20-689" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td class="second"&gt;2.00&lt;/td&gt;</span></span>
<span id="cb20-690"><a href="#cb20-690" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td class="second"&gt;1.21&lt;/td&gt;</span></span>
<span id="cb20-691"><a href="#cb20-691" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td class="best"&gt;1.87&lt;/td&gt;</span></span>
<span id="cb20-692"><a href="#cb20-692" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;/tr&gt;</span></span>
<span id="cb20-693"><a href="#cb20-693" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;tr class="maskanyone-api border-bottom"&gt;</span></span>
<span id="cb20-694"><a href="#cb20-694" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;MaskAnyoneAPI-OpenPose&lt;/td&gt;</span></span>
<span id="cb20-695"><a href="#cb20-695" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;2.73&lt;/td&gt;</span></span>
<span id="cb20-696"><a href="#cb20-696" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;2.46&lt;/td&gt;</span></span>
<span id="cb20-697"><a href="#cb20-697" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;3.53&lt;/td&gt;</span></span>
<span id="cb20-698"><a href="#cb20-698" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;/tr&gt;</span></span>
<span id="cb20-699"><a href="#cb20-699" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;tr class="maskanyone-ui border-top"&gt;</span></span>
<span id="cb20-700"><a href="#cb20-700" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;MaskAnyoneUI-MediaPipe&lt;/td&gt;</span></span>
<span id="cb20-701"><a href="#cb20-701" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td class="best"&gt;1.97&lt;/td&gt;</span></span>
<span id="cb20-702"><a href="#cb20-702" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td class="best"&gt;1.20&lt;/td&gt;</span></span>
<span id="cb20-703"><a href="#cb20-703" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td class="second"&gt;1.89&lt;/td&gt;</span></span>
<span id="cb20-704"><a href="#cb20-704" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;/tr&gt;</span></span>
<span id="cb20-705"><a href="#cb20-705" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;tr class="maskanyone-ui border-bottom"&gt;</span></span>
<span id="cb20-706"><a href="#cb20-706" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;MaskAnyoneUI-OpenPose&lt;/td&gt;</span></span>
<span id="cb20-707"><a href="#cb20-707" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;2.62&lt;/td&gt;</span></span>
<span id="cb20-708"><a href="#cb20-708" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;2.09&lt;/td&gt;</span></span>
<span id="cb20-709"><a href="#cb20-709" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;2.75&lt;/td&gt;</span></span>
<span id="cb20-710"><a href="#cb20-710" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;/tr&gt;</span></span>
<span id="cb20-711"><a href="#cb20-711" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/tbody&gt;</span></span>
<span id="cb20-712"><a href="#cb20-712" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/table&gt;</span></span>
<span id="cb20-713"><a href="#cb20-713" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-714"><a href="#cb20-714" aria-hidden="true" tabindex="-1"></a>Average metric results for different pose estimators on the TED kid video.</span>
<span id="cb20-715"><a href="#cb20-715" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-716"><a href="#cb20-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-717"><a href="#cb20-717" aria-hidden="true" tabindex="-1"></a>@fig-ted-kid-acceleration-distribution and @fig-ted-kid-jerk-distribution present the distribution of the acceleration and jerk metrics for the different pose estimators.</span>
<span id="cb20-718"><a href="#cb20-718" aria-hidden="true" tabindex="-1"></a>These plots show the percentage of keypoints within fixed value ranges for the acceleration and jerk metrics over all frames.</span>
<span id="cb20-719"><a href="#cb20-719" aria-hidden="true" tabindex="-1"></a>The ideal curve for a stable pose estimation follows an exponential decay curve, with most kinematic values near zero and a few large values.</span>
<span id="cb20-720"><a href="#cb20-720" aria-hidden="true" tabindex="-1"></a>Both plots confirm the results from @tbl-results-ted-kid, showing that the MaskAnyone pose estimators have a very high concentration of low acceleration and jerk values.</span>
<span id="cb20-721"><a href="#cb20-721" aria-hidden="true" tabindex="-1"></a>The UI and API variants of MaskAnyone-MediaPipe most closely resemble the ideal curve, with over 80% of keypoints having acceleration below 1 pixel/frame².</span>
<span id="cb20-722"><a href="#cb20-722" aria-hidden="true" tabindex="-1"></a>MaskAnyone-OpenPose estimators rank third and fourth, with around 57% of keypoints below this threshold.</span>
<span id="cb20-723"><a href="#cb20-723" aria-hidden="true" tabindex="-1"></a>YoloPose ranks fifth with 52%, followed by OpenPose at 40%.</span>
<span id="cb20-724"><a href="#cb20-724" aria-hidden="true" tabindex="-1"></a>MediaPipePose is the most unstable, with only 30% of keypoints below one pixel/frame² and a relatively flat distribution curve.</span>
<span id="cb20-725"><a href="#cb20-725" aria-hidden="true" tabindex="-1"></a>Similar patterns can be observed for the jerk distribution.</span>
<span id="cb20-726"><a href="#cb20-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-727"><a href="#cb20-727" aria-hidden="true" tabindex="-1"></a>@fig-ted-kid-acceleration-per-keypoint shows the median acceleration per keypoint for the different pose estimators.</span>
<span id="cb20-728"><a href="#cb20-728" aria-hidden="true" tabindex="-1"></a>Each keypoint contains a set of seven bars, one for each pose estimator, indicating the median acceleration value for that keypoint and pose estimator.</span>
<span id="cb20-729"><a href="#cb20-729" aria-hidden="true" tabindex="-1"></a>The first notable finding is that keypoints like the wrists, elbows, hips, and ankles exhibit consistently higher median acceleration compared to more stable points like the eyes, ears, and nose.</span>
<span id="cb20-730"><a href="#cb20-730" aria-hidden="true" tabindex="-1"></a>This aligns with expectations since these joints undergo more frequent and pronounced movement.</span>
<span id="cb20-731"><a href="#cb20-731" aria-hidden="true" tabindex="-1"></a>Secondly, MaskAnyoneAPI-MediaPipe and MaskAnyoneUI-MediaPipe consistently achieve the lowest acceleration values across all keypoints.</span>
<span id="cb20-732"><a href="#cb20-732" aria-hidden="true" tabindex="-1"></a>Both MaskAnyoneUI variants improve upon their default counterparts, MediaPipePose and OpenPose, for every keypoint.</span>
<span id="cb20-733"><a href="#cb20-733" aria-hidden="true" tabindex="-1"></a>The most pronounced gains appear at the hips, knees, and ankles, where MaskAnyoneUI-MediaPipe reduces median acceleration from about six pixels/frame² down to less than one.</span>
<span id="cb20-734"><a href="#cb20-734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-735"><a href="#cb20-735" aria-hidden="true" tabindex="-1"></a>::: {#fig-ted-kid-plots layout="[<span class="co">[</span><span class="ot">1,1</span><span class="co">]</span>, <span class="co">[</span><span class="ot">1</span><span class="co">]</span>]"}</span>
<span id="cb20-736"><a href="#cb20-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-737"><a href="#cb20-737" aria-hidden="true" tabindex="-1"></a><span class="al">![Acceleration Distribution](plots/TED-kid/acceleration_distribution.png)</span>{#fig-ted-kid-acceleration-distribution}</span>
<span id="cb20-738"><a href="#cb20-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-739"><a href="#cb20-739" aria-hidden="true" tabindex="-1"></a><span class="al">![Jerk Distribution](plots/TED-kid/jerk_distribution.png)</span>{#fig-ted-kid-jerk-distribution}</span>
<span id="cb20-740"><a href="#cb20-740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-741"><a href="#cb20-741" aria-hidden="true" tabindex="-1"></a><span class="al">![Median Acceleration per Keypoint](plots/TED-kid/keypoint_plot_acceleration.png)</span>{#fig-ted-kid-acceleration-per-keypoint}</span>
<span id="cb20-742"><a href="#cb20-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-743"><a href="#cb20-743" aria-hidden="true" tabindex="-1"></a>**Comparison of pose estimation models on the TED-kid video.**</span>
<span id="cb20-744"><a href="#cb20-744" aria-hidden="true" tabindex="-1"></a>(a, b) Acceleration and jerk distribution showing the proportion of keypoints within a fixed value range. </span>
<span id="cb20-745"><a href="#cb20-745" aria-hidden="true" tabindex="-1"></a>A larger concentration at lower values indicates a smoother and more stable pose, whereas a high number of keypoints with large values indicates more jittery movements.</span>
<span id="cb20-746"><a href="#cb20-746" aria-hidden="true" tabindex="-1"></a>(c) Median acceleration per keypoint, indicating stability across individual body parts. Keypoints like the wrist, elbow, and ankle are expected to have a higher median acceleration than other body parts, which tend to be more stable during movements, like the eyes, ears, and nose.</span>
<span id="cb20-747"><a href="#cb20-747" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-748"><a href="#cb20-748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-749"><a href="#cb20-749" aria-hidden="true" tabindex="-1"></a>Last but not least, it is important to not only evaluate pose estimation results analytically but also to visually inspect pose quality.</span>
<span id="cb20-750"><a href="#cb20-750" aria-hidden="true" tabindex="-1"></a>@tbl-results-ted-kid-videos shows rendered videos for the seven pose estimators on the TED kid video.</span>
<span id="cb20-751"><a href="#cb20-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-752"><a href="#cb20-752" aria-hidden="true" tabindex="-1"></a>In the MediaPipePose video, the pose estimation appears unstable, showing more jitter and sudden pose changes.</span>
<span id="cb20-753"><a href="#cb20-753" aria-hidden="true" tabindex="-1"></a>At the beginning, the model fails to detect the right elbow joint, which all other estimators detect correctly.</span>
<span id="cb20-754"><a href="#cb20-754" aria-hidden="true" tabindex="-1"></a>Additionally, the hips, ankles, and elbows display rapid, jerky movements throughout the video.</span>
<span id="cb20-755"><a href="#cb20-755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-756"><a href="#cb20-756" aria-hidden="true" tabindex="-1"></a>Comparing MaskAnyoneUI-MediaPipe and MaskAnyoneAPI-MediaPipe rendered videos reveals that both are considerably more stable and smoother than pure MediaPipePose.</span>
<span id="cb20-757"><a href="#cb20-757" aria-hidden="true" tabindex="-1"></a>Aside from the person’s natural movement, key points generally remain fixed and steady.</span>
<span id="cb20-758"><a href="#cb20-758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-759"><a href="#cb20-759" aria-hidden="true" tabindex="-1"></a>Observing the other pose estimators shows that none are as stable as MaskAnyoneUI-MediaPipe, but most outperform pure MediaPipePose in stability.</span>
<span id="cb20-760"><a href="#cb20-760" aria-hidden="true" tabindex="-1"></a>This visual evidence supports the quantitative results in @tbl-results-ted-kid and @fig-ted-kid-plots.</span>
<span id="cb20-761"><a href="#cb20-761" aria-hidden="true" tabindex="-1"></a>It also confirms that our kinematic metrics effectively indicate pose estimation stability.</span>
<span id="cb20-762"><a href="#cb20-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-763"><a href="#cb20-763" aria-hidden="true" tabindex="-1"></a>::: {#tbl-results-ted-kid-videos}</span>
<span id="cb20-764"><a href="#cb20-764" aria-hidden="true" tabindex="-1"></a><span class="in">```{=html}</span></span>
<span id="cb20-765"><a href="#cb20-765" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;table class="video-table"&gt;</span></span>
<span id="cb20-766"><a href="#cb20-766" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;tr&gt;</span></span>
<span id="cb20-767"><a href="#cb20-767" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;td&gt;</span></span>
<span id="cb20-768"><a href="#cb20-768" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-zoom-wrapper ted-kid-video"&gt;</span></span>
<span id="cb20-769"><a href="#cb20-769" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;video class="video-zoom ted-kid-sync" autoplay muted playsinline&gt;</span></span>
<span id="cb20-770"><a href="#cb20-770" aria-hidden="true" tabindex="-1"></a><span class="in">                    &lt;source src="videos/TED-kid/TED-kid.mp4" type="video/mp4"&gt;</span></span>
<span id="cb20-771"><a href="#cb20-771" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;/video&gt;</span></span>
<span id="cb20-772"><a href="#cb20-772" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;/div&gt;</span></span>
<span id="cb20-773"><a href="#cb20-773" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-caption"&gt;Raw Video&lt;/div&gt;</span></span>
<span id="cb20-774"><a href="#cb20-774" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;/td&gt;</span></span>
<span id="cb20-775"><a href="#cb20-775" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;td&gt;</span></span>
<span id="cb20-776"><a href="#cb20-776" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-zoom-wrapper ted-kid-video"&gt;</span></span>
<span id="cb20-777"><a href="#cb20-777" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;video class="video-zoom ted-kid-sync" autoplay muted playsinline&gt;</span></span>
<span id="cb20-778"><a href="#cb20-778" aria-hidden="true" tabindex="-1"></a><span class="in">                    &lt;source src="videos/TED-kid/TED-kid_YoloPose.mp4" type="video/mp4"&gt;</span></span>
<span id="cb20-779"><a href="#cb20-779" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;/video&gt;</span></span>
<span id="cb20-780"><a href="#cb20-780" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;/div&gt;</span></span>
<span id="cb20-781"><a href="#cb20-781" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-caption"&gt;YOLOPose&lt;/div&gt;</span></span>
<span id="cb20-782"><a href="#cb20-782" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;/td&gt;</span></span>
<span id="cb20-783"><a href="#cb20-783" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/tr&gt;</span></span>
<span id="cb20-784"><a href="#cb20-784" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;tr&gt;</span></span>
<span id="cb20-785"><a href="#cb20-785" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;td&gt;</span></span>
<span id="cb20-786"><a href="#cb20-786" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-zoom-wrapper ted-kid-video"&gt;</span></span>
<span id="cb20-787"><a href="#cb20-787" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;video class="video-zoom ted-kid-sync" autoplay muted playsinline&gt;</span></span>
<span id="cb20-788"><a href="#cb20-788" aria-hidden="true" tabindex="-1"></a><span class="in">                    &lt;source src="videos/TED-kid/TED-kid_MediaPipePose.mp4" type="video/mp4"&gt;</span></span>
<span id="cb20-789"><a href="#cb20-789" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;/video&gt;</span></span>
<span id="cb20-790"><a href="#cb20-790" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;/div&gt;</span></span>
<span id="cb20-791"><a href="#cb20-791" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-caption"&gt;MediaPipe Pose&lt;/div&gt;</span></span>
<span id="cb20-792"><a href="#cb20-792" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;/td&gt;</span></span>
<span id="cb20-793"><a href="#cb20-793" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;td&gt;</span></span>
<span id="cb20-794"><a href="#cb20-794" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-zoom-wrapper ted-kid-video"&gt;</span></span>
<span id="cb20-795"><a href="#cb20-795" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;video class="video-zoom ted-kid-sync" autoplay muted playsinline&gt;</span></span>
<span id="cb20-796"><a href="#cb20-796" aria-hidden="true" tabindex="-1"></a><span class="in">                    &lt;source src="videos/TED-kid/TED-kid_OpenPose.mp4" type="video/mp4"&gt;</span></span>
<span id="cb20-797"><a href="#cb20-797" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;/video&gt;</span></span>
<span id="cb20-798"><a href="#cb20-798" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;/div&gt;</span></span>
<span id="cb20-799"><a href="#cb20-799" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-caption"&gt;OpenPose&lt;/div&gt;</span></span>
<span id="cb20-800"><a href="#cb20-800" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;/td&gt;</span></span>
<span id="cb20-801"><a href="#cb20-801" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/tr&gt;</span></span>
<span id="cb20-802"><a href="#cb20-802" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;tr&gt;</span></span>
<span id="cb20-803"><a href="#cb20-803" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;td&gt;</span></span>
<span id="cb20-804"><a href="#cb20-804" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-zoom-wrapper ted-kid-video"&gt;</span></span>
<span id="cb20-805"><a href="#cb20-805" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;video class="video-zoom ted-kid-sync" autoplay muted playsinline&gt;</span></span>
<span id="cb20-806"><a href="#cb20-806" aria-hidden="true" tabindex="-1"></a><span class="in">                    &lt;source src="videos/TED-kid/TED-kid_MaskAnyoneAPI-MediaPipe.mp4" type="video/mp4"&gt;</span></span>
<span id="cb20-807"><a href="#cb20-807" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;/video&gt;</span></span>
<span id="cb20-808"><a href="#cb20-808" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;/div&gt;</span></span>
<span id="cb20-809"><a href="#cb20-809" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-caption"&gt;MaskAnyoneAPI-MediaPipe&lt;/div&gt;</span></span>
<span id="cb20-810"><a href="#cb20-810" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;/td&gt;</span></span>
<span id="cb20-811"><a href="#cb20-811" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;td&gt;</span></span>
<span id="cb20-812"><a href="#cb20-812" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-zoom-wrapper ted-kid-video"&gt;</span></span>
<span id="cb20-813"><a href="#cb20-813" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;video class="video-zoom ted-kid-sync" autoplay muted playsinline&gt;</span></span>
<span id="cb20-814"><a href="#cb20-814" aria-hidden="true" tabindex="-1"></a><span class="in">                    &lt;source src="videos/TED-kid/TED-kid_MaskAnyoneAPI-OpenPose.mp4" type="video/mp4"&gt;</span></span>
<span id="cb20-815"><a href="#cb20-815" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;/video&gt;</span></span>
<span id="cb20-816"><a href="#cb20-816" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;/div&gt;</span></span>
<span id="cb20-817"><a href="#cb20-817" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-caption"&gt;MaskAnyoneAPI-OpenPose&lt;/div&gt;</span></span>
<span id="cb20-818"><a href="#cb20-818" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;/td&gt;</span></span>
<span id="cb20-819"><a href="#cb20-819" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/tr&gt;</span></span>
<span id="cb20-820"><a href="#cb20-820" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;tr&gt;</span></span>
<span id="cb20-821"><a href="#cb20-821" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;td&gt;</span></span>
<span id="cb20-822"><a href="#cb20-822" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-zoom-wrapper ted-kid-video"&gt;</span></span>
<span id="cb20-823"><a href="#cb20-823" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;video class="video-zoom ted-kid-sync" autoplay muted playsinline&gt;</span></span>
<span id="cb20-824"><a href="#cb20-824" aria-hidden="true" tabindex="-1"></a><span class="in">                    &lt;source src="videos/TED-kid/TED-kid_MaskAnyoneUI-MediaPipe.mp4" type="video/mp4"&gt;</span></span>
<span id="cb20-825"><a href="#cb20-825" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;/video&gt;</span></span>
<span id="cb20-826"><a href="#cb20-826" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;/div&gt;</span></span>
<span id="cb20-827"><a href="#cb20-827" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-caption"&gt;MaskAnyoneUI-MediaPipe&lt;/div&gt;</span></span>
<span id="cb20-828"><a href="#cb20-828" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;/td&gt;</span></span>
<span id="cb20-829"><a href="#cb20-829" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;td&gt;</span></span>
<span id="cb20-830"><a href="#cb20-830" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-zoom-wrapper ted-kid-video"&gt;</span></span>
<span id="cb20-831"><a href="#cb20-831" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;video class="video-zoom ted-kid-sync" autoplay muted playsinline&gt;</span></span>
<span id="cb20-832"><a href="#cb20-832" aria-hidden="true" tabindex="-1"></a><span class="in">                    &lt;source src="videos/TED-kid/TED-kid_MaskAnyoneUI-OpenPose.mp4" type="video/mp4"&gt;</span></span>
<span id="cb20-833"><a href="#cb20-833" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;/video&gt;</span></span>
<span id="cb20-834"><a href="#cb20-834" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;/div&gt;</span></span>
<span id="cb20-835"><a href="#cb20-835" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-caption"&gt;MaskAnyoneUI-OpenPose&lt;/div&gt;</span></span>
<span id="cb20-836"><a href="#cb20-836" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;/td&gt;</span></span>
<span id="cb20-837"><a href="#cb20-837" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/tr&gt;</span></span>
<span id="cb20-838"><a href="#cb20-838" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/table&gt;</span></span>
<span id="cb20-839"><a href="#cb20-839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-840"><a href="#cb20-840" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;script&gt;</span></span>
<span id="cb20-841"><a href="#cb20-841" aria-hidden="true" tabindex="-1"></a><span class="in">document.addEventListener('DOMContentLoaded', function() {</span></span>
<span id="cb20-842"><a href="#cb20-842" aria-hidden="true" tabindex="-1"></a><span class="in">    const videos = document.querySelectorAll('.ted-kid-sync');</span></span>
<span id="cb20-843"><a href="#cb20-843" aria-hidden="true" tabindex="-1"></a><span class="in">    let endedCount = 0;</span></span>
<span id="cb20-844"><a href="#cb20-844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-845"><a href="#cb20-845" aria-hidden="true" tabindex="-1"></a><span class="in">    function restartAllVideos() {</span></span>
<span id="cb20-846"><a href="#cb20-846" aria-hidden="true" tabindex="-1"></a><span class="in">        videos.forEach(video =&gt; {</span></span>
<span id="cb20-847"><a href="#cb20-847" aria-hidden="true" tabindex="-1"></a><span class="in">            video.currentTime = 0;</span></span>
<span id="cb20-848"><a href="#cb20-848" aria-hidden="true" tabindex="-1"></a><span class="in">            video.play();</span></span>
<span id="cb20-849"><a href="#cb20-849" aria-hidden="true" tabindex="-1"></a><span class="in">        });</span></span>
<span id="cb20-850"><a href="#cb20-850" aria-hidden="true" tabindex="-1"></a><span class="in">        endedCount = 0;</span></span>
<span id="cb20-851"><a href="#cb20-851" aria-hidden="true" tabindex="-1"></a><span class="in">    }</span></span>
<span id="cb20-852"><a href="#cb20-852" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-853"><a href="#cb20-853" aria-hidden="true" tabindex="-1"></a><span class="in">    videos.forEach(video =&gt; {</span></span>
<span id="cb20-854"><a href="#cb20-854" aria-hidden="true" tabindex="-1"></a><span class="in">        video.addEventListener('ended', () =&gt; {</span></span>
<span id="cb20-855"><a href="#cb20-855" aria-hidden="true" tabindex="-1"></a><span class="in">            endedCount++;</span></span>
<span id="cb20-856"><a href="#cb20-856" aria-hidden="true" tabindex="-1"></a><span class="in">            if (endedCount === videos.length) {</span></span>
<span id="cb20-857"><a href="#cb20-857" aria-hidden="true" tabindex="-1"></a><span class="in">                restartAllVideos();</span></span>
<span id="cb20-858"><a href="#cb20-858" aria-hidden="true" tabindex="-1"></a><span class="in">            }</span></span>
<span id="cb20-859"><a href="#cb20-859" aria-hidden="true" tabindex="-1"></a><span class="in">        });</span></span>
<span id="cb20-860"><a href="#cb20-860" aria-hidden="true" tabindex="-1"></a><span class="in">    });</span></span>
<span id="cb20-861"><a href="#cb20-861" aria-hidden="true" tabindex="-1"></a><span class="in">});</span></span>
<span id="cb20-862"><a href="#cb20-862" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/script&gt;</span></span>
<span id="cb20-863"><a href="#cb20-863" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-864"><a href="#cb20-864" aria-hidden="true" tabindex="-1"></a>Rendered result videos of different pose estimators on the TED-kid video.</span>
<span id="cb20-865"><a href="#cb20-865" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-866"><a href="#cb20-866" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-867"><a href="#cb20-867" aria-hidden="true" tabindex="-1"></a><span class="fu">## TED Talks {#sec-results-ted-talks}</span></span>
<span id="cb20-868"><a href="#cb20-868" aria-hidden="true" tabindex="-1"></a>The results on ten full TED talks closely mirror those on the single TED kid video, as shown in @tbl-results-ted-talks.</span>
<span id="cb20-869"><a href="#cb20-869" aria-hidden="true" tabindex="-1"></a>Among the evaluated pose estimators, MaskAnyoneUI-MediaPipe consistently achieved the best stability, with the lowest average velocity, acceleration, and jerk values of 1.25, 1.08, and 1.83, respectively.</span>
<span id="cb20-870"><a href="#cb20-870" aria-hidden="true" tabindex="-1"></a>MaskAnyoneAPI-MediaPipe followed, showing the second-best performance in acceleration and jerk, closely trailed by YoloPose.</span>
<span id="cb20-871"><a href="#cb20-871" aria-hidden="true" tabindex="-1"></a>OpenPose ranked next, while both MaskAnyone-OpenPose variants exhibited greater instability than the pure OpenPose model.</span>
<span id="cb20-872"><a href="#cb20-872" aria-hidden="true" tabindex="-1"></a>Consistent with earlier findings, MediaPipePose was the least stable estimator, with the highest values across all metrics: 3.29 for velocity, 4.52 for acceleration, and 7.94 for jerk.</span>
<span id="cb20-873"><a href="#cb20-873" aria-hidden="true" tabindex="-1"></a>An additional observation is the clear trend that MediaPipe-based MaskAnyone variants generally outperform OpenPose-based ones in stability, as reflected by their consistently lower velocity, acceleration, and jerk values.</span>
<span id="cb20-874"><a href="#cb20-874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-875"><a href="#cb20-875" aria-hidden="true" tabindex="-1"></a>::: {#tbl-results-ted-talks}</span>
<span id="cb20-876"><a href="#cb20-876" aria-hidden="true" tabindex="-1"></a><span class="in">```{=html}</span></span>
<span id="cb20-877"><a href="#cb20-877" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;table class="results"&gt;&lt;thead&gt;</span></span>
<span id="cb20-878"><a href="#cb20-878" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;tr&gt;</span></span>
<span id="cb20-879"><a href="#cb20-879" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;th&gt;Pose Estimator&lt;/th&gt;</span></span>
<span id="cb20-880"><a href="#cb20-880" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;th&gt;Velocity&lt;/th&gt;</span></span>
<span id="cb20-881"><a href="#cb20-881" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;th&gt;Acceleration&lt;/th&gt;</span></span>
<span id="cb20-882"><a href="#cb20-882" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;th&gt;Jerk&lt;/th&gt;</span></span>
<span id="cb20-883"><a href="#cb20-883" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;/tr&gt;&lt;/thead&gt;</span></span>
<span id="cb20-884"><a href="#cb20-884" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;tbody&gt;</span></span>
<span id="cb20-885"><a href="#cb20-885" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;tr&gt;</span></span>
<span id="cb20-886"><a href="#cb20-886" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;YoloPose&lt;/td&gt;</span></span>
<span id="cb20-887"><a href="#cb20-887" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td class="second"&gt;1.35&lt;/td&gt;</span></span>
<span id="cb20-888"><a href="#cb20-888" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;1.46&lt;/td&gt;</span></span>
<span id="cb20-889"><a href="#cb20-889" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;2.44&lt;/td&gt;</span></span>
<span id="cb20-890"><a href="#cb20-890" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;/tr&gt;</span></span>
<span id="cb20-891"><a href="#cb20-891" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;tr&gt;</span></span>
<span id="cb20-892"><a href="#cb20-892" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;MediaPipePose&lt;/td&gt;</span></span>
<span id="cb20-893"><a href="#cb20-893" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;3.29&lt;/td&gt;</span></span>
<span id="cb20-894"><a href="#cb20-894" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;4.52&lt;/td&gt;</span></span>
<span id="cb20-895"><a href="#cb20-895" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;7.94&lt;/td&gt;</span></span>
<span id="cb20-896"><a href="#cb20-896" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;/tr&gt;</span></span>
<span id="cb20-897"><a href="#cb20-897" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;tr&gt;</span></span>
<span id="cb20-898"><a href="#cb20-898" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;OpenPose&lt;/td&gt;</span></span>
<span id="cb20-899"><a href="#cb20-899" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;1.58&lt;/td&gt;</span></span>
<span id="cb20-900"><a href="#cb20-900" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;2.22&lt;/td&gt;</span></span>
<span id="cb20-901"><a href="#cb20-901" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;3.44&lt;/td&gt;</span></span>
<span id="cb20-902"><a href="#cb20-902" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;/tr&gt;</span></span>
<span id="cb20-903"><a href="#cb20-903" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;tr class="maskanyone-api border-top"&gt;</span></span>
<span id="cb20-904"><a href="#cb20-904" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;MaskAnyoneAPI-MediaPipe&lt;/td&gt;</span></span>
<span id="cb20-905"><a href="#cb20-905" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;1.44&lt;/td&gt;</span></span>
<span id="cb20-906"><a href="#cb20-906" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td class="second"&gt;1.25&lt;/td&gt;</span></span>
<span id="cb20-907"><a href="#cb20-907" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td class="second"&gt;2.04&lt;/td&gt;</span></span>
<span id="cb20-908"><a href="#cb20-908" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;/tr&gt;</span></span>
<span id="cb20-909"><a href="#cb20-909" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;tr class="maskanyone-api border-bottom"&gt;</span></span>
<span id="cb20-910"><a href="#cb20-910" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;MaskAnyoneAPI-OpenPose&lt;/td&gt;</span></span>
<span id="cb20-911"><a href="#cb20-911" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;2.30&lt;/td&gt;</span></span>
<span id="cb20-912"><a href="#cb20-912" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;2.42&lt;/td&gt;</span></span>
<span id="cb20-913"><a href="#cb20-913" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;4.07&lt;/td&gt;</span></span>
<span id="cb20-914"><a href="#cb20-914" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;/tr&gt;</span></span>
<span id="cb20-915"><a href="#cb20-915" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;tr class="maskanyone-ui border-top"&gt;</span></span>
<span id="cb20-916"><a href="#cb20-916" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;MaskAnyoneUI-MediaPipe&lt;/td&gt;</span></span>
<span id="cb20-917"><a href="#cb20-917" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td class="best"&gt;1.25&lt;/td&gt;</span></span>
<span id="cb20-918"><a href="#cb20-918" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td class="best"&gt;1.08&lt;/td&gt;</span></span>
<span id="cb20-919"><a href="#cb20-919" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td class="best"&gt;1.83&lt;/td&gt;</span></span>
<span id="cb20-920"><a href="#cb20-920" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;/tr&gt;</span></span>
<span id="cb20-921"><a href="#cb20-921" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;tr class="maskanyone-ui border-bottom"&gt;</span></span>
<span id="cb20-922"><a href="#cb20-922" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;MaskAnyoneUI-OpenPose&lt;/td&gt;</span></span>
<span id="cb20-923"><a href="#cb20-923" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;2.07&lt;/td&gt;</span></span>
<span id="cb20-924"><a href="#cb20-924" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;2.29&lt;/td&gt;</span></span>
<span id="cb20-925"><a href="#cb20-925" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;3.72&lt;/td&gt;</span></span>
<span id="cb20-926"><a href="#cb20-926" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;/tr&gt;</span></span>
<span id="cb20-927"><a href="#cb20-927" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/tbody&gt;</span></span>
<span id="cb20-928"><a href="#cb20-928" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/table&gt;</span></span>
<span id="cb20-929"><a href="#cb20-929" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-930"><a href="#cb20-930" aria-hidden="true" tabindex="-1"></a>Average metric results for different pose estimators aggregated over all TED talk videos.</span>
<span id="cb20-931"><a href="#cb20-931" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-932"><a href="#cb20-932" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-933"><a href="#cb20-933" aria-hidden="true" tabindex="-1"></a>::: {#fig-ted-talks-plots layout="[<span class="co">[</span><span class="ot">1,1</span><span class="co">]</span>, <span class="co">[</span><span class="ot">1</span><span class="co">]</span>]"}</span>
<span id="cb20-934"><a href="#cb20-934" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-935"><a href="#cb20-935" aria-hidden="true" tabindex="-1"></a><span class="al">![Acceleration Distribution](plots/TED-talks/acceleration_distribution.png)</span>{#fig-ted-talks-acceleration-distribution}</span>
<span id="cb20-936"><a href="#cb20-936" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-937"><a href="#cb20-937" aria-hidden="true" tabindex="-1"></a><span class="al">![Jerk Distribution](plots/TED-talks/jerk_distribution.png)</span>{#fig-ted-talks-jerk-distribution}</span>
<span id="cb20-938"><a href="#cb20-938" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-939"><a href="#cb20-939" aria-hidden="true" tabindex="-1"></a><span class="al">![Median Acceleration per Keypoint](plots/TED-talks/keypoint_plot_acceleration.png)</span>{#fig-ted-kid-acceleration-per-keypoint}</span>
<span id="cb20-940"><a href="#cb20-940" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-941"><a href="#cb20-941" aria-hidden="true" tabindex="-1"></a>**Comparison of pose estimation models on the TED talks dataset.**</span>
<span id="cb20-942"><a href="#cb20-942" aria-hidden="true" tabindex="-1"></a>(a, b) Acceleration and jerk distribution showing the proportion of keypoints within a fixed value range. </span>
<span id="cb20-943"><a href="#cb20-943" aria-hidden="true" tabindex="-1"></a>A larger concentration at lower values indicates a smoother and more stable pose, whereas a high number of keypoints with large values indicates more jittery movements.</span>
<span id="cb20-944"><a href="#cb20-944" aria-hidden="true" tabindex="-1"></a>(c) Median acceleration per keypoint, indicating stability across individual body parts.</span>
<span id="cb20-945"><a href="#cb20-945" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-946"><a href="#cb20-946" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-947"><a href="#cb20-947" aria-hidden="true" tabindex="-1"></a>@fig-ted-talks-acceleration-distribution and @fig-ted-talks-jerk-distribution indicate that pose estimators are less stable in TED talks than in the TED kid video, with high acceleration and jerk values occurring more frequently.</span>
<span id="cb20-948"><a href="#cb20-948" aria-hidden="true" tabindex="-1"></a>This is likely because TED talks include camera movements, scene changes, segments without visible people, and audience views, none of which appear in the TED kid video.</span>
<span id="cb20-949"><a href="#cb20-949" aria-hidden="true" tabindex="-1"></a>We included two particularly challenging video chunks in @tbl-results-ted-videos.</span>
<span id="cb20-950"><a href="#cb20-950" aria-hidden="true" tabindex="-1"></a>The first column shows results for the qualitatively worst-performing pose estimator, MediaPipePose, while the second column presents the best performer, MaskAnyoneUI-MediaPipe.</span>
<span id="cb20-951"><a href="#cb20-951" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-952"><a href="#cb20-952" aria-hidden="true" tabindex="-1"></a>In the first scene <span class="co">[</span><span class="ot">@ted-tarana</span><span class="co">]</span> from the TED talk “Me Too is a Movement, Not a Moment”, the woman wears a long dress, and the video contains multiple scene changes, audience views with the speaker in the background, and parts where the speaker is not visible.</span>
<span id="cb20-953"><a href="#cb20-953" aria-hidden="true" tabindex="-1"></a>MaskAnyone substantially improves the stability and visual accuracy of pose estimation in all these scenarios.</span>
<span id="cb20-954"><a href="#cb20-954" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-955"><a href="#cb20-955" aria-hidden="true" tabindex="-1"></a>In the second scene <span class="co">[</span><span class="ot">@ted-song</span><span class="co">]</span> from the TED talk “Universe / Statues / Liberation”, the main challenges are rapid camera view changes and close-up shots of the singing woman.</span>
<span id="cb20-956"><a href="#cb20-956" aria-hidden="true" tabindex="-1"></a>Both MaskAnyoneUI-MediaPipe and raw MediaPipe struggle with close-ups of the hips and arms.</span>
<span id="cb20-957"><a href="#cb20-957" aria-hidden="true" tabindex="-1"></a>The model attempts to fit a full human pose into the small visible area of an arm or hip, leading to incorrect pose estimation and unstable motion.</span>
<span id="cb20-958"><a href="#cb20-958" aria-hidden="true" tabindex="-1"></a>It appears that once the model detects one joint, it tries to estimate the entire pose, which can cause errors in these conditions.</span>
<span id="cb20-959"><a href="#cb20-959" aria-hidden="true" tabindex="-1"></a>This issue was primarily observed with MediaPipe models, including MaskAnyone-MediaPipe variants, and not with other pose estimators.</span>
<span id="cb20-960"><a href="#cb20-960" aria-hidden="true" tabindex="-1"></a>Despite this, MaskAnyoneUI-MediaPipe still provides more stable and accurate pose estimations than pure MediaPipePose for most frames in this video.</span>
<span id="cb20-961"><a href="#cb20-961" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-962"><a href="#cb20-962" aria-hidden="true" tabindex="-1"></a>::: {#tbl-results-ted-videos}</span>
<span id="cb20-963"><a href="#cb20-963" aria-hidden="true" tabindex="-1"></a><span class="in">```{=html}</span></span>
<span id="cb20-964"><a href="#cb20-964" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;table class="video-table"&gt;</span></span>
<span id="cb20-965"><a href="#cb20-965" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;tr&gt;</span></span>
<span id="cb20-966"><a href="#cb20-966" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;td&gt;</span></span>
<span id="cb20-967"><a href="#cb20-967" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-zoom-wrapper"&gt;</span></span>
<span id="cb20-968"><a href="#cb20-968" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;video class="video-zoom tarana-sync" autoplay muted playsinline&gt;</span></span>
<span id="cb20-969"><a href="#cb20-969" aria-hidden="true" tabindex="-1"></a><span class="in">                    &lt;source src="videos/ted-talks/tarana_chunk17_MediaPipePose.mp4" type="video/mp4"&gt;</span></span>
<span id="cb20-970"><a href="#cb20-970" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;/video&gt;</span></span>
<span id="cb20-971"><a href="#cb20-971" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;/div&gt;</span></span>
<span id="cb20-972"><a href="#cb20-972" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;/td&gt;</span></span>
<span id="cb20-973"><a href="#cb20-973" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;td&gt;</span></span>
<span id="cb20-974"><a href="#cb20-974" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-zoom-wrapper"&gt;</span></span>
<span id="cb20-975"><a href="#cb20-975" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;video class="video-zoom tarana-sync" autoplay muted playsinline&gt;</span></span>
<span id="cb20-976"><a href="#cb20-976" aria-hidden="true" tabindex="-1"></a><span class="in">                    &lt;source src="videos/ted-talks/tarana_chunk17_MaskAnyoneUI-MediaPipe.mp4" type="video/mp4"&gt;</span></span>
<span id="cb20-977"><a href="#cb20-977" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;/video&gt;</span></span>
<span id="cb20-978"><a href="#cb20-978" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;/div&gt;</span></span>
<span id="cb20-979"><a href="#cb20-979" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;/td&gt;</span></span>
<span id="cb20-980"><a href="#cb20-980" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/tr&gt;</span></span>
<span id="cb20-981"><a href="#cb20-981" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;tr&gt;</span></span>
<span id="cb20-982"><a href="#cb20-982" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;td&gt;</span></span>
<span id="cb20-983"><a href="#cb20-983" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-zoom-wrapper"&gt;</span></span>
<span id="cb20-984"><a href="#cb20-984" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;video class="video-zoom song-sync" autoplay muted playsinline&gt;</span></span>
<span id="cb20-985"><a href="#cb20-985" aria-hidden="true" tabindex="-1"></a><span class="in">                    &lt;source src="videos/ted-talks/song_chunk1_MediaPipePose.mp4" type="video/mp4"&gt;</span></span>
<span id="cb20-986"><a href="#cb20-986" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;/video&gt;</span></span>
<span id="cb20-987"><a href="#cb20-987" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;/div&gt;</span></span>
<span id="cb20-988"><a href="#cb20-988" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-caption"&gt;MediaPipePose&lt;/div&gt;</span></span>
<span id="cb20-989"><a href="#cb20-989" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;/td&gt;</span></span>
<span id="cb20-990"><a href="#cb20-990" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;td&gt;</span></span>
<span id="cb20-991"><a href="#cb20-991" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-zoom-wrapper"&gt;</span></span>
<span id="cb20-992"><a href="#cb20-992" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;video class="video-zoom song-sync" autoplay muted playsinline&gt;</span></span>
<span id="cb20-993"><a href="#cb20-993" aria-hidden="true" tabindex="-1"></a><span class="in">                    &lt;source src="videos/ted-talks/song_chunk1_MaskAnyoneUI-MediaPipe.mp4" type="video/mp4"&gt;</span></span>
<span id="cb20-994"><a href="#cb20-994" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;/video&gt;</span></span>
<span id="cb20-995"><a href="#cb20-995" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;/div&gt;</span></span>
<span id="cb20-996"><a href="#cb20-996" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-caption"&gt;MaskAnyoneUI-MediaPipe&lt;/div&gt;</span></span>
<span id="cb20-997"><a href="#cb20-997" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;/td&gt;</span></span>
<span id="cb20-998"><a href="#cb20-998" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/tr&gt;</span></span>
<span id="cb20-999"><a href="#cb20-999" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/table&gt;</span></span>
<span id="cb20-1000"><a href="#cb20-1000" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1001"><a href="#cb20-1001" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;script&gt;</span></span>
<span id="cb20-1002"><a href="#cb20-1002" aria-hidden="true" tabindex="-1"></a><span class="in">document.addEventListener('DOMContentLoaded', function() {</span></span>
<span id="cb20-1003"><a href="#cb20-1003" aria-hidden="true" tabindex="-1"></a><span class="in">    const videos = document.querySelectorAll('.tarana-sync');</span></span>
<span id="cb20-1004"><a href="#cb20-1004" aria-hidden="true" tabindex="-1"></a><span class="in">    let endedCount = 0;</span></span>
<span id="cb20-1005"><a href="#cb20-1005" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1006"><a href="#cb20-1006" aria-hidden="true" tabindex="-1"></a><span class="in">    function restartAllVideos() {</span></span>
<span id="cb20-1007"><a href="#cb20-1007" aria-hidden="true" tabindex="-1"></a><span class="in">        videos.forEach(video =&gt; {</span></span>
<span id="cb20-1008"><a href="#cb20-1008" aria-hidden="true" tabindex="-1"></a><span class="in">            video.currentTime = 0;</span></span>
<span id="cb20-1009"><a href="#cb20-1009" aria-hidden="true" tabindex="-1"></a><span class="in">            video.play();</span></span>
<span id="cb20-1010"><a href="#cb20-1010" aria-hidden="true" tabindex="-1"></a><span class="in">        });</span></span>
<span id="cb20-1011"><a href="#cb20-1011" aria-hidden="true" tabindex="-1"></a><span class="in">        endedCount = 0;</span></span>
<span id="cb20-1012"><a href="#cb20-1012" aria-hidden="true" tabindex="-1"></a><span class="in">    }</span></span>
<span id="cb20-1013"><a href="#cb20-1013" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1014"><a href="#cb20-1014" aria-hidden="true" tabindex="-1"></a><span class="in">    videos.forEach(video =&gt; {</span></span>
<span id="cb20-1015"><a href="#cb20-1015" aria-hidden="true" tabindex="-1"></a><span class="in">        video.addEventListener('ended', () =&gt; {</span></span>
<span id="cb20-1016"><a href="#cb20-1016" aria-hidden="true" tabindex="-1"></a><span class="in">            endedCount++;</span></span>
<span id="cb20-1017"><a href="#cb20-1017" aria-hidden="true" tabindex="-1"></a><span class="in">            if (endedCount === videos.length) {</span></span>
<span id="cb20-1018"><a href="#cb20-1018" aria-hidden="true" tabindex="-1"></a><span class="in">                restartAllVideos();</span></span>
<span id="cb20-1019"><a href="#cb20-1019" aria-hidden="true" tabindex="-1"></a><span class="in">            }</span></span>
<span id="cb20-1020"><a href="#cb20-1020" aria-hidden="true" tabindex="-1"></a><span class="in">        });</span></span>
<span id="cb20-1021"><a href="#cb20-1021" aria-hidden="true" tabindex="-1"></a><span class="in">    });</span></span>
<span id="cb20-1022"><a href="#cb20-1022" aria-hidden="true" tabindex="-1"></a><span class="in">});</span></span>
<span id="cb20-1023"><a href="#cb20-1023" aria-hidden="true" tabindex="-1"></a><span class="in">document.addEventListener('DOMContentLoaded', function() {</span></span>
<span id="cb20-1024"><a href="#cb20-1024" aria-hidden="true" tabindex="-1"></a><span class="in">    const videos = document.querySelectorAll('.song-sync');</span></span>
<span id="cb20-1025"><a href="#cb20-1025" aria-hidden="true" tabindex="-1"></a><span class="in">    let endedCount = 0;</span></span>
<span id="cb20-1026"><a href="#cb20-1026" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1027"><a href="#cb20-1027" aria-hidden="true" tabindex="-1"></a><span class="in">    function restartAllVideos() {</span></span>
<span id="cb20-1028"><a href="#cb20-1028" aria-hidden="true" tabindex="-1"></a><span class="in">        videos.forEach(video =&gt; {</span></span>
<span id="cb20-1029"><a href="#cb20-1029" aria-hidden="true" tabindex="-1"></a><span class="in">            video.currentTime = 0;</span></span>
<span id="cb20-1030"><a href="#cb20-1030" aria-hidden="true" tabindex="-1"></a><span class="in">            video.play();</span></span>
<span id="cb20-1031"><a href="#cb20-1031" aria-hidden="true" tabindex="-1"></a><span class="in">        });</span></span>
<span id="cb20-1032"><a href="#cb20-1032" aria-hidden="true" tabindex="-1"></a><span class="in">        endedCount = 0;</span></span>
<span id="cb20-1033"><a href="#cb20-1033" aria-hidden="true" tabindex="-1"></a><span class="in">    }</span></span>
<span id="cb20-1034"><a href="#cb20-1034" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1035"><a href="#cb20-1035" aria-hidden="true" tabindex="-1"></a><span class="in">    videos.forEach(video =&gt; {</span></span>
<span id="cb20-1036"><a href="#cb20-1036" aria-hidden="true" tabindex="-1"></a><span class="in">        video.addEventListener('ended', () =&gt; {</span></span>
<span id="cb20-1037"><a href="#cb20-1037" aria-hidden="true" tabindex="-1"></a><span class="in">            endedCount++;</span></span>
<span id="cb20-1038"><a href="#cb20-1038" aria-hidden="true" tabindex="-1"></a><span class="in">            if (endedCount === videos.length) {</span></span>
<span id="cb20-1039"><a href="#cb20-1039" aria-hidden="true" tabindex="-1"></a><span class="in">                restartAllVideos();</span></span>
<span id="cb20-1040"><a href="#cb20-1040" aria-hidden="true" tabindex="-1"></a><span class="in">            }</span></span>
<span id="cb20-1041"><a href="#cb20-1041" aria-hidden="true" tabindex="-1"></a><span class="in">        });</span></span>
<span id="cb20-1042"><a href="#cb20-1042" aria-hidden="true" tabindex="-1"></a><span class="in">    });</span></span>
<span id="cb20-1043"><a href="#cb20-1043" aria-hidden="true" tabindex="-1"></a><span class="in">});</span></span>
<span id="cb20-1044"><a href="#cb20-1044" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/script&gt;</span></span>
<span id="cb20-1045"><a href="#cb20-1045" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-1046"><a href="#cb20-1046" aria-hidden="true" tabindex="-1"></a>Two TED Talk chunks overlaid with pose estimations from MediaPipePose and MaskAnyoneUI-MediaPipe, featuring challenging segments with scene changes, camera movements, and periods without visible persons.</span>
<span id="cb20-1047"><a href="#cb20-1047" aria-hidden="true" tabindex="-1"></a>The first row shows a clip from the TED talk “Me Too is a Movement, Not a Moment” <span class="co">[</span><span class="ot">@ted-tarana</span><span class="co">]</span>.</span>
<span id="cb20-1048"><a href="#cb20-1048" aria-hidden="true" tabindex="-1"></a>The second row shows a clip from the TED talk “Universe / Statues / Liberation” <span class="co">[</span><span class="ot">@ted-song</span><span class="co">]</span>.</span>
<span id="cb20-1049"><a href="#cb20-1049" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1050"><a href="#cb20-1050" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1051"><a href="#cb20-1051" aria-hidden="true" tabindex="-1"></a><span class="fu">## Tragic Talkers {#sec-results-tragic-talkers}</span></span>
<span id="cb20-1052"><a href="#cb20-1052" aria-hidden="true" tabindex="-1"></a>@tbl-results-tragic-talkers presents the average metric results of various pose estimators on the Tragic Talkers dataset.</span>
<span id="cb20-1053"><a href="#cb20-1053" aria-hidden="true" tabindex="-1"></a>Regarding accuracy against the pseudo-ground truth, YoloPose achieves the highest PCK at 96%, followed by OpenPose at 87%.</span>
<span id="cb20-1054"><a href="#cb20-1054" aria-hidden="true" tabindex="-1"></a>All pose estimators except MediaPipePose exceed a PCK of 83%, with MediaPipePose detecting only 69% of keypoints correctly.</span>
<span id="cb20-1055"><a href="#cb20-1055" aria-hidden="true" tabindex="-1"></a>However, these PCK and RMSE values should be interpreted cautiously, as the pseudo-ground truth was generated by an AI model rather than human annotators, making it inherently imperfect.</span>
<span id="cb20-1056"><a href="#cb20-1056" aria-hidden="true" tabindex="-1"></a>Thus, the results reflect how closely each pose estimator matches the OpenPose output, rather than absolute pose estimation quality.</span>
<span id="cb20-1057"><a href="#cb20-1057" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1058"><a href="#cb20-1058" aria-hidden="true" tabindex="-1"></a>The kinematic metrics, especially acceleration and jerk, provide clearer results.</span>
<span id="cb20-1059"><a href="#cb20-1059" aria-hidden="true" tabindex="-1"></a>MaskAnyoneAPI-MediaPipe performs best, with the lowest acceleration and jerk values of approximately 2.9 and 5.0, respectively.</span>
<span id="cb20-1060"><a href="#cb20-1060" aria-hidden="true" tabindex="-1"></a>MaskAnyoneUI-MediaPipe follows closely behind MaskAnyoneAPI-MediaPipe with slightly increased acceleration and jerk, while YoloPose shows similar acceleration but a somewhat higher jerk.</span>
<span id="cb20-1061"><a href="#cb20-1061" aria-hidden="true" tabindex="-1"></a>Although MaskAnyone-OpenPose variants outperform standard OpenPose, they still exhibit noticeably greater acceleration and jerk, reflecting less smooth motion.</span>
<span id="cb20-1062"><a href="#cb20-1062" aria-hidden="true" tabindex="-1"></a>Pure MediaPipePose remains the least stable estimator, with average acceleration and jerk values of approximately 9.8 and 17.6, respectively.</span>
<span id="cb20-1063"><a href="#cb20-1063" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1064"><a href="#cb20-1064" aria-hidden="true" tabindex="-1"></a>::: {#tbl-results-tragic-talkers}</span>
<span id="cb20-1065"><a href="#cb20-1065" aria-hidden="true" tabindex="-1"></a><span class="in">```{=html}</span></span>
<span id="cb20-1066"><a href="#cb20-1066" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;table class="results"&gt;&lt;thead&gt;</span></span>
<span id="cb20-1067"><a href="#cb20-1067" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;tr&gt;</span></span>
<span id="cb20-1068"><a href="#cb20-1068" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;th&gt;Pose Estimator&lt;/th&gt;</span></span>
<span id="cb20-1069"><a href="#cb20-1069" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;th&gt;PCK&lt;/th&gt;</span></span>
<span id="cb20-1070"><a href="#cb20-1070" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;th&gt;RMSE&lt;/th&gt;</span></span>
<span id="cb20-1071"><a href="#cb20-1071" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;th&gt;Velocity&lt;/th&gt;</span></span>
<span id="cb20-1072"><a href="#cb20-1072" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;th&gt;Acceleration&lt;/th&gt;</span></span>
<span id="cb20-1073"><a href="#cb20-1073" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;th&gt;Jerk&lt;/th&gt;</span></span>
<span id="cb20-1074"><a href="#cb20-1074" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;/tr&gt;&lt;/thead&gt;</span></span>
<span id="cb20-1075"><a href="#cb20-1075" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;tbody&gt;</span></span>
<span id="cb20-1076"><a href="#cb20-1076" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;tr&gt;</span></span>
<span id="cb20-1077"><a href="#cb20-1077" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;YoloPose&lt;/td&gt;</span></span>
<span id="cb20-1078"><a href="#cb20-1078" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td class="best"&gt;0.96&lt;/td&gt;</span></span>
<span id="cb20-1079"><a href="#cb20-1079" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td class="second"&gt;0.11&lt;/td&gt;</span></span>
<span id="cb20-1080"><a href="#cb20-1080" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;4.36&lt;/td&gt;</span></span>
<span id="cb20-1081"><a href="#cb20-1081" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;3.27&lt;/td&gt;</span></span>
<span id="cb20-1082"><a href="#cb20-1082" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;5.57&lt;/td&gt;</span></span>
<span id="cb20-1083"><a href="#cb20-1083" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;/tr&gt;</span></span>
<span id="cb20-1084"><a href="#cb20-1084" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;tr&gt;</span></span>
<span id="cb20-1085"><a href="#cb20-1085" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;MediaPipePose&lt;/td&gt;</span></span>
<span id="cb20-1086"><a href="#cb20-1086" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;0.69&lt;/td&gt;</span></span>
<span id="cb20-1087"><a href="#cb20-1087" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;0.47&lt;/td&gt;</span></span>
<span id="cb20-1088"><a href="#cb20-1088" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;6.48&lt;/td&gt;</span></span>
<span id="cb20-1089"><a href="#cb20-1089" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;9.80&lt;/td&gt;</span></span>
<span id="cb20-1090"><a href="#cb20-1090" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;17.58&lt;/td&gt;</span></span>
<span id="cb20-1091"><a href="#cb20-1091" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;/tr&gt;</span></span>
<span id="cb20-1092"><a href="#cb20-1092" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;tr&gt;</span></span>
<span id="cb20-1093"><a href="#cb20-1093" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;OpenPose&lt;/td&gt;</span></span>
<span id="cb20-1094"><a href="#cb20-1094" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td class="second"&gt;0.87&lt;/td&gt;</span></span>
<span id="cb20-1095"><a href="#cb20-1095" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;0.33&lt;/td&gt;</span></span>
<span id="cb20-1096"><a href="#cb20-1096" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;4.63&lt;/td&gt;</span></span>
<span id="cb20-1097"><a href="#cb20-1097" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;6.38&lt;/td&gt;</span></span>
<span id="cb20-1098"><a href="#cb20-1098" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;10.00&lt;/td&gt;</span></span>
<span id="cb20-1099"><a href="#cb20-1099" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;/tr&gt;</span></span>
<span id="cb20-1100"><a href="#cb20-1100" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;tr class="maskanyone-api border-top"&gt;</span></span>
<span id="cb20-1101"><a href="#cb20-1101" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;MaskAnyoneAPI-MediaPipe&lt;/td&gt;</span></span>
<span id="cb20-1102"><a href="#cb20-1102" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;0.78&lt;/td&gt;</span></span>
<span id="cb20-1103"><a href="#cb20-1103" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;0.12&lt;/td&gt;</span></span>
<span id="cb20-1104"><a href="#cb20-1104" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td class="second"&gt;3.46&lt;/td&gt;</span></span>
<span id="cb20-1105"><a href="#cb20-1105" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td class="best"&gt;2.86&lt;/td&gt;</span></span>
<span id="cb20-1106"><a href="#cb20-1106" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td class="best"&gt;5.01&lt;/td&gt;</span></span>
<span id="cb20-1107"><a href="#cb20-1107" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;/tr&gt;</span></span>
<span id="cb20-1108"><a href="#cb20-1108" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;tr class="maskanyone-api border-bottom"&gt;</span></span>
<span id="cb20-1109"><a href="#cb20-1109" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;MaskAnyoneAPI-OpenPose&lt;/td&gt;</span></span>
<span id="cb20-1110"><a href="#cb20-1110" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;0.85&lt;/td&gt;</span></span>
<span id="cb20-1111"><a href="#cb20-1111" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;0.36&lt;/td&gt;</span></span>
<span id="cb20-1112"><a href="#cb20-1112" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;5.69&lt;/td&gt;</span></span>
<span id="cb20-1113"><a href="#cb20-1113" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;6.08&lt;/td&gt;</span></span>
<span id="cb20-1114"><a href="#cb20-1114" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;10.22&lt;/td&gt;</span></span>
<span id="cb20-1115"><a href="#cb20-1115" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;/tr&gt;</span></span>
<span id="cb20-1116"><a href="#cb20-1116" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;tr class="maskanyone-ui border-top"&gt;</span></span>
<span id="cb20-1117"><a href="#cb20-1117" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;MaskAnyoneUI-MediaPipe&lt;/td&gt;</span></span>
<span id="cb20-1118"><a href="#cb20-1118" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;0.83&lt;/td&gt;</span></span>
<span id="cb20-1119"><a href="#cb20-1119" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td class="best"&gt;0.07&lt;/td&gt;</span></span>
<span id="cb20-1120"><a href="#cb20-1120" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td class="best"&gt;3.26&lt;/td&gt;</span></span>
<span id="cb20-1121"><a href="#cb20-1121" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td class="second"&gt;2.91&lt;/td&gt;</span></span>
<span id="cb20-1122"><a href="#cb20-1122" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td class="second"&gt;5.10&lt;/td&gt;</span></span>
<span id="cb20-1123"><a href="#cb20-1123" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;/tr&gt;</span></span>
<span id="cb20-1124"><a href="#cb20-1124" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;tr class="maskanyone-ui border-bottom"&gt;</span></span>
<span id="cb20-1125"><a href="#cb20-1125" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;MaskAnyoneUI-OpenPose&lt;/td&gt;</span></span>
<span id="cb20-1126"><a href="#cb20-1126" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;0.85&lt;/td&gt;</span></span>
<span id="cb20-1127"><a href="#cb20-1127" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;0.36&lt;/td&gt;</span></span>
<span id="cb20-1128"><a href="#cb20-1128" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;5.53&lt;/td&gt;</span></span>
<span id="cb20-1129"><a href="#cb20-1129" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;5.12&lt;/td&gt;</span></span>
<span id="cb20-1130"><a href="#cb20-1130" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;td&gt;9.06&lt;/td&gt;</span></span>
<span id="cb20-1131"><a href="#cb20-1131" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;/tr&gt;</span></span>
<span id="cb20-1132"><a href="#cb20-1132" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/tbody&gt;</span></span>
<span id="cb20-1133"><a href="#cb20-1133" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/table&gt;</span></span>
<span id="cb20-1134"><a href="#cb20-1134" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-1135"><a href="#cb20-1135" aria-hidden="true" tabindex="-1"></a>Average metric results for different pose estimators aggregated over four camera angles of five Tragic Talkers sequences with pseudo-ground truth.</span>
<span id="cb20-1136"><a href="#cb20-1136" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1137"><a href="#cb20-1137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1138"><a href="#cb20-1138" aria-hidden="true" tabindex="-1"></a>@fig-tragic-talkers-acceleration-distribution and @fig-tragic-talkers-jerk-distribution confirm the results from @tbl-results-tragic-talkers.</span>
<span id="cb20-1139"><a href="#cb20-1139" aria-hidden="true" tabindex="-1"></a>Both plots show that the MaskAnyone-MediaPipe pose estimators achieve the highest proportion of low acceleration and jerk values, followed by YoloPose, the MaskAnyone-OpenPose pose estimators, and OpenPose.</span>
<span id="cb20-1140"><a href="#cb20-1140" aria-hidden="true" tabindex="-1"></a>MediaPipePose once again has a very flat curve, indicating a lot of large acceleration and jerk values.</span>
<span id="cb20-1141"><a href="#cb20-1141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1142"><a href="#cb20-1142" aria-hidden="true" tabindex="-1"></a>@fig-tragic-talkers-acceleration-per-keypoint shows that MediaPipePose is among the pose estimators with the highest median acceleration values for all keypoints.</span>
<span id="cb20-1143"><a href="#cb20-1143" aria-hidden="true" tabindex="-1"></a>YoloPose, MaskAnyoneAPI-MediaPipe, and MaskAnyoneUI-MediaPipe achieve consistently low median acceleration values for all keypoints.</span>
<span id="cb20-1144"><a href="#cb20-1144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1145"><a href="#cb20-1145" aria-hidden="true" tabindex="-1"></a>::: {#fig-tragic-talkers-plots layout="[<span class="co">[</span><span class="ot">1,1</span><span class="co">]</span>, <span class="co">[</span><span class="ot">1</span><span class="co">]</span>]"}</span>
<span id="cb20-1146"><a href="#cb20-1146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1147"><a href="#cb20-1147" aria-hidden="true" tabindex="-1"></a><span class="al">![Acceleration Distribution](plots/TragicTalkers/acceleration_distribution.png)</span>{#fig-tragic-talkers-acceleration-distribution}</span>
<span id="cb20-1148"><a href="#cb20-1148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1149"><a href="#cb20-1149" aria-hidden="true" tabindex="-1"></a><span class="al">![Jerk Distribution](plots/TragicTalkers/jerk_distribution.png)</span>{#fig-tragic-talkers-jerk-distribution}</span>
<span id="cb20-1150"><a href="#cb20-1150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1151"><a href="#cb20-1151" aria-hidden="true" tabindex="-1"></a><span class="al">![Median Acceleration per Keypoint](plots/TragicTalkers/keypoint_plot_Acceleration.png)</span>{#fig-tragic-talkers-acceleration-per-keypoint}</span>
<span id="cb20-1152"><a href="#cb20-1152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1153"><a href="#cb20-1153" aria-hidden="true" tabindex="-1"></a>**Comparison of pose estimation models on the Tragic Talkers dataset.**</span>
<span id="cb20-1154"><a href="#cb20-1154" aria-hidden="true" tabindex="-1"></a>(a, b) Acceleration and jerk distribution showing the proportion of keypoints within a fixed value range. </span>
<span id="cb20-1155"><a href="#cb20-1155" aria-hidden="true" tabindex="-1"></a>A larger concentration at lower values indicates a smoother and more stable pose, whereas a high number of keypoints with large values indicates more jittery movements.</span>
<span id="cb20-1156"><a href="#cb20-1156" aria-hidden="true" tabindex="-1"></a>(c) Median acceleration per keypoint, indicating stability across individual body parts.</span>
<span id="cb20-1157"><a href="#cb20-1157" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1158"><a href="#cb20-1158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1159"><a href="#cb20-1159" aria-hidden="true" tabindex="-1"></a>Interestingly, although the MaskAnyone-OpenPose pose estimators achieve lower acceleration values for nose, eye, ear, shoulder, and ankle keypoints than pure OpenPose, they perform worse for the elbow, hip, and knee keypoints.</span>
<span id="cb20-1160"><a href="#cb20-1160" aria-hidden="true" tabindex="-1"></a>A potential reason for this could be that MaskAnyone uses a higher confidence threshold for keypoints than our OpenPose implementation, which leads to the elbow, hip, and knee keypoints not being detected or rendered.</span>
<span id="cb20-1161"><a href="#cb20-1161" aria-hidden="true" tabindex="-1"></a>As an example, consider @tbl-results-tragic-talkers-bad-legs, which shows the first seconds of the rendered video for OpenPose, MaskAnyoneAPI-OpenPose, and MaskAnyoneUI-OpenPose for the “conversation1_t3-cam08” sequence.</span>
<span id="cb20-1162"><a href="#cb20-1162" aria-hidden="true" tabindex="-1"></a>In this scene, both MaskAnyone-OpenPose pose estimators fail to detect the legs of the woman, while OpenPose correctly detects them.</span>
<span id="cb20-1163"><a href="#cb20-1163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1164"><a href="#cb20-1164" aria-hidden="true" tabindex="-1"></a>::: {#tbl-results-tragic-talkers-bad-legs}</span>
<span id="cb20-1165"><a href="#cb20-1165" aria-hidden="true" tabindex="-1"></a><span class="in">```{=html}</span></span>
<span id="cb20-1166"><a href="#cb20-1166" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;table class="video-table"&gt;</span></span>
<span id="cb20-1167"><a href="#cb20-1167" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;tr&gt;</span></span>
<span id="cb20-1168"><a href="#cb20-1168" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;td&gt;</span></span>
<span id="cb20-1169"><a href="#cb20-1169" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-zoom-wrapper"&gt;</span></span>
<span id="cb20-1170"><a href="#cb20-1170" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;video class="video-zoom tt-legs-sync" autoplay muted playsinline&gt;</span></span>
<span id="cb20-1171"><a href="#cb20-1171" aria-hidden="true" tabindex="-1"></a><span class="in">                    &lt;source src="videos/TragicTalkers/conversation1_t3-cam08/conversation1_t3-cam08_OpenPose.mp4#t=0,10" type="video/mp4"&gt;</span></span>
<span id="cb20-1172"><a href="#cb20-1172" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;/video&gt;</span></span>
<span id="cb20-1173"><a href="#cb20-1173" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;/div&gt;</span></span>
<span id="cb20-1174"><a href="#cb20-1174" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-caption"&gt;OpenPose&lt;/div&gt;</span></span>
<span id="cb20-1175"><a href="#cb20-1175" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;/td&gt;</span></span>
<span id="cb20-1176"><a href="#cb20-1176" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;td&gt;</span></span>
<span id="cb20-1177"><a href="#cb20-1177" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-zoom-wrapper"&gt;</span></span>
<span id="cb20-1178"><a href="#cb20-1178" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;video class="video-zoom tt-legs-sync" autoplay muted playsinline&gt;</span></span>
<span id="cb20-1179"><a href="#cb20-1179" aria-hidden="true" tabindex="-1"></a><span class="in">                    &lt;source src="videos/TragicTalkers/conversation1_t3-cam08/conversation1_t3-cam08_MaskAnyoneAPI-OpenPose.mp4#t=0,10" type="video/mp4"&gt;</span></span>
<span id="cb20-1180"><a href="#cb20-1180" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;/video&gt;</span></span>
<span id="cb20-1181"><a href="#cb20-1181" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;/div&gt;</span></span>
<span id="cb20-1182"><a href="#cb20-1182" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-caption"&gt;MaskAnyoneAPI-OpenPose&lt;/div&gt;</span></span>
<span id="cb20-1183"><a href="#cb20-1183" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;/td&gt;</span></span>
<span id="cb20-1184"><a href="#cb20-1184" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;td&gt;</span></span>
<span id="cb20-1185"><a href="#cb20-1185" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-zoom-wrapper"&gt;</span></span>
<span id="cb20-1186"><a href="#cb20-1186" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;video class="video-zoom tt-legs-sync" autoplay muted playsinline&gt;</span></span>
<span id="cb20-1187"><a href="#cb20-1187" aria-hidden="true" tabindex="-1"></a><span class="in">                    &lt;source src="videos/TragicTalkers/conversation1_t3-cam08/conversation1_t3-cam08_MaskAnyoneUI-OpenPose.mp4#t=0,10" type="video/mp4"&gt;</span></span>
<span id="cb20-1188"><a href="#cb20-1188" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;/video&gt;</span></span>
<span id="cb20-1189"><a href="#cb20-1189" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;/div&gt;</span></span>
<span id="cb20-1190"><a href="#cb20-1190" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-caption"&gt;MaskAnyoneUI-OpenPose&lt;/div&gt;</span></span>
<span id="cb20-1191"><a href="#cb20-1191" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;/td&gt;</span></span>
<span id="cb20-1192"><a href="#cb20-1192" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/tr&gt;</span></span>
<span id="cb20-1193"><a href="#cb20-1193" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/table&gt;</span></span>
<span id="cb20-1194"><a href="#cb20-1194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1195"><a href="#cb20-1195" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;script&gt;</span></span>
<span id="cb20-1196"><a href="#cb20-1196" aria-hidden="true" tabindex="-1"></a><span class="in">document.addEventListener('DOMContentLoaded', function() {</span></span>
<span id="cb20-1197"><a href="#cb20-1197" aria-hidden="true" tabindex="-1"></a><span class="in">    const videos = document.querySelectorAll('.tt-legs-sync');</span></span>
<span id="cb20-1198"><a href="#cb20-1198" aria-hidden="true" tabindex="-1"></a><span class="in">    let videosReady = 0;</span></span>
<span id="cb20-1199"><a href="#cb20-1199" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb20-1200"><a href="#cb20-1200" aria-hidden="true" tabindex="-1"></a><span class="in">    function checkAllVideosEnded(video) {</span></span>
<span id="cb20-1201"><a href="#cb20-1201" aria-hidden="true" tabindex="-1"></a><span class="in">        if (video.currentTime &gt;= 10) {  // Check if we've reached the fragment end time</span></span>
<span id="cb20-1202"><a href="#cb20-1202" aria-hidden="true" tabindex="-1"></a><span class="in">            videosReady++;</span></span>
<span id="cb20-1203"><a href="#cb20-1203" aria-hidden="true" tabindex="-1"></a><span class="in">            if (videosReady === videos.length) {</span></span>
<span id="cb20-1204"><a href="#cb20-1204" aria-hidden="true" tabindex="-1"></a><span class="in">                videos.forEach(v =&gt; {</span></span>
<span id="cb20-1205"><a href="#cb20-1205" aria-hidden="true" tabindex="-1"></a><span class="in">                    v.currentTime = 0;</span></span>
<span id="cb20-1206"><a href="#cb20-1206" aria-hidden="true" tabindex="-1"></a><span class="in">                    v.play();</span></span>
<span id="cb20-1207"><a href="#cb20-1207" aria-hidden="true" tabindex="-1"></a><span class="in">                });</span></span>
<span id="cb20-1208"><a href="#cb20-1208" aria-hidden="true" tabindex="-1"></a><span class="in">                videosReady = 0;</span></span>
<span id="cb20-1209"><a href="#cb20-1209" aria-hidden="true" tabindex="-1"></a><span class="in">            }</span></span>
<span id="cb20-1210"><a href="#cb20-1210" aria-hidden="true" tabindex="-1"></a><span class="in">        }</span></span>
<span id="cb20-1211"><a href="#cb20-1211" aria-hidden="true" tabindex="-1"></a><span class="in">    }</span></span>
<span id="cb20-1212"><a href="#cb20-1212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1213"><a href="#cb20-1213" aria-hidden="true" tabindex="-1"></a><span class="in">    videos.forEach(video =&gt; {</span></span>
<span id="cb20-1214"><a href="#cb20-1214" aria-hidden="true" tabindex="-1"></a><span class="in">        video.addEventListener('timeupdate', () =&gt; checkAllVideosEnded(video));</span></span>
<span id="cb20-1215"><a href="#cb20-1215" aria-hidden="true" tabindex="-1"></a><span class="in">    });</span></span>
<span id="cb20-1216"><a href="#cb20-1216" aria-hidden="true" tabindex="-1"></a><span class="in">});</span></span>
<span id="cb20-1217"><a href="#cb20-1217" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/script&gt;</span></span>
<span id="cb20-1218"><a href="#cb20-1218" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-1219"><a href="#cb20-1219" aria-hidden="true" tabindex="-1"></a>First 10 seconds of the rendered Tragic Talkers videos for OpenPose, MaskAnyoneAPI-OpenPose, and MaskAnyoneUI-OpenPose for the "conversation1_t3-cam08" sequence.</span>
<span id="cb20-1220"><a href="#cb20-1220" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1221"><a href="#cb20-1221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1222"><a href="#cb20-1222" aria-hidden="true" tabindex="-1"></a>Last but not least, we qualitatively compare MaskAnyoneAPI-MediaPipe, MaskAnyoneUI-MediaPipe, and YoloPose on the “interactive4_t3-cam08” sequence (@tbl-results-tragic-talkers-best-models).</span>
<span id="cb20-1223"><a href="#cb20-1223" aria-hidden="true" tabindex="-1"></a>These three pose estimators have the lowest overall average acceleration values, as shown in @tbl-results-tragic-talkers.</span>
<span id="cb20-1224"><a href="#cb20-1224" aria-hidden="true" tabindex="-1"></a>Two important observations were made:</span>
<span id="cb20-1225"><a href="#cb20-1225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1226"><a href="#cb20-1226" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>YoloPose is the only estimator that correctly identifies the woman when she turns around, facing away from the camera.</span>
<span id="cb20-1227"><a href="#cb20-1227" aria-hidden="true" tabindex="-1"></a>Both MaskAnyone variants fail in this scenario.</span>
<span id="cb20-1228"><a href="#cb20-1228" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>At the start of the sequence, where both actors stand with hands stretched out, only YoloPose correctly captures the lower part of the woman’s body.</span>
<span id="cb20-1229"><a href="#cb20-1229" aria-hidden="true" tabindex="-1"></a>Both MaskAnyone estimators produce an incorrect upper-body pose initially, which improves as the woman lowers her arms, eventually stabilizing in the correct position.</span>
<span id="cb20-1230"><a href="#cb20-1230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1231"><a href="#cb20-1231" aria-hidden="true" tabindex="-1"></a>Qualitatively, YoloPose performs best on this sequence.</span>
<span id="cb20-1232"><a href="#cb20-1232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1233"><a href="#cb20-1233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1234"><a href="#cb20-1234" aria-hidden="true" tabindex="-1"></a>::: {#tbl-results-tragic-talkers-best-models}</span>
<span id="cb20-1235"><a href="#cb20-1235" aria-hidden="true" tabindex="-1"></a><span class="in">```{=html}</span></span>
<span id="cb20-1236"><a href="#cb20-1236" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;table class="video-table"&gt;</span></span>
<span id="cb20-1237"><a href="#cb20-1237" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;tr&gt;</span></span>
<span id="cb20-1238"><a href="#cb20-1238" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;td&gt;</span></span>
<span id="cb20-1239"><a href="#cb20-1239" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-zoom-wrapper"&gt;</span></span>
<span id="cb20-1240"><a href="#cb20-1240" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;video class="video-zoom tt-best-models-sync" autoplay muted playsinline&gt;</span></span>
<span id="cb20-1241"><a href="#cb20-1241" aria-hidden="true" tabindex="-1"></a><span class="in">                    &lt;source src="videos/TragicTalkers/interactive4_t3-cam08/interactive4_t3-cam08_YoloPose.mp4" type="video/mp4"&gt;</span></span>
<span id="cb20-1242"><a href="#cb20-1242" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;/video&gt;</span></span>
<span id="cb20-1243"><a href="#cb20-1243" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;/div&gt;</span></span>
<span id="cb20-1244"><a href="#cb20-1244" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-caption"&gt;YoloPose&lt;/div&gt;</span></span>
<span id="cb20-1245"><a href="#cb20-1245" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;/td&gt;</span></span>
<span id="cb20-1246"><a href="#cb20-1246" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;td&gt;</span></span>
<span id="cb20-1247"><a href="#cb20-1247" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-zoom-wrapper"&gt;</span></span>
<span id="cb20-1248"><a href="#cb20-1248" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;video class="video-zoom tt-best-models-sync" autoplay muted playsinline&gt;</span></span>
<span id="cb20-1249"><a href="#cb20-1249" aria-hidden="true" tabindex="-1"></a><span class="in">                    &lt;source src="videos/TragicTalkers/interactive4_t3-cam08/interactive4_t3-cam08_MaskAnyoneAPI-MediaPipe.mp4" type="video/mp4"&gt;</span></span>
<span id="cb20-1250"><a href="#cb20-1250" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;/video&gt;</span></span>
<span id="cb20-1251"><a href="#cb20-1251" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;/div&gt;</span></span>
<span id="cb20-1252"><a href="#cb20-1252" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-caption"&gt;MaskAnyoneAPI-MediaPipe&lt;/div&gt;</span></span>
<span id="cb20-1253"><a href="#cb20-1253" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;/td&gt;</span></span>
<span id="cb20-1254"><a href="#cb20-1254" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;td&gt;</span></span>
<span id="cb20-1255"><a href="#cb20-1255" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-zoom-wrapper"&gt;</span></span>
<span id="cb20-1256"><a href="#cb20-1256" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;video class="video-zoom tt-best-models-sync" autoplay muted playsinline&gt;</span></span>
<span id="cb20-1257"><a href="#cb20-1257" aria-hidden="true" tabindex="-1"></a><span class="in">                    &lt;source src="videos/TragicTalkers/interactive4_t3-cam08/interactive4_t3-cam08_MaskAnyoneUI-MediaPipe.mp4" type="video/mp4"&gt;</span></span>
<span id="cb20-1258"><a href="#cb20-1258" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;/video&gt;</span></span>
<span id="cb20-1259"><a href="#cb20-1259" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;/div&gt;</span></span>
<span id="cb20-1260"><a href="#cb20-1260" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-caption"&gt;MaskAnyoneUI-MediaPipe&lt;/div&gt;</span></span>
<span id="cb20-1261"><a href="#cb20-1261" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;/td&gt;</span></span>
<span id="cb20-1262"><a href="#cb20-1262" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/tr&gt;</span></span>
<span id="cb20-1263"><a href="#cb20-1263" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/table&gt;</span></span>
<span id="cb20-1264"><a href="#cb20-1264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1265"><a href="#cb20-1265" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;script&gt;</span></span>
<span id="cb20-1266"><a href="#cb20-1266" aria-hidden="true" tabindex="-1"></a><span class="in">document.addEventListener('DOMContentLoaded', function() {</span></span>
<span id="cb20-1267"><a href="#cb20-1267" aria-hidden="true" tabindex="-1"></a><span class="in">    const videos = document.querySelectorAll('.tt-best-models-sync');</span></span>
<span id="cb20-1268"><a href="#cb20-1268" aria-hidden="true" tabindex="-1"></a><span class="in">    let endedCount = 0;</span></span>
<span id="cb20-1269"><a href="#cb20-1269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1270"><a href="#cb20-1270" aria-hidden="true" tabindex="-1"></a><span class="in">    function restartAllVideos() {</span></span>
<span id="cb20-1271"><a href="#cb20-1271" aria-hidden="true" tabindex="-1"></a><span class="in">        videos.forEach(video =&gt; {</span></span>
<span id="cb20-1272"><a href="#cb20-1272" aria-hidden="true" tabindex="-1"></a><span class="in">            video.currentTime = 0;</span></span>
<span id="cb20-1273"><a href="#cb20-1273" aria-hidden="true" tabindex="-1"></a><span class="in">            video.play();</span></span>
<span id="cb20-1274"><a href="#cb20-1274" aria-hidden="true" tabindex="-1"></a><span class="in">        });</span></span>
<span id="cb20-1275"><a href="#cb20-1275" aria-hidden="true" tabindex="-1"></a><span class="in">        endedCount = 0;</span></span>
<span id="cb20-1276"><a href="#cb20-1276" aria-hidden="true" tabindex="-1"></a><span class="in">    }</span></span>
<span id="cb20-1277"><a href="#cb20-1277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1278"><a href="#cb20-1278" aria-hidden="true" tabindex="-1"></a><span class="in">    videos.forEach(video =&gt; {</span></span>
<span id="cb20-1279"><a href="#cb20-1279" aria-hidden="true" tabindex="-1"></a><span class="in">        video.addEventListener('ended', () =&gt; {</span></span>
<span id="cb20-1280"><a href="#cb20-1280" aria-hidden="true" tabindex="-1"></a><span class="in">            endedCount++;</span></span>
<span id="cb20-1281"><a href="#cb20-1281" aria-hidden="true" tabindex="-1"></a><span class="in">            if (endedCount === videos.length) {</span></span>
<span id="cb20-1282"><a href="#cb20-1282" aria-hidden="true" tabindex="-1"></a><span class="in">                restartAllVideos();</span></span>
<span id="cb20-1283"><a href="#cb20-1283" aria-hidden="true" tabindex="-1"></a><span class="in">            }</span></span>
<span id="cb20-1284"><a href="#cb20-1284" aria-hidden="true" tabindex="-1"></a><span class="in">        });</span></span>
<span id="cb20-1285"><a href="#cb20-1285" aria-hidden="true" tabindex="-1"></a><span class="in">    });</span></span>
<span id="cb20-1286"><a href="#cb20-1286" aria-hidden="true" tabindex="-1"></a><span class="in">});</span></span>
<span id="cb20-1287"><a href="#cb20-1287" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/script&gt;</span></span>
<span id="cb20-1288"><a href="#cb20-1288" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-1289"><a href="#cb20-1289" aria-hidden="true" tabindex="-1"></a>The most stable pose estimators on the Tragic Talkers "interactive4_t3-cam08" sequence.</span>
<span id="cb20-1290"><a href="#cb20-1290" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1291"><a href="#cb20-1291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1292"><a href="#cb20-1292" aria-hidden="true" tabindex="-1"></a><span class="fu">## Inference on Raw vs. Masked Videos {#sec-results-inference-raw-masked}</span></span>
<span id="cb20-1293"><a href="#cb20-1293" aria-hidden="true" tabindex="-1"></a>::: {#tbl-pck-raw-masked}</span>
<span id="cb20-1294"><a href="#cb20-1294" aria-hidden="true" tabindex="-1"></a><span class="in">```{=html}</span></span>
<span id="cb20-1295"><a href="#cb20-1295" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;table class="results raw-masked-table"&gt;</span></span>
<span id="cb20-1296"><a href="#cb20-1296" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;thead&gt;</span></span>
<span id="cb20-1297"><a href="#cb20-1297" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;tr&gt;</span></span>
<span id="cb20-1298"><a href="#cb20-1298" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;th&gt;Pose Estimator&lt;/th&gt;</span></span>
<span id="cb20-1299"><a href="#cb20-1299" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;th&gt;Blurring&lt;/th&gt;</span></span>
<span id="cb20-1300"><a href="#cb20-1300" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;th&gt;Pixelation&lt;/th&gt;</span></span>
<span id="cb20-1301"><a href="#cb20-1301" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;th&gt;Contours&lt;/th&gt;</span></span>
<span id="cb20-1302"><a href="#cb20-1302" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;th&gt;Solid Fill&lt;/th&gt;</span></span>
<span id="cb20-1303"><a href="#cb20-1303" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;th&gt;Average&lt;/th&gt;</span></span>
<span id="cb20-1304"><a href="#cb20-1304" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/tr&gt;</span></span>
<span id="cb20-1305"><a href="#cb20-1305" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;/thead&gt;</span></span>
<span id="cb20-1306"><a href="#cb20-1306" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;tbody&gt;</span></span>
<span id="cb20-1307"><a href="#cb20-1307" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;tr&gt;</span></span>
<span id="cb20-1308"><a href="#cb20-1308" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;YoloPose&lt;/td&gt;</span></span>
<span id="cb20-1309"><a href="#cb20-1309" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td class="best"&gt;0.95&lt;/td&gt;</span></span>
<span id="cb20-1310"><a href="#cb20-1310" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.09&lt;/td&gt;</span></span>
<span id="cb20-1311"><a href="#cb20-1311" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td class="best"&gt;0.93&lt;/td&gt;</span></span>
<span id="cb20-1312"><a href="#cb20-1312" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td class="second"&gt;0.32&lt;/td&gt;</span></span>
<span id="cb20-1313"><a href="#cb20-1313" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td class="second"&gt;0.57&lt;/td&gt;</span></span>
<span id="cb20-1314"><a href="#cb20-1314" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/tr&gt;</span></span>
<span id="cb20-1315"><a href="#cb20-1315" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;tr&gt;</span></span>
<span id="cb20-1316"><a href="#cb20-1316" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;MediaPipePose&lt;/td&gt;</span></span>
<span id="cb20-1317"><a href="#cb20-1317" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td class="best"&gt;0.95&lt;/td&gt;</span></span>
<span id="cb20-1318"><a href="#cb20-1318" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td class="best"&gt;0.81&lt;/td&gt;</span></span>
<span id="cb20-1319"><a href="#cb20-1319" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.56&lt;/td&gt;</span></span>
<span id="cb20-1320"><a href="#cb20-1320" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td class="best"&gt;0.34&lt;/td&gt;</span></span>
<span id="cb20-1321"><a href="#cb20-1321" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td class="best"&gt;0.67&lt;/td&gt;</span></span>
<span id="cb20-1322"><a href="#cb20-1322" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/tr&gt;</span></span>
<span id="cb20-1323"><a href="#cb20-1323" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;tr&gt;</span></span>
<span id="cb20-1324"><a href="#cb20-1324" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;OpenPose&lt;/td&gt;</span></span>
<span id="cb20-1325"><a href="#cb20-1325" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td class="second"&gt;0.88&lt;/td&gt;</span></span>
<span id="cb20-1326"><a href="#cb20-1326" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.10&lt;/td&gt;</span></span>
<span id="cb20-1327"><a href="#cb20-1327" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td class="second"&gt;0.62&lt;/td&gt;</span></span>
<span id="cb20-1328"><a href="#cb20-1328" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.01&lt;/td&gt;</span></span>
<span id="cb20-1329"><a href="#cb20-1329" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.40&lt;/td&gt;</span></span>
<span id="cb20-1330"><a href="#cb20-1330" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/tr&gt;</span></span>
<span id="cb20-1331"><a href="#cb20-1331" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;tr class="maskanyone-api border-top"&gt;</span></span>
<span id="cb20-1332"><a href="#cb20-1332" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;MaskAnyoneAPI-MediaPipe&lt;/td&gt;</span></span>
<span id="cb20-1333"><a href="#cb20-1333" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.85&lt;/td&gt;</span></span>
<span id="cb20-1334"><a href="#cb20-1334" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.30&lt;/td&gt;</span></span>
<span id="cb20-1335"><a href="#cb20-1335" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.00&lt;/td&gt;</span></span>
<span id="cb20-1336"><a href="#cb20-1336" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.00&lt;/td&gt;</span></span>
<span id="cb20-1337"><a href="#cb20-1337" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.29&lt;/td&gt;</span></span>
<span id="cb20-1338"><a href="#cb20-1338" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/tr&gt;</span></span>
<span id="cb20-1339"><a href="#cb20-1339" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;tr class="maskanyone-api border-bottom"&gt;</span></span>
<span id="cb20-1340"><a href="#cb20-1340" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;MaskAnyoneAPI-OpenPose&lt;/td&gt;</span></span>
<span id="cb20-1341"><a href="#cb20-1341" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.75&lt;/td&gt;</span></span>
<span id="cb20-1342"><a href="#cb20-1342" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.00&lt;/td&gt;</span></span>
<span id="cb20-1343"><a href="#cb20-1343" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.36&lt;/td&gt;</span></span>
<span id="cb20-1344"><a href="#cb20-1344" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.00&lt;/td&gt;</span></span>
<span id="cb20-1345"><a href="#cb20-1345" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.28&lt;/td&gt;</span></span>
<span id="cb20-1346"><a href="#cb20-1346" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/tr&gt;</span></span>
<span id="cb20-1347"><a href="#cb20-1347" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;tr class="maskanyone-ui border-top"&gt;</span></span>
<span id="cb20-1348"><a href="#cb20-1348" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;MaskAnyoneUI-MediaPipe&lt;/td&gt;</span></span>
<span id="cb20-1349"><a href="#cb20-1349" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td class="best"&gt;0.95&lt;/td&gt;</span></span>
<span id="cb20-1350"><a href="#cb20-1350" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td class="second"&gt;0.63&lt;/td&gt;</span></span>
<span id="cb20-1351"><a href="#cb20-1351" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.00&lt;/td&gt;</span></span>
<span id="cb20-1352"><a href="#cb20-1352" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.07&lt;/td&gt;</span></span>
<span id="cb20-1353"><a href="#cb20-1353" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.41&lt;/td&gt;</span></span>
<span id="cb20-1354"><a href="#cb20-1354" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/tr&gt;</span></span>
<span id="cb20-1355"><a href="#cb20-1355" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;tr class="maskanyone-ui border-bottom"&gt;</span></span>
<span id="cb20-1356"><a href="#cb20-1356" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;MaskAnyoneUI-OpenPose&lt;/td&gt;</span></span>
<span id="cb20-1357"><a href="#cb20-1357" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.87&lt;/td&gt;</span></span>
<span id="cb20-1358"><a href="#cb20-1358" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.03&lt;/td&gt;</span></span>
<span id="cb20-1359"><a href="#cb20-1359" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.58&lt;/td&gt;</span></span>
<span id="cb20-1360"><a href="#cb20-1360" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.00&lt;/td&gt;</span></span>
<span id="cb20-1361"><a href="#cb20-1361" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.37&lt;/td&gt;</span></span>
<span id="cb20-1362"><a href="#cb20-1362" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/tr&gt;</span></span>
<span id="cb20-1363"><a href="#cb20-1363" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;tr&gt;</span></span>
<span id="cb20-1364"><a href="#cb20-1364" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;Average&lt;/td&gt;</span></span>
<span id="cb20-1365"><a href="#cb20-1365" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.86&lt;/td&gt;</span></span>
<span id="cb20-1366"><a href="#cb20-1366" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.23&lt;/td&gt;</span></span>
<span id="cb20-1367"><a href="#cb20-1367" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.44&lt;/td&gt;</span></span>
<span id="cb20-1368"><a href="#cb20-1368" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.11&lt;/td&gt;</span></span>
<span id="cb20-1369"><a href="#cb20-1369" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;/&lt;/td&gt;</span></span>
<span id="cb20-1370"><a href="#cb20-1370" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/tr&gt; </span></span>
<span id="cb20-1371"><a href="#cb20-1371" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;/tbody&gt;</span></span>
<span id="cb20-1372"><a href="#cb20-1372" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/table&gt;</span></span>
<span id="cb20-1373"><a href="#cb20-1373" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-1374"><a href="#cb20-1374" aria-hidden="true" tabindex="-1"></a>Percentage of correct keypoints (PCK) for different pose estimators on videos masked by different hiding strategies.</span>
<span id="cb20-1375"><a href="#cb20-1375" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1376"><a href="#cb20-1376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1377"><a href="#cb20-1377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1378"><a href="#cb20-1378" aria-hidden="true" tabindex="-1"></a>::: {#tbl-rmse-raw-masked}</span>
<span id="cb20-1379"><a href="#cb20-1379" aria-hidden="true" tabindex="-1"></a><span class="in">```{=html}</span></span>
<span id="cb20-1380"><a href="#cb20-1380" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;table class="results raw-masked-table"&gt;</span></span>
<span id="cb20-1381"><a href="#cb20-1381" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;thead&gt;</span></span>
<span id="cb20-1382"><a href="#cb20-1382" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;tr&gt;</span></span>
<span id="cb20-1383"><a href="#cb20-1383" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;th&gt;Pose Estimator&lt;/th&gt;</span></span>
<span id="cb20-1384"><a href="#cb20-1384" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;th&gt;Blurring&lt;/th&gt;</span></span>
<span id="cb20-1385"><a href="#cb20-1385" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;th&gt;Pixelation&lt;/th&gt;</span></span>
<span id="cb20-1386"><a href="#cb20-1386" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;th&gt;Contours&lt;/th&gt;</span></span>
<span id="cb20-1387"><a href="#cb20-1387" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;th&gt;Solid Fill&lt;/th&gt;</span></span>
<span id="cb20-1388"><a href="#cb20-1388" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;th&gt;Average&lt;/th&gt;</span></span>
<span id="cb20-1389"><a href="#cb20-1389" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/tr&gt;</span></span>
<span id="cb20-1390"><a href="#cb20-1390" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;/thead&gt;</span></span>
<span id="cb20-1391"><a href="#cb20-1391" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;tbody&gt;</span></span>
<span id="cb20-1392"><a href="#cb20-1392" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;tr&gt;</span></span>
<span id="cb20-1393"><a href="#cb20-1393" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;YoloPose&lt;/td&gt;</span></span>
<span id="cb20-1394"><a href="#cb20-1394" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td class="second"&gt;0.12&lt;/td&gt;</span></span>
<span id="cb20-1395"><a href="#cb20-1395" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.92&lt;/td&gt;</span></span>
<span id="cb20-1396"><a href="#cb20-1396" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td class="best"&gt;0.13&lt;/td&gt;</span></span>
<span id="cb20-1397"><a href="#cb20-1397" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td class="second"&gt;0.74&lt;/td&gt;</span></span>
<span id="cb20-1398"><a href="#cb20-1398" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td class="second"&gt;0.48&lt;/td&gt;</span></span>
<span id="cb20-1399"><a href="#cb20-1399" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/tr&gt;</span></span>
<span id="cb20-1400"><a href="#cb20-1400" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;tr&gt;</span></span>
<span id="cb20-1401"><a href="#cb20-1401" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;MediaPipePose&lt;/td&gt;</span></span>
<span id="cb20-1402"><a href="#cb20-1402" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td class="second"&gt;0.12&lt;/td&gt;</span></span>
<span id="cb20-1403"><a href="#cb20-1403" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td class="best"&gt;0.26&lt;/td&gt;</span></span>
<span id="cb20-1404"><a href="#cb20-1404" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.49&lt;/td&gt;</span></span>
<span id="cb20-1405"><a href="#cb20-1405" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td class="best"&gt;0.64&lt;/td&gt;</span></span>
<span id="cb20-1406"><a href="#cb20-1406" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td class="best"&gt;0.38&lt;/td&gt;</span></span>
<span id="cb20-1407"><a href="#cb20-1407" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/tr&gt;</span></span>
<span id="cb20-1408"><a href="#cb20-1408" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;tr&gt;</span></span>
<span id="cb20-1409"><a href="#cb20-1409" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;OpenPose&lt;/td&gt;</span></span>
<span id="cb20-1410"><a href="#cb20-1410" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.25&lt;/td&gt;</span></span>
<span id="cb20-1411"><a href="#cb20-1411" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.94&lt;/td&gt;</span></span>
<span id="cb20-1412"><a href="#cb20-1412" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td class="second"&gt;0.47&lt;/td&gt;</span></span>
<span id="cb20-1413"><a href="#cb20-1413" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;1.0&lt;/td&gt;</span></span>
<span id="cb20-1414"><a href="#cb20-1414" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.67&lt;/td&gt;</span></span>
<span id="cb20-1415"><a href="#cb20-1415" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/tr&gt;</span></span>
<span id="cb20-1416"><a href="#cb20-1416" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;tr class="maskanyone-api border-top"&gt;</span></span>
<span id="cb20-1417"><a href="#cb20-1417" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;MaskAnyoneAPI-MediaPipe&lt;/td&gt;</span></span>
<span id="cb20-1418"><a href="#cb20-1418" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.27&lt;/td&gt;</span></span>
<span id="cb20-1419"><a href="#cb20-1419" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.74&lt;/td&gt;</span></span>
<span id="cb20-1420"><a href="#cb20-1420" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;1.00&lt;/td&gt;</span></span>
<span id="cb20-1421"><a href="#cb20-1421" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.99&lt;/td&gt;</span></span>
<span id="cb20-1422"><a href="#cb20-1422" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.75&lt;/td&gt;</span></span>
<span id="cb20-1423"><a href="#cb20-1423" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/tr&gt;</span></span>
<span id="cb20-1424"><a href="#cb20-1424" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;tr class="maskanyone-api border-bottom"&gt;</span></span>
<span id="cb20-1425"><a href="#cb20-1425" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;MaskAnyoneAPI-OpenPose&lt;/td&gt;</span></span>
<span id="cb20-1426"><a href="#cb20-1426" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.43&lt;/td&gt;</span></span>
<span id="cb20-1427"><a href="#cb20-1427" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.99&lt;/td&gt;</span></span>
<span id="cb20-1428"><a href="#cb20-1428" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.75&lt;/td&gt;</span></span>
<span id="cb20-1429"><a href="#cb20-1429" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;1.00&lt;/td&gt;</span></span>
<span id="cb20-1430"><a href="#cb20-1430" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.79&lt;/td&gt;</span></span>
<span id="cb20-1431"><a href="#cb20-1431" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/tr&gt;</span></span>
<span id="cb20-1432"><a href="#cb20-1432" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;tr class="maskanyone-ui border-top"&gt;</span></span>
<span id="cb20-1433"><a href="#cb20-1433" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;MaskAnyoneUI-MediaPipe&lt;/td&gt;</span></span>
<span id="cb20-1434"><a href="#cb20-1434" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td class="best"&gt;0.07&lt;/td&gt;</span></span>
<span id="cb20-1435"><a href="#cb20-1435" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td class="second"&gt;0.41&lt;/td&gt;</span></span>
<span id="cb20-1436"><a href="#cb20-1436" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;1.00&lt;/td&gt;</span></span>
<span id="cb20-1437"><a href="#cb20-1437" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.94&lt;/td&gt;</span></span>
<span id="cb20-1438"><a href="#cb20-1438" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.60&lt;/td&gt;</span></span>
<span id="cb20-1439"><a href="#cb20-1439" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/tr&gt;</span></span>
<span id="cb20-1440"><a href="#cb20-1440" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;tr class="maskanyone-ui border-bottom"&gt;</span></span>
<span id="cb20-1441"><a href="#cb20-1441" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;MaskAnyoneUI-OpenPose&lt;/td&gt;</span></span>
<span id="cb20-1442"><a href="#cb20-1442" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.24&lt;/td&gt;</span></span>
<span id="cb20-1443"><a href="#cb20-1443" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.98&lt;/td&gt;</span></span>
<span id="cb20-1444"><a href="#cb20-1444" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.52&lt;/td&gt;</span></span>
<span id="cb20-1445"><a href="#cb20-1445" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;1.00&lt;/td&gt;</span></span>
<span id="cb20-1446"><a href="#cb20-1446" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.69&lt;/td&gt;</span></span>
<span id="cb20-1447"><a href="#cb20-1447" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/tr&gt;</span></span>
<span id="cb20-1448"><a href="#cb20-1448" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;tr&gt;</span></span>
<span id="cb20-1449"><a href="#cb20-1449" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;Average&lt;/td&gt;</span></span>
<span id="cb20-1450"><a href="#cb20-1450" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.21&lt;/td&gt;</span></span>
<span id="cb20-1451"><a href="#cb20-1451" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.78&lt;/td&gt;</span></span>
<span id="cb20-1452"><a href="#cb20-1452" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.62&lt;/td&gt;</span></span>
<span id="cb20-1453"><a href="#cb20-1453" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;0.90&lt;/td&gt;</span></span>
<span id="cb20-1454"><a href="#cb20-1454" aria-hidden="true" tabindex="-1"></a><span class="in">      &lt;td&gt;/&lt;/td&gt;</span></span>
<span id="cb20-1455"><a href="#cb20-1455" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/tr&gt; </span></span>
<span id="cb20-1456"><a href="#cb20-1456" aria-hidden="true" tabindex="-1"></a><span class="in">  &lt;/tbody&gt;</span></span>
<span id="cb20-1457"><a href="#cb20-1457" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/table&gt;</span></span>
<span id="cb20-1458"><a href="#cb20-1458" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-1459"><a href="#cb20-1459" aria-hidden="true" tabindex="-1"></a>Root mean square error (RMSE) for different pose estimators on videos masked by different hiding strategies.</span>
<span id="cb20-1460"><a href="#cb20-1460" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1461"><a href="#cb20-1461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1462"><a href="#cb20-1462" aria-hidden="true" tabindex="-1"></a>As described in @sec-datasets-masked-video, three videos were masked using four different hiding strategies.</span>
<span id="cb20-1463"><a href="#cb20-1463" aria-hidden="true" tabindex="-1"></a>@tbl-pck-raw-masked and @tbl-rmse-raw-masked present the percentage of correct keypoints (PCK) and root mean square error (RMSE) for various pose estimators on the masked videos, compared to the original videos.</span>
<span id="cb20-1464"><a href="#cb20-1464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1465"><a href="#cb20-1465" aria-hidden="true" tabindex="-1"></a>**Comparison of pose estimators**</span>
<span id="cb20-1466"><a href="#cb20-1466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1467"><a href="#cb20-1467" aria-hidden="true" tabindex="-1"></a>MediaPipePose achieves the highest average PCK of 67% and the lowest average RMSE of 0.38, indicating robustness across all hiding strategies.</span>
<span id="cb20-1468"><a href="#cb20-1468" aria-hidden="true" tabindex="-1"></a>YoloPose also performs well with an average PCK of 57%, particularly on the blurring and contours strategies, where it detects 95% and 93% of the original keypoints, respectively.</span>
<span id="cb20-1469"><a href="#cb20-1469" aria-hidden="true" tabindex="-1"></a>In contrast, OpenPose performs weaker, with an average PCK of only 40% and a high RMSE of 0.67.</span>
<span id="cb20-1470"><a href="#cb20-1470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1471"><a href="#cb20-1471" aria-hidden="true" tabindex="-1"></a>Among the MaskAnyone variants, UI-based models generally outperform API-based ones.</span>
<span id="cb20-1472"><a href="#cb20-1472" aria-hidden="true" tabindex="-1"></a>MaskAnyoneUI-MediaPipe achieves a moderate average PCK of 41% and RMSE of 0.6.</span>
<span id="cb20-1473"><a href="#cb20-1473" aria-hidden="true" tabindex="-1"></a>The API variants perform poorly, with average PCKs around 28% to 29%, indicating that human input improves performance on masked videos.</span>
<span id="cb20-1474"><a href="#cb20-1474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1475"><a href="#cb20-1475" aria-hidden="true" tabindex="-1"></a>However, unlike in other datasets, MaskAnyone UI variants do not improve but rather degrade performance compared to the pure AI models.</span>
<span id="cb20-1476"><a href="#cb20-1476" aria-hidden="true" tabindex="-1"></a>Masking the videos makes keypoint detection more challenging, often lowering the confidence scores assigned by the models.</span>
<span id="cb20-1477"><a href="#cb20-1477" aria-hidden="true" tabindex="-1"></a>Because MaskAnyone applies higher confidence thresholds than the base AI models, many keypoints with reduced confidence may be discarded, leading to more undetected keypoints.</span>
<span id="cb20-1478"><a href="#cb20-1478" aria-hidden="true" tabindex="-1"></a>Additionally, if the first stage of MaskAnyone, where YoloPose detects the person, performs poorly, the second stage, which uses SAM2 <span class="co">[</span><span class="ot">@sam2</span><span class="co">]</span> to segment and crop the person, also suffers.</span>
<span id="cb20-1479"><a href="#cb20-1479" aria-hidden="true" tabindex="-1"></a>This cascades to low-quality input for the final pose estimation stage, degrading overall performance.</span>
<span id="cb20-1480"><a href="#cb20-1480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1481"><a href="#cb20-1481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1482"><a href="#cb20-1482" aria-hidden="true" tabindex="-1"></a>**Comparison of hiding strategies**</span>
<span id="cb20-1483"><a href="#cb20-1483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1484"><a href="#cb20-1484" aria-hidden="true" tabindex="-1"></a>Last, we compare the hiding strategies in terms of balancing privacy and pose estimation performance on masked videos.</span>
<span id="cb20-1485"><a href="#cb20-1485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1486"><a href="#cb20-1486" aria-hidden="true" tabindex="-1"></a>Blurring produced the highest average PCK of 86% across all pose estimators.</span>
<span id="cb20-1487"><a href="#cb20-1487" aria-hidden="true" tabindex="-1"></a>This shows that models can still recognize and track people accurately, even when the image is partially obscured.</span>
<span id="cb20-1488"><a href="#cb20-1488" aria-hidden="true" tabindex="-1"></a>The result highlights that pose estimation does not depend solely on the visibility of individual joints.</span>
<span id="cb20-1489"><a href="#cb20-1489" aria-hidden="true" tabindex="-1"></a>Instead, the models appear to rely on the overall shape and structure of the body, using contextual cues to fill in missing details.</span>
<span id="cb20-1490"><a href="#cb20-1490" aria-hidden="true" tabindex="-1"></a>They likely infer joint positions by applying spatial relationships and body priors learned during training, such as limb proportions, symmetry, and common human poses.</span>
<span id="cb20-1491"><a href="#cb20-1491" aria-hidden="true" tabindex="-1"></a>For instance, even when specific features like eyes or hands are hidden, the surrounding geometry, such as head position, shoulder width, or arm direction, provides sufficient context for pose estimation.</span>
<span id="cb20-1492"><a href="#cb20-1492" aria-hidden="true" tabindex="-1"></a>This suggests that these models depend more on learned pose patterns than on fine-grained pixel information.</span>
<span id="cb20-1493"><a href="#cb20-1493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1494"><a href="#cb20-1494" aria-hidden="true" tabindex="-1"></a>The results for other hiding strategies are more mixed.</span>
<span id="cb20-1495"><a href="#cb20-1495" aria-hidden="true" tabindex="-1"></a>YoloPose achieves an impressive 93% PCK on videos masked with contours, indicating its ability to utilize edge and shape information rather than texture or color.</span>
<span id="cb20-1496"><a href="#cb20-1496" aria-hidden="true" tabindex="-1"></a>In contrast, other pose estimators perform poorly on this strategy.</span>
<span id="cb20-1497"><a href="#cb20-1497" aria-hidden="true" tabindex="-1"></a>Both MaskAnyone API variants detect almost no keypoints, and the remaining models achieve between 36% and 62% PCK.</span>
<span id="cb20-1498"><a href="#cb20-1498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1499"><a href="#cb20-1499" aria-hidden="true" tabindex="-1"></a>For pixelation, only MediaPipePose detects persons reasonably well, with 81% PCK.</span>
<span id="cb20-1500"><a href="#cb20-1500" aria-hidden="true" tabindex="-1"></a>YoloPose, OpenPose, and both MaskAnyone-OpenPose variants detect fewer than 10% of keypoints.</span>
<span id="cb20-1501"><a href="#cb20-1501" aria-hidden="true" tabindex="-1"></a>This suggests that the pixelation level used drastically reduces usable information for pose estimation and that this method is currently unsuitable for such tasks.</span>
<span id="cb20-1502"><a href="#cb20-1502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1503"><a href="#cb20-1503" aria-hidden="true" tabindex="-1"></a>The solid fill hiding strategy is the most challenging, removing nearly all information about the person except the outline.</span>
<span id="cb20-1504"><a href="#cb20-1504" aria-hidden="true" tabindex="-1"></a>As a result, it yields the lowest average PCK of 11%.</span>
<span id="cb20-1505"><a href="#cb20-1505" aria-hidden="true" tabindex="-1"></a>MediaPipePose performs best here but reaches only 34% PCK.</span>
<span id="cb20-1506"><a href="#cb20-1506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1507"><a href="#cb20-1507" aria-hidden="true" tabindex="-1"></a>In conclusion, blurring offers the best trade-off between privacy and pose estimation performance on masked videos.</span>
<span id="cb20-1508"><a href="#cb20-1508" aria-hidden="true" tabindex="-1"></a>While it may not fully de-identify individuals, it retains sufficient information for accurate pose predictions.</span>
<span id="cb20-1509"><a href="#cb20-1509" aria-hidden="true" tabindex="-1"></a>The contour hiding strategy can be considered when stronger privacy is required, though it reduces accuracy for all but YoloPose.</span>
<span id="cb20-1510"><a href="#cb20-1510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1511"><a href="#cb20-1511" aria-hidden="true" tabindex="-1"></a>@tbl-results-raw-masked-rendered presents qualitative results of the best-performing pose estimators on masked videos from the TED talk “Let curiosity lead” and the Tragic Talkers sequence “interactive1_t1-cam06.”</span>
<span id="cb20-1512"><a href="#cb20-1512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1513"><a href="#cb20-1513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1514"><a href="#cb20-1514" aria-hidden="true" tabindex="-1"></a>::: {#tbl-results-raw-masked-rendered}</span>
<span id="cb20-1515"><a href="#cb20-1515" aria-hidden="true" tabindex="-1"></a><span class="in">```{=html}</span></span>
<span id="cb20-1516"><a href="#cb20-1516" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;style&gt;</span></span>
<span id="cb20-1517"><a href="#cb20-1517" aria-hidden="true" tabindex="-1"></a><span class="in">/* Style for raw-masked videos table */</span></span>
<span id="cb20-1518"><a href="#cb20-1518" aria-hidden="true" tabindex="-1"></a><span class="in">.video-table {</span></span>
<span id="cb20-1519"><a href="#cb20-1519" aria-hidden="true" tabindex="-1"></a><span class="in">    border-collapse: collapse; /* Remove spacing between cells */</span></span>
<span id="cb20-1520"><a href="#cb20-1520" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb20-1521"><a href="#cb20-1521" aria-hidden="true" tabindex="-1"></a><span class="in">.video-table td {</span></span>
<span id="cb20-1522"><a href="#cb20-1522" aria-hidden="true" tabindex="-1"></a><span class="in">    text-align: center;</span></span>
<span id="cb20-1523"><a href="#cb20-1523" aria-hidden="true" tabindex="-1"></a><span class="in">    vertical-align: middle;</span></span>
<span id="cb20-1524"><a href="#cb20-1524" aria-hidden="true" tabindex="-1"></a><span class="in">    padding: 4px; /* Minimal padding around cells */</span></span>
<span id="cb20-1525"><a href="#cb20-1525" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb20-1526"><a href="#cb20-1526" aria-hidden="true" tabindex="-1"></a><span class="in">.video-table .video-zoom-wrapper {</span></span>
<span id="cb20-1527"><a href="#cb20-1527" aria-hidden="true" tabindex="-1"></a><span class="in">    display: flex;</span></span>
<span id="cb20-1528"><a href="#cb20-1528" aria-hidden="true" tabindex="-1"></a><span class="in">    justify-content: center;</span></span>
<span id="cb20-1529"><a href="#cb20-1529" aria-hidden="true" tabindex="-1"></a><span class="in">    align-items: center;</span></span>
<span id="cb20-1530"><a href="#cb20-1530" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb20-1531"><a href="#cb20-1531" aria-hidden="true" tabindex="-1"></a><span class="in">.video-table .video-zoom-wrapper video {</span></span>
<span id="cb20-1532"><a href="#cb20-1532" aria-hidden="true" tabindex="-1"></a><span class="in">    height: 200px; /* Fixed height for all videos in this table */</span></span>
<span id="cb20-1533"><a href="#cb20-1533" aria-hidden="true" tabindex="-1"></a><span class="in">    width: auto;   /* Maintain aspect ratio */</span></span>
<span id="cb20-1534"><a href="#cb20-1534" aria-hidden="true" tabindex="-1"></a><span class="in">}</span></span>
<span id="cb20-1535"><a href="#cb20-1535" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/style&gt;</span></span>
<span id="cb20-1536"><a href="#cb20-1536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1537"><a href="#cb20-1537" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;table class="video-table"&gt;</span></span>
<span id="cb20-1538"><a href="#cb20-1538" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;tr&gt;</span></span>
<span id="cb20-1539"><a href="#cb20-1539" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;td&gt;</span></span>
<span id="cb20-1540"><a href="#cb20-1540" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-zoom-wrapper"&gt;</span></span>
<span id="cb20-1541"><a href="#cb20-1541" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;video class="video-zoom ted-raw-masked-sync" autoplay muted playsinline&gt;</span></span>
<span id="cb20-1542"><a href="#cb20-1542" aria-hidden="true" tabindex="-1"></a><span class="in">                    &lt;source src="videos/raw-masked/Blurring-TED-curiosity-chunk_YoloPose.mp4" type="video/mp4"&gt;</span></span>
<span id="cb20-1543"><a href="#cb20-1543" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;/video&gt;</span></span>
<span id="cb20-1544"><a href="#cb20-1544" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;!-- &lt;div class="video-caption"&gt;YoloPose&lt;/div&gt; --&gt;</span></span>
<span id="cb20-1545"><a href="#cb20-1545" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;/div&gt;</span></span>
<span id="cb20-1546"><a href="#cb20-1546" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;/td&gt;</span></span>
<span id="cb20-1547"><a href="#cb20-1547" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;td&gt;</span></span>
<span id="cb20-1548"><a href="#cb20-1548" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-zoom-wrapper"&gt;</span></span>
<span id="cb20-1549"><a href="#cb20-1549" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;video class="video-zoom tt-raw-masked-sync" autoplay muted playsinline&gt;</span></span>
<span id="cb20-1550"><a href="#cb20-1550" aria-hidden="true" tabindex="-1"></a><span class="in">                    &lt;source src="videos/raw-masked/Blurring-TT-interactive1_t1-cam06_YoloPose.mp4" type="video/mp4"&gt;</span></span>
<span id="cb20-1551"><a href="#cb20-1551" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;/video&gt;</span></span>
<span id="cb20-1552"><a href="#cb20-1552" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;!-- &lt;div class="video-caption"&gt;YoloPose&lt;/div&gt; --&gt;</span></span>
<span id="cb20-1553"><a href="#cb20-1553" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;/div&gt;</span></span>
<span id="cb20-1554"><a href="#cb20-1554" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;/td&gt;</span></span>
<span id="cb20-1555"><a href="#cb20-1555" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/tr&gt;</span></span>
<span id="cb20-1556"><a href="#cb20-1556" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;tr&gt;</span></span>
<span id="cb20-1557"><a href="#cb20-1557" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;td&gt;</span></span>
<span id="cb20-1558"><a href="#cb20-1558" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-zoom-wrapper"&gt;</span></span>
<span id="cb20-1559"><a href="#cb20-1559" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;video class="video-zoom ted-raw-masked-sync" autoplay muted playsinline&gt;</span></span>
<span id="cb20-1560"><a href="#cb20-1560" aria-hidden="true" tabindex="-1"></a><span class="in">                    &lt;source src="videos/raw-masked/Pixelation-TED-curiosity-chunk_MaskAnyoneUI-MediaPipe.mp4" type="video/mp4"&gt;</span></span>
<span id="cb20-1561"><a href="#cb20-1561" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;/video&gt;</span></span>
<span id="cb20-1562"><a href="#cb20-1562" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;!-- &lt;div class="video-caption"&gt;MaskAnyoneUI-MediaPipe&lt;/div&gt; --&gt;</span></span>
<span id="cb20-1563"><a href="#cb20-1563" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;/div&gt;</span></span>
<span id="cb20-1564"><a href="#cb20-1564" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;/td&gt;</span></span>
<span id="cb20-1565"><a href="#cb20-1565" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;td&gt;</span></span>
<span id="cb20-1566"><a href="#cb20-1566" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-zoom-wrapper"&gt;</span></span>
<span id="cb20-1567"><a href="#cb20-1567" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;video class="video-zoom tt-raw-masked-sync" autoplay muted playsinline&gt;</span></span>
<span id="cb20-1568"><a href="#cb20-1568" aria-hidden="true" tabindex="-1"></a><span class="in">                    &lt;source src="videos/raw-masked/Pixelation-TT-interactive1_t1-cam06_MediaPipePose.mp4" type="video/mp4"&gt;</span></span>
<span id="cb20-1569"><a href="#cb20-1569" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;/video&gt;</span></span>
<span id="cb20-1570"><a href="#cb20-1570" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;!-- &lt;div class="video-caption"&gt;MediaPipePose&lt;/div&gt; --&gt;</span></span>
<span id="cb20-1571"><a href="#cb20-1571" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;/div&gt;</span></span>
<span id="cb20-1572"><a href="#cb20-1572" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;/td&gt;</span></span>
<span id="cb20-1573"><a href="#cb20-1573" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/tr&gt;</span></span>
<span id="cb20-1574"><a href="#cb20-1574" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;tr&gt;</span></span>
<span id="cb20-1575"><a href="#cb20-1575" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;td&gt;</span></span>
<span id="cb20-1576"><a href="#cb20-1576" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-zoom-wrapper"&gt;</span></span>
<span id="cb20-1577"><a href="#cb20-1577" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;video class="video-zoom ted-raw-masked-sync" autoplay muted playsinline&gt;</span></span>
<span id="cb20-1578"><a href="#cb20-1578" aria-hidden="true" tabindex="-1"></a><span class="in">                    &lt;source src="videos/raw-masked/Contours-TED-curiosity-chunk_YoloPose.mp4" type="video/mp4"&gt;</span></span>
<span id="cb20-1579"><a href="#cb20-1579" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;/video&gt;</span></span>
<span id="cb20-1580"><a href="#cb20-1580" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;!-- &lt;div class="video-caption"&gt;YoloPose&lt;/div&gt; --&gt;</span></span>
<span id="cb20-1581"><a href="#cb20-1581" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;/div&gt;</span></span>
<span id="cb20-1582"><a href="#cb20-1582" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;/td&gt;</span></span>
<span id="cb20-1583"><a href="#cb20-1583" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;td&gt;</span></span>
<span id="cb20-1584"><a href="#cb20-1584" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-zoom-wrapper"&gt;</span></span>
<span id="cb20-1585"><a href="#cb20-1585" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;video class="video-zoom tt-raw-masked-sync" autoplay muted playsinline&gt;</span></span>
<span id="cb20-1586"><a href="#cb20-1586" aria-hidden="true" tabindex="-1"></a><span class="in">                    &lt;source src="videos/raw-masked/Contours-TT-interactive1_t1-cam06_YoloPose.mp4" type="video/mp4"&gt;</span></span>
<span id="cb20-1587"><a href="#cb20-1587" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;/video&gt;</span></span>
<span id="cb20-1588"><a href="#cb20-1588" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;!-- &lt;div class="video-caption"&gt;YoloPose&lt;/div&gt; --&gt;</span></span>
<span id="cb20-1589"><a href="#cb20-1589" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;/div&gt;</span></span>
<span id="cb20-1590"><a href="#cb20-1590" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;/td&gt;</span></span>
<span id="cb20-1591"><a href="#cb20-1591" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/tr&gt;</span></span>
<span id="cb20-1592"><a href="#cb20-1592" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;tr&gt;</span></span>
<span id="cb20-1593"><a href="#cb20-1593" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;td&gt;</span></span>
<span id="cb20-1594"><a href="#cb20-1594" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-zoom-wrapper"&gt;</span></span>
<span id="cb20-1595"><a href="#cb20-1595" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;video class="video-zoom ted-raw-masked-sync" autoplay muted playsinline&gt;</span></span>
<span id="cb20-1596"><a href="#cb20-1596" aria-hidden="true" tabindex="-1"></a><span class="in">                    &lt;source src="videos/raw-masked/SolidFill-TED-curiosity-chunk_YoloPose.mp4" type="video/mp4"&gt;</span></span>
<span id="cb20-1597"><a href="#cb20-1597" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;/video&gt;</span></span>
<span id="cb20-1598"><a href="#cb20-1598" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;!-- &lt;div class="video-caption"&gt;YoloPose&lt;/div&gt; --&gt;</span></span>
<span id="cb20-1599"><a href="#cb20-1599" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;/div&gt;</span></span>
<span id="cb20-1600"><a href="#cb20-1600" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;/td&gt;</span></span>
<span id="cb20-1601"><a href="#cb20-1601" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;td&gt;</span></span>
<span id="cb20-1602"><a href="#cb20-1602" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;div class="video-zoom-wrapper"&gt;</span></span>
<span id="cb20-1603"><a href="#cb20-1603" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;video class="video-zoom tt-raw-masked-sync" autoplay muted playsinline&gt;</span></span>
<span id="cb20-1604"><a href="#cb20-1604" aria-hidden="true" tabindex="-1"></a><span class="in">                    &lt;source src="videos/raw-masked/SolidFill-TT-interactive1_t1-cam06_MaskAnyoneUI-MediaPipe.mp4" type="video/mp4"&gt;</span></span>
<span id="cb20-1605"><a href="#cb20-1605" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;/video&gt;</span></span>
<span id="cb20-1606"><a href="#cb20-1606" aria-hidden="true" tabindex="-1"></a><span class="in">                &lt;!-- &lt;div class="video-caption"&gt;MaskAnyoneUI-MediaPipe&lt;/div&gt; --&gt;</span></span>
<span id="cb20-1607"><a href="#cb20-1607" aria-hidden="true" tabindex="-1"></a><span class="in">            &lt;/div&gt;</span></span>
<span id="cb20-1608"><a href="#cb20-1608" aria-hidden="true" tabindex="-1"></a><span class="in">        &lt;/td&gt;</span></span>
<span id="cb20-1609"><a href="#cb20-1609" aria-hidden="true" tabindex="-1"></a><span class="in">    &lt;/tr&gt;</span></span>
<span id="cb20-1610"><a href="#cb20-1610" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/table&gt;</span></span>
<span id="cb20-1611"><a href="#cb20-1611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1612"><a href="#cb20-1612" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;script&gt;</span></span>
<span id="cb20-1613"><a href="#cb20-1613" aria-hidden="true" tabindex="-1"></a><span class="in">document.addEventListener('DOMContentLoaded', function() {</span></span>
<span id="cb20-1614"><a href="#cb20-1614" aria-hidden="true" tabindex="-1"></a><span class="in">    const videos = document.querySelectorAll('.ted-raw-masked-sync');</span></span>
<span id="cb20-1615"><a href="#cb20-1615" aria-hidden="true" tabindex="-1"></a><span class="in">    let endedCount = 0;</span></span>
<span id="cb20-1616"><a href="#cb20-1616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1617"><a href="#cb20-1617" aria-hidden="true" tabindex="-1"></a><span class="in">    function restartAllVideos() {</span></span>
<span id="cb20-1618"><a href="#cb20-1618" aria-hidden="true" tabindex="-1"></a><span class="in">        videos.forEach(video =&gt; {</span></span>
<span id="cb20-1619"><a href="#cb20-1619" aria-hidden="true" tabindex="-1"></a><span class="in">            video.currentTime = 0;</span></span>
<span id="cb20-1620"><a href="#cb20-1620" aria-hidden="true" tabindex="-1"></a><span class="in">            video.play();</span></span>
<span id="cb20-1621"><a href="#cb20-1621" aria-hidden="true" tabindex="-1"></a><span class="in">        });</span></span>
<span id="cb20-1622"><a href="#cb20-1622" aria-hidden="true" tabindex="-1"></a><span class="in">        endedCount = 0;</span></span>
<span id="cb20-1623"><a href="#cb20-1623" aria-hidden="true" tabindex="-1"></a><span class="in">    }</span></span>
<span id="cb20-1624"><a href="#cb20-1624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1625"><a href="#cb20-1625" aria-hidden="true" tabindex="-1"></a><span class="in">    videos.forEach(video =&gt; {</span></span>
<span id="cb20-1626"><a href="#cb20-1626" aria-hidden="true" tabindex="-1"></a><span class="in">        video.addEventListener('ended', () =&gt; {</span></span>
<span id="cb20-1627"><a href="#cb20-1627" aria-hidden="true" tabindex="-1"></a><span class="in">            endedCount++;</span></span>
<span id="cb20-1628"><a href="#cb20-1628" aria-hidden="true" tabindex="-1"></a><span class="in">            if (endedCount === videos.length) {</span></span>
<span id="cb20-1629"><a href="#cb20-1629" aria-hidden="true" tabindex="-1"></a><span class="in">                restartAllVideos();</span></span>
<span id="cb20-1630"><a href="#cb20-1630" aria-hidden="true" tabindex="-1"></a><span class="in">            }</span></span>
<span id="cb20-1631"><a href="#cb20-1631" aria-hidden="true" tabindex="-1"></a><span class="in">        });</span></span>
<span id="cb20-1632"><a href="#cb20-1632" aria-hidden="true" tabindex="-1"></a><span class="in">    });</span></span>
<span id="cb20-1633"><a href="#cb20-1633" aria-hidden="true" tabindex="-1"></a><span class="in">});</span></span>
<span id="cb20-1634"><a href="#cb20-1634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1635"><a href="#cb20-1635" aria-hidden="true" tabindex="-1"></a><span class="in">document.addEventListener('DOMContentLoaded', function() {</span></span>
<span id="cb20-1636"><a href="#cb20-1636" aria-hidden="true" tabindex="-1"></a><span class="in">    const videos = document.querySelectorAll('.tt-raw-masked-sync');</span></span>
<span id="cb20-1637"><a href="#cb20-1637" aria-hidden="true" tabindex="-1"></a><span class="in">    let endedCount = 0;</span></span>
<span id="cb20-1638"><a href="#cb20-1638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1639"><a href="#cb20-1639" aria-hidden="true" tabindex="-1"></a><span class="in">    function restartAllVideos() {</span></span>
<span id="cb20-1640"><a href="#cb20-1640" aria-hidden="true" tabindex="-1"></a><span class="in">        videos.forEach(video =&gt; {</span></span>
<span id="cb20-1641"><a href="#cb20-1641" aria-hidden="true" tabindex="-1"></a><span class="in">            video.currentTime = 0;</span></span>
<span id="cb20-1642"><a href="#cb20-1642" aria-hidden="true" tabindex="-1"></a><span class="in">            video.play();</span></span>
<span id="cb20-1643"><a href="#cb20-1643" aria-hidden="true" tabindex="-1"></a><span class="in">        });</span></span>
<span id="cb20-1644"><a href="#cb20-1644" aria-hidden="true" tabindex="-1"></a><span class="in">        endedCount = 0;</span></span>
<span id="cb20-1645"><a href="#cb20-1645" aria-hidden="true" tabindex="-1"></a><span class="in">    }</span></span>
<span id="cb20-1646"><a href="#cb20-1646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1647"><a href="#cb20-1647" aria-hidden="true" tabindex="-1"></a><span class="in">    videos.forEach(video =&gt; {</span></span>
<span id="cb20-1648"><a href="#cb20-1648" aria-hidden="true" tabindex="-1"></a><span class="in">        video.addEventListener('ended', () =&gt; {</span></span>
<span id="cb20-1649"><a href="#cb20-1649" aria-hidden="true" tabindex="-1"></a><span class="in">            endedCount++;</span></span>
<span id="cb20-1650"><a href="#cb20-1650" aria-hidden="true" tabindex="-1"></a><span class="in">            if (endedCount === videos.length) {</span></span>
<span id="cb20-1651"><a href="#cb20-1651" aria-hidden="true" tabindex="-1"></a><span class="in">                restartAllVideos();</span></span>
<span id="cb20-1652"><a href="#cb20-1652" aria-hidden="true" tabindex="-1"></a><span class="in">            }</span></span>
<span id="cb20-1653"><a href="#cb20-1653" aria-hidden="true" tabindex="-1"></a><span class="in">        });</span></span>
<span id="cb20-1654"><a href="#cb20-1654" aria-hidden="true" tabindex="-1"></a><span class="in">    });</span></span>
<span id="cb20-1655"><a href="#cb20-1655" aria-hidden="true" tabindex="-1"></a><span class="in">});</span></span>
<span id="cb20-1656"><a href="#cb20-1656" aria-hidden="true" tabindex="-1"></a><span class="in">&lt;/script&gt;</span></span>
<span id="cb20-1657"><a href="#cb20-1657" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-1658"><a href="#cb20-1658" aria-hidden="true" tabindex="-1"></a>The best-performing pose estimators on the masked videos are shown for the TED sequence “Let curiosity lead” and the Tragic Talkers sequence “interactive1_t1-cam06”.</span>
<span id="cb20-1659"><a href="#cb20-1659" aria-hidden="true" tabindex="-1"></a>Each row corresponds to a different hiding strategy, in the order: Blurring, Pixelation, Contours, and Solid Fill.</span>
<span id="cb20-1660"><a href="#cb20-1660" aria-hidden="true" tabindex="-1"></a>For the TED sequence (first column), the pose estimators are YoloPose, MaskAnyoneUI-MediaPipe, YoloPose, and YoloPose.</span>
<span id="cb20-1661"><a href="#cb20-1661" aria-hidden="true" tabindex="-1"></a>For the Tragic Talkers sequence (second column), the pose estimators are YoloPose, MediaPipePose, YoloPose, and MaskAnyoneUI-MediaPipe.</span>
<span id="cb20-1662"><a href="#cb20-1662" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb20-1663"><a href="#cb20-1663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1664"><a href="#cb20-1664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1665"><a href="#cb20-1665" aria-hidden="true" tabindex="-1"></a><span class="fu"># Future Work &amp; Limitations {#sec-future-work}</span></span>
<span id="cb20-1666"><a href="#cb20-1666" aria-hidden="true" tabindex="-1"></a>In this section, we outline future work and limitations of MaskBench and MaskAnyone.</span>
<span id="cb20-1667"><a href="#cb20-1667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1668"><a href="#cb20-1668" aria-hidden="true" tabindex="-1"></a><span class="fu">## MaskAnyone Limitations</span></span>
<span id="cb20-1669"><a href="#cb20-1669" aria-hidden="true" tabindex="-1"></a>Both MaskAnyone-API and MaskAnyone-UI have introduced improvements in detection accuracy and user interaction. However, they still exhibit notable limitations in complex and long video scenarios such as TED talks, which often feature background noise, occlusions, scene transitions, and unrelated visual elements. </span>
<span id="cb20-1670"><a href="#cb20-1670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1671"><a href="#cb20-1671" aria-hidden="true" tabindex="-1"></a>The first and most important challenge of MaskAnyone-API and MaskAnyone-UI is that it does not support long videos, such as TED talks. It requires manual chunking of the video; however, chunking introduces another issue. After chunking, some videos start with frames where no human is visible. When a video begins with such frames, MaskAnyone falsely predicts objects in the scene as humans and then continues to the end of the video with that false prediction. For better detection and pose estimation, we would need to remove the start of chunks without visible humans, but this results in a loss of content.</span>
<span id="cb20-1672"><a href="#cb20-1672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1673"><a href="#cb20-1673" aria-hidden="true" tabindex="-1"></a>Another major challenge lies in handling abrupt scene changes or shifts in camera perspective. For example, when a video cuts from a close-up to a full-body shot (or vice versa), MaskAnyone fails to maintain consistent detection and tracking, resulting in missed detections or inaccurate pose estimation. MaskAnyone-UI addresses this issue through a human-in-the-loop mechanism that allows users to manually select key frames, ensuring more reliable tracking throughout the video.</span>
<span id="cb20-1674"><a href="#cb20-1674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1675"><a href="#cb20-1675" aria-hidden="true" tabindex="-1"></a>Another issue, observed primarily in MaskAnyone-API, is the double overlaying of pose skeletons on the same person. This results in duplicate or misaligned pose renderings. This problem has not been observed in the UI version, as manual frame selection allows users to avoid such misdetections.</span>
<span id="cb20-1676"><a href="#cb20-1676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1677"><a href="#cb20-1677" aria-hidden="true" tabindex="-1"></a>Finally, false positive predictions remain a common problem in MaskAnyone-API. Not only in scenes where no human is present, where the system interprets non-human objects such as buildings, cigarettes, or images as people, but it also occurs in scenarios where a human is actually present, yet MaskAnyone-API segments the background instead of the person. False positive predictions occur in MaskAnyone-UI as well, but only on rare occasions. </span>
<span id="cb20-1678"><a href="#cb20-1678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1679"><a href="#cb20-1679" aria-hidden="true" tabindex="-1"></a><span class="fu">## MaskBench Outlook</span></span>
<span id="cb20-1680"><a href="#cb20-1680" aria-hidden="true" tabindex="-1"></a>With our promising results, we are laying the groundwork for a more versatile benchmarking framework for pose estimation on masked videos.</span>
<span id="cb20-1681"><a href="#cb20-1681" aria-hidden="true" tabindex="-1"></a>There are several directions in which we plan to extend our work.</span>
<span id="cb20-1682"><a href="#cb20-1682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1683"><a href="#cb20-1683" aria-hidden="true" tabindex="-1"></a><span class="fu">### Pipelining {#sec-future-work-pipelining}</span></span>
<span id="cb20-1684"><a href="#cb20-1684" aria-hidden="true" tabindex="-1"></a>Currently, MaskBench supports a single workflow: running inference on a set of videos, evaluating results using metrics, visualizing them, and rendering the videos with overlaid poses.</span>
<span id="cb20-1685"><a href="#cb20-1685" aria-hidden="true" tabindex="-1"></a>As demonstrated with the masked video dataset experiment, there are many more potential workflows that could be integrated.</span>
<span id="cb20-1686"><a href="#cb20-1686" aria-hidden="true" tabindex="-1"></a>We aim to introduce an extensible and customizable pipeline class to MaskBench.</span>
<span id="cb20-1687"><a href="#cb20-1687" aria-hidden="true" tabindex="-1"></a>Each pipeline would define a specific workflow (i.e., our current workflow, the masked video dataset workflow, or other, yet to be defined workflows) by chaining MaskBench components in a particular order, reusing existing modules and adding new ones where necessary.</span>
<span id="cb20-1688"><a href="#cb20-1688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1689"><a href="#cb20-1689" aria-hidden="true" tabindex="-1"></a>For example, the masked video dataset workflow could be structured as follows:</span>
<span id="cb20-1690"><a href="#cb20-1690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1691"><a href="#cb20-1691" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Run inference on the raw videos with all pose estimators.</span>
<span id="cb20-1692"><a href="#cb20-1692" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Evaluate results with metrics.</span>
<span id="cb20-1693"><a href="#cb20-1693" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Visualize pose estimator performance on raw videos using plots or tables.</span>
<span id="cb20-1694"><a href="#cb20-1694" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>Render the raw videos with overlaid poses.</span>
<span id="cb20-1695"><a href="#cb20-1695" aria-hidden="true" tabindex="-1"></a><span class="ss">5.  </span>Reuse the SAM2 masks from MaskAnyone to apply different hiding strategies to the videos. Masking parameters could be adjusted by the user to explore not only different strategies but also varying degrees of masking, helping determine the optimal balance between privacy and performance.</span>
<span id="cb20-1696"><a href="#cb20-1696" aria-hidden="true" tabindex="-1"></a><span class="ss">6.  </span>For each pose estimator, run inference on all videos across all hiding strategies.</span>
<span id="cb20-1697"><a href="#cb20-1697" aria-hidden="true" tabindex="-1"></a><span class="ss">7.  </span>Evaluate results with metrics.</span>
<span id="cb20-1698"><a href="#cb20-1698" aria-hidden="true" tabindex="-1"></a><span class="ss">8.  </span>Visualize performance on masked videos using plots or tables.</span>
<span id="cb20-1699"><a href="#cb20-1699" aria-hidden="true" tabindex="-1"></a><span class="ss">9.  </span>Render the masked videos with overlaid poses.</span>
<span id="cb20-1700"><a href="#cb20-1700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1701"><a href="#cb20-1701" aria-hidden="true" tabindex="-1"></a><span class="fu">### Evaluation of downstream tasks {#sec-future-work-downstream-tasks}</span></span>
<span id="cb20-1702"><a href="#cb20-1702" aria-hidden="true" tabindex="-1"></a>Estimating a person’s pose can serve as a preliminary step for many downstream tasks, such as gesture recognition <span class="co">[</span><span class="ot">@gesture-recognition; @gesture-recognition-3d</span><span class="co">]</span>, 3D human reconstruction <span class="co">[</span><span class="ot">@sith; @pose2mesh</span><span class="co">]</span>, and action classification.</span>
<span id="cb20-1703"><a href="#cb20-1703" aria-hidden="true" tabindex="-1"></a>MaskBench could be extended to evaluate how different upstream pose estimators affect performance on these downstream tasks for both raw and masked videos.</span>
<span id="cb20-1704"><a href="#cb20-1704" aria-hidden="true" tabindex="-1"></a>This extension would give researchers practical guidance on which pose estimator to choose for a given downstream application.</span>
<span id="cb20-1705"><a href="#cb20-1705" aria-hidden="true" tabindex="-1"></a>Furthermore, researchers could use MaskBench to mask sensitive datasets and publish the masked videos together with pose outputs derived from the original raw videos.</span>
<span id="cb20-1706"><a href="#cb20-1706" aria-hidden="true" tabindex="-1"></a>Other researchers could then use the masked videos plus the provided pose outputs as input for downstream tasks without accessing the original raw data.</span>
<span id="cb20-1707"><a href="#cb20-1707" aria-hidden="true" tabindex="-1"></a>MaskBench should include an evaluation framework that quantifies the potential performance loss in downstream tasks when using masked videos or alternative upstream estimators.</span>
<span id="cb20-1708"><a href="#cb20-1708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1709"><a href="#cb20-1709" aria-hidden="true" tabindex="-1"></a><span class="fu">### User interface</span></span>
<span id="cb20-1710"><a href="#cb20-1710" aria-hidden="true" tabindex="-1"></a>Adding a web-based user interface to MaskBench would make the framework significantly more accessible.</span>
<span id="cb20-1711"><a href="#cb20-1711" aria-hidden="true" tabindex="-1"></a>At present, running MaskBench requires technical expertise, such as working with Docker containers, setting environment variables, and editing configuration files.</span>
<span id="cb20-1712"><a href="#cb20-1712" aria-hidden="true" tabindex="-1"></a>A dedicated interface could replace these steps with an intuitive, visual workflow for configuring and running pipelines.</span>
<span id="cb20-1713"><a href="#cb20-1713" aria-hidden="true" tabindex="-1"></a>It could also provide built-in visualization panels for metrics, interactive plots, and side-by-side video comparisons, making it easier to explore results without leaving the application.</span>
<span id="cb20-1714"><a href="#cb20-1714" aria-hidden="true" tabindex="-1"></a>Ultimately, this would lower the entry barrier for non-technical users while speeding up experimentation for advanced users.</span>
<span id="cb20-1715"><a href="#cb20-1715" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1716"><a href="#cb20-1716" aria-hidden="true" tabindex="-1"></a><span class="fu">### Additional improvements</span></span>
<span id="cb20-1717"><a href="#cb20-1717" aria-hidden="true" tabindex="-1"></a>In addition to the major extensions outlined above, several smaller improvements could further enhance MaskBench in the future:</span>
<span id="cb20-1718"><a href="#cb20-1718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1719"><a href="#cb20-1719" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Expanded normalization options for the Euclidean distance metric**.</span>
<span id="cb20-1720"><a href="#cb20-1720" aria-hidden="true" tabindex="-1"></a>Currently, normalization is only possible using bounding boxes, but can be extended to support head and torso normalization as described in @sec-metrics-euclidean-distance.</span>
<span id="cb20-1721"><a href="#cb20-1721" aria-hidden="true" tabindex="-1"></a>This requires identifying the relevant head and torso keypoints while keeping the system flexible enough to support multiple keypoint formats beyond COCO.</span>
<span id="cb20-1722"><a href="#cb20-1722" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Integrated logging system**.</span>
<span id="cb20-1723"><a href="#cb20-1723" aria-hidden="true" tabindex="-1"></a>A built-in logger could provide cleaner, more structured terminal output.</span>
<span id="cb20-1724"><a href="#cb20-1724" aria-hidden="true" tabindex="-1"></a>For debugging, an option to display all logs from the underlying Docker containers would make error tracing during development much easier.</span>
<span id="cb20-1725"><a href="#cb20-1725" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Support for face and hand keypoints**.</span>
<span id="cb20-1726"><a href="#cb20-1726" aria-hidden="true" tabindex="-1"></a>This would enable evaluation of a broader set of downstream tasks where fine-grained keypoint data is important.</span>
<span id="cb20-1727"><a href="#cb20-1727" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Additional ground-truth–independent metrics and plots**.</span>
<span id="cb20-1728"><a href="#cb20-1728" aria-hidden="true" tabindex="-1"></a>Beyond velocity, acceleration, and jerk, metrics could assess the physical plausibility of a pose given human body constraints.</span>
<span id="cb20-1729"><a href="#cb20-1729" aria-hidden="true" tabindex="-1"></a>This would shift part of the evaluation focus from pure numerical quality to biomechanical realism.</span>
<span id="cb20-1730"><a href="#cb20-1730" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**3D pose estimation support** as a long-term goal.</span>
<span id="cb20-1731"><a href="#cb20-1731" aria-hidden="true" tabindex="-1"></a>This could include both evaluating 3D models directly and projecting 3D ground truth keypoints onto the 2D image plane using camera calibration data.</span>
<span id="cb20-1732"><a href="#cb20-1732" aria-hidden="true" tabindex="-1"></a>Leveraging marker-based motion capture datasets—such as BioCV from the University of Bath <span class="co">[</span><span class="ot">@bio-cv</span><span class="co">]</span>—would allow for more precise real-world benchmarking than 2D pseudo-ground truth data.</span>
<span id="cb20-1733"><a href="#cb20-1733" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Integrate Samurai [@samurai] into MaskAnyone**. This would allow for more stable tracking of persons over time by using adapted memory modules that more consistently maintain the identity of a person across frames.</span>
<span id="cb20-1734"><a href="#cb20-1734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1735"><a href="#cb20-1735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1736"><a href="#cb20-1736" aria-hidden="true" tabindex="-1"></a><span class="fu"># Conclusion {#sec-conclusion}</span></span>
<span id="cb20-1737"><a href="#cb20-1737" aria-hidden="true" tabindex="-1"></a>This work introduced MaskBench, a modular and extensible benchmarking framework for evaluating pose estimation models under diverse conditions, including privacy-preserving masking strategies.</span>
<span id="cb20-1738"><a href="#cb20-1738" aria-hidden="true" tabindex="-1"></a>We evaluated four datasets of increasing complexity, including real-world TED talk recordings to examine how models perform in unconstrained, natural scenarios rather than under controlled laboratory conditions.</span>
<span id="cb20-1739"><a href="#cb20-1739" aria-hidden="true" tabindex="-1"></a>The study includes popular pose estimators such as YoloPose <span class="co">[</span><span class="ot">@yolo</span><span class="co">]</span>, MediaPipe <span class="co">[</span><span class="ot">@mediapipe</span><span class="co">]</span>, and OpenPose <span class="co">[</span><span class="ot">@openpose-1</span><span class="co">]</span>, alongside the mixture-of-expert-model pipeline MaskAnyone <span class="co">[</span><span class="ot">@schilling2023maskanyone</span><span class="co">]</span>, to assess their performance across these varied settings.</span>
<span id="cb20-1740"><a href="#cb20-1740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1741"><a href="#cb20-1741" aria-hidden="true" tabindex="-1"></a>Our quantitative evaluation, using acceleration and jerk metrics to measure temporal stability, showed that the MaskAnyone pipeline, particularly the human-in-the-loop MediaPipe variant, substantially improves stability by reducing acceleration and jerk compared to standard models.</span>
<span id="cb20-1742"><a href="#cb20-1742" aria-hidden="true" tabindex="-1"></a>YoloPose was the most robust standalone estimator, while MediaPipePose consistently exhibited the highest instability.</span>
<span id="cb20-1743"><a href="#cb20-1743" aria-hidden="true" tabindex="-1"></a>Visual inspection of the output poses confirmed these findings, with noticeably smoother and more consistent motion in cases where the metrics indicated high stability.</span>
<span id="cb20-1744"><a href="#cb20-1744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1745"><a href="#cb20-1745" aria-hidden="true" tabindex="-1"></a>Our small study on masked videos revealed that blurring offers the best trade-off between privacy and accuracy, maintaining high PCK values across models, whereas pixelation and solid fills significantly degraded performance.</span>
<span id="cb20-1746"><a href="#cb20-1746" aria-hidden="true" tabindex="-1"></a>Model-specific responses to masking strategies highlighted that pose estimation often relies more on overall body structure than on pixel-level detail.</span>
<span id="cb20-1747"><a href="#cb20-1747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1748"><a href="#cb20-1748" aria-hidden="true" tabindex="-1"></a>While results are promising, limitations remain, including reliance on pseudo-ground truth in some datasets and the preliminary implementation of masked-video workflows.</span>
<span id="cb20-1749"><a href="#cb20-1749" aria-hidden="true" tabindex="-1"></a>Future work will focus on extending MaskBench with flexible pipelining, downstream task evaluation, and user-friendly interfaces, enabling systematic exploration of how privacy-preserving transformations affect pose estimation and subsequent applications.</span>
<span id="cb20-1750"><a href="#cb20-1750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1751"><a href="#cb20-1751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-1752"><a href="#cb20-1752" aria-hidden="true" tabindex="-1"></a><span class="fu"># References</span></span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>