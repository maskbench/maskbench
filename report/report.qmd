---
title: "MaskBench - A Comprehensive Benchmark Framework for Video De-Identification"
date: "2025 08 08"
author:
    - name: Tim Riedel
      affiliation: Hasso Plattner Institute, University of Potsdam, Germany
    - name: Zainab Zafari
      affiliation: Hasso Plattner Institute, University of Potsdam, Germany
    - name: Sharjeel Shaik
      affiliation: University of Potsdam, Germany
    - name: Babajide Alamu Owoyele
      affiliation: Hasso Plattner Institute, University of Potsdam, Germany
    - name: Wim Pouw
      affiliation: Tilburg University, Netherlands
contact:
    - name: Tim Riedel
      email: tim.riedel@student.hpi.de
    - name: Zainab Zafari
      email: zainab.zafari@student.hpi.de
    - name: Babajide Alamu Owoyele
      email: babajide.owoyele@student.hpi.de
    - name: Wim Pouw
      email: w.pouw@tilburguniversity.edu
bibliography: dependencies/refs.bib
css: dependencies/styles.css
theme: journal
format:
    html:
        toc: true
        toc-location: left
        toc-title: "Contents"
        toc-depth: 3
        number-sections: true
        code-fold: true
        code-tools: true
filters:
    - include-code-files
# engine: knitr
jupyter: python3
---

# Abstract / Overview {#sec-abstract}
(Tim - 6.)



# Getting Started {#sec-installation}
(Zainab - 3.)

## Installation / Setup {#sec-installation-setup}

## Usage {#sec-installation-usage}

## Configuration of Experiments {#sec-installation-configuration}




# Introduction {#sec-introduction}
(Zainab - 5.)



# Related Work {#sec-related-work}
(Zainab - 4.)



# MaskBench Architecture {#sec-architecture}
Figure of architecture & workflow (Zainab - 2.)

The general workflow of MaskBench is to first load the dataset, pose estimators and evaluation metrics.
The application creates a checkpoint folder in the specified `/output` directory, named after the dataset and a timestamp (e.g. `/output/TedTalks-20250724-121127`).
Thereafter, inference is performed on all videos of the dataset using the pose estimators specified in the configuration file. 
A `poses` folder is created within the checkpoint, with a subfolder for each pose estimator and a single json file for each video.
Thereafter, the application evaluates all the specified metrics and visualizes them in plots, which are stored in the `plots` folder in the checkpoint.
Lastly, for each video, the application creates a multiple rendered video, one for each pose estimator, which are stored in the `renderings` folder in the checkpoint.

Each component of MaskBench is implemented in a modular way, so that it can be easily extended and modified, which we will discuss in the following sections.

## Dataset {#sec-architecture-dataset}
The dataset provides the video data for the pose estimation and ground truth data for the evaluation, if available.
When adding a new dataset, the user needs to create a new class that inherits from the `Dataset` class and overwrite the `_load_samples` method, which creates one `VideoSample` object for each video in the dataset.
If the dataset provides ground truth data, the user additionally needs to overwrite the `get_gt_pose_results` and `get_gt_keypoint_pairs` methods.
For each video, the `get_gt_pose_results` method should return a `VideoPoseResult` object.
The `get_gt_keypoint_pairs` method is used for rendering the ground truth keypoints and contains a list of tuples, where each tuple contains the index of two keypoints to be connected in the rendered video. We provide default keypoint pairs for "YOLO", "Mediapipe" and various implementations of "OpenPose" models in the file `keypoint_pairs.py`.

Below is the code implementation for the abstract dataset class, for the very simple TED talks dataset (without ground truth) and the more complicated TragicTalkers dataset (with ground truth data).

::: {.callout-note collapse="true"}
## Dataset Class
```{.python include="../src/datasets/dataset.py" code-line-numbers="true" filename="src/datasets/dataset.py"}
```
:::

::: {.callout-note collapse="true"}
## TED Talks Dataset
```{.python include="../src/datasets/ted_dataset.py" code-line-numbers="true" filename="src/datasets/ted_dataset.py"}
```
:::

::: {.callout-note collapse="true"}
## Tragic Talkers Dataset
```{.python include="../src/datasets/tragic_talkers_dataset.py" code-line-numbers="true" filename="src/datasets/tragic_talkers_dataset.py"}
```
:::

## Inference Engine {#sec-architecture-inference}

## Evaluation {#sec-architecture-evaluation}

## Visualization {#sec-architecture-visualization}


## Rendering {#sec-architecture-rendering}




# Datasets {#sec-datasets}
(Zainab - 1.)

## TED Kid Video {#sec-datasets-ted-kid}

## TED Talks {#sec-datasets-ted-talks}

## Tragic Talkers {#sec-datasets-tragic-talkers}





# Experiments & Evaluation Metrics {#sec-experiments-metrics}
(Tim - 2.)

## Evaluation Metrics {#sec-metrics}

### Euclidean Distance {#sec-metrics-euclidean-distance}

### Percentage of Keypoints (PCK) {#sec-metrics-pck}

### Root Mean Square Error (RMSE) {#sec-metrics-rmse}

### Velocity {#sec-metrics-velocity}

### Acceleration {#sec-metrics-acceleration}

### Jerk {#sec-metrics-jerk}



## Experimental Setup {#sec-experiments}
(Tim - 3.)

### TED Kid Video {#sec-experiments-ted-kid}

### TED Talks {#sec-experiments-ted-talks}

### Tragic Talkers {#sec-experiments-tragic-talkers}

### Inference on Raw vs. Masked Videos {#sec-experiments-inference-raw-masked}




# Results {#sec-results}
(Tim - 4.)

## TED Kid Video {#sec-results-ted-kid}

## TED Kid Video {#sec-results-ted-talks}

## Tragic Talkers {#sec-results-tragic-talkers}

## Inference on Raw vs. Masked Videos {#sec-results-inference-raw-masked}




# Future Work & Limitations {#sec-future-work}
(Zainab - 4.)
(Tim - 5.)




# Conclusion {#sec-conclusion}
(Tim - 7.)


# References
::: {#refs}
:::

