---
title: "MaskBench - A Comprehensive Benchmark Framework for 2D Pose Estimation and Video De-Identification"
date: "2025 08 08"
author:
    - name: Tim Riedel
      affiliation: Hasso Plattner Institute, University of Potsdam, Germany
    - name: Zainab Zafari
      affiliation: Hasso Plattner Institute, University of Potsdam, Germany
    - name: Sharjeel Shaik
      affiliation: University of Potsdam, Germany
    - name: Babajide Alamu Owoyele
      affiliation: Hasso Plattner Institute, University of Potsdam, Germany
    - name: Wim Pouw
      affiliation: Tilburg University, Netherlands
contact:
    - name: Tim Riedel
      email: tim.riedel@student.hpi.de
    - name: Zainab Zafari
      email: zainab.zafari@student.hpi.de
    - name: Babajide Alamu Owoyele
      email: babajide.owoyele@.hpi.de
    - name: Wim Pouw
      email: w.pouw@tilburguniversity.edu
bibliography: dependencies/refs.bib
css: dependencies/styles.css
theme:
    - journal
    - dependencies/theme.scss
format:
    html:
        tbl-cap-location: bottom
        toc: true
        toc-location: left
        toc-title: "Contents"
        toc-depth: 3
        number-sections: true
        code-fold: true
        code-tools: true
        output-file: index.html
        grid:
            margin-width: 100px
filters:
    - include-code-files
# engine: knitr
jupyter: python3
---

# Abstract {#sec-abstract}
Pose estimation plays a critical role in numerous computer vision applications but remains challenging in scenarios involving privacy-sensitive data and in real-world, unconstrained videos like TED Talks, that are not recorded under controlled laboratory conditions.
To address the issue of sharing datasets across academic institutions without compromising privacy, we explore how masking strategies like blurring, pixelation, contour overlays, and solid fills impact pose estimation performance.
We introduce MaskBench, a modular and extensible benchmarking framework designed to evaluate pose estimators under varying conditions, including masked video inputs.
MaskBench integrates a total of seven pose estimators, including YoloPose, MediaPipePose, OpenPose, and both automated and human-in-the-loop variants of MaskAnyone, a multi-stage pipeline combining segmentation and pose estimation through a mixture-of-expert approach.
Our evaluation was done on four datasets with increasing scene complexity, and uses both kinematic metrics like velocity, acceleration, and jerk as well as accuracy-based metrics like Percentage of Correct Keypoints (PCK) and Root Mean Square Error (RMSE).
Results show that MaskAnyone variants significantly improve the visual quality of the pose estimation by reducing jitter and improving keypoint stability, especially the human-in-the-loop variants of MaskAnyone-MediaPipe.
These visual results are supported by quantitative metrics, with the aforementioned models achieving the lowest acceleration and jerk values across all datasets.
YoloPose consistently ranks as the most robust standalone model.
Regarding masking techniques, preliminary results suggest that blurring offers a promising balance between privacy and pose estimation quality.
However, since this experiment was conducted on a limited set of videos, further investigation is needed to draw general conclusions.
These findings highlight the potential of pipelines like MaskAnyone and the extensibility of MaskBench for future research on pose estimation under privacy-preserving constraints.



# Getting Started {#sec-installation}

## ðŸ› ï¸ Installation {#sec-installation-setup}
Follow the instructions below to install and run experiments with MaskBench:

1. **Install Docker** and ensure the daemon is running.
2. **Clone this repo**:
   ```bash
   git clone https://github.com/maskbench/maskbench.git
   ```
3. **Setup the folder structure**. You can store the datasets, outputs or weights in any location you want (for example, if your dataset is large and stored on a separate disk), however, to minimize setup overhead, we suggest the following folder structure:

    ```bash
    maskbench/
    â”œâ”€â”€ src
    â””â”€â”€ config/
        â””â”€â”€ your-experiment-config.yml
    
    maskbench_assets/
    â”œâ”€â”€ weights
    â”œâ”€â”€ output
    â””â”€â”€ datasets/
        â””â”€â”€ your-dataset/
            â”œâ”€â”€ videos/
            â”‚   â””â”€â”€ video_name1.mp4
            â”œâ”€â”€ labels/
            â”‚   â””â”€â”€ video_name1.json
            â”œâ”€â”€ maskanyone_ui_mediapipe/
            â”‚   â””â”€â”€ video_name1.json
            â””â”€â”€ maskanyone_ui_openpose/
                â””â”€â”€ video_name1.json
    ```
4. **Switch to the git repository**

    ```bash
    cd maskbench
    ```
5. **Create the environment file**. This file is used to tell MaskBench about your dataset, output and weights directory, as well as the configuration file to use for an experiment. Copy the .env file using:

    ```bash
    cp .env.dist .env
    ```
6. **Edit the .env file**. Open it using `vim .env` or `nano .env.`. Adjust the following variables:
    * `MASKBENCH_GPU_ID:` If you are on a multi-GPU setup, tell MaskBench which GPU to use. Either specify a number (0, 1, ...) or "all" in which case all available GPUs on the system are used. Currently, MaskBench only supports inference on a single GPU or on all GPUs.
    * `MASKBENCH_CONFIG_FILE:` The configuration file used to define your experiment setup.

    The following variables only need to be adjusted, if you use a different folder structure than the one proposed above:
    * `MASKBENCH_DATASET_DIR:` The directory where video files are located. MaskBench supports video files with .mp4 and .avi extensions.
    * `MASKBENCH_OUTPUT_DIR:` The directory where experiment results will be saved.
    * `MASKBENCH_WEIGHTS_DIR:` Directory for storing model weights (e.g., MediaPipe, YOLOv11, OpenPose).
7. **Run the MaskBench Docker container**.

    ```bash
    docker compose up
    ```
    If multiple users run MaskBench simultaneously, use `docker compose -p $USER up`.

## ðŸš€ Usage {#sec-installation-usage}
The following paragraphs describe how to structure your dataset, configure the application, and understand the output of MaskBench. Following these guidelines ensures the application runs smoothly and recognizes your data correctly.

### ðŸ“‚ Dataset structure

1. **Videos**: Place all videos you want to evaluate in the `videos` folder.

    ```bash
    your-dataset/
    â”œâ”€â”€ videos/
    â”‚    â”œâ”€â”€ video_name1.mp4
    â”‚    â”œâ”€â”€ video_name2.mp4
    ```

2. **Labels** (Optional): If you provide labels, there must be exactly one label file for each video, with the same file name. Example:

    ```bash
    your-dataset/
    â”œâ”€â”€ videos/
    â”‚    â””â”€â”€ video_name1.mp4
    â”œâ”€â”€ labels/
    â”‚    â””â”€â”€ video_name2.json
    ```

3. **MaskAnyoneUI Output**: If you use MaskAnyoneUI, run the application, download the resulting pose file, store it in either the  `maskanyone_ui_openpose` or `maskanyone_ui_mediapipe` folder and once again name it exactly like the corresponding video file.

### âš™ï¸ Configuration Files

We provide four sample configuration files from our experiments. Feel free to copy and adapt them to your needs. The following note explains some parameters in more detail.

::: {.callout-note collapse="true"}
## Explanation of config parameters
```{.yaml}
# A directory name (MaskBench run name) inside the output directory from which to load existing results.
# If set, inference is skipped and results are loaded. To run inference from scratch, comment out or set to "None".
inference_checkpoint_name: None
execute_evaluation: true                    # Set to false to skip calculating evaluation metrics and plotting.
execute_rendering: true                     # Set to false to skip rendering the videos.

dataset:
  name: TragicTalkers                       # User-definable name of the dataset
  module: datasets.tragic_talkers_dataset
  class: TragicTalkersDataset
  dataset_folder: /datasets/tragic-talkers  # Location of the dataset folder
  config:
    video_folder: videos
    ground_truth_folder: labels
    convert_gt_keypoints_to_coco: true


pose_estimators:                            # List of pose estimators (specificy as many as needed)
  - name: YoloPose                          # User-definable name of the pose estimator. 
    enabled: true                           # Enable or disable this pose estimator.
    module: models.yolo_pose_estimator
    class: YoloPoseEstimator
    config:                                 # Pose estimator specific configuration variables.
      weights: yolo11l-pose.pt              # Weights file name inside the specified weights directory.
      save_keypoints_in_coco_format: true   # Whether to store keypoints in COCO format (18 keypoints) or not)
      confidence_threshold: 0.3             # Confidence threshold below which keyopints are considered undetected.

  - name: MaskAnyoneUI-MediaPipe
    enabled: true
    module: models.maskanyone_ui_pose_estimator
    class: MaskAnyoneUiPoseEstimator  
    config:
      dataset_folder_path: /datasets/tragic-talkers/maskanyone_ui_mediapipe # Folder of MaskAnyone poses
      overlay_strategy: mp_pose             # Mediapipe: mp_pose, OpenPose: openpose, openpose_body25b
      save_keypoints_in_coco_format: true
      confidence_threshold: 0               # Confidence thresholds not supported by MaskAnyone

metrics:                                    # List of metrics  (specificy as many as needed)
  - name: PCK                               # User-definable name of the metric
    module: evaluation.metrics.pck
    class: PCKMetric
    config:                                 # Metric specific configuration variables.
      normalize_by: bbox
      threshold: 0.2

  - name: Velocity
    module: evaluation.metrics.velocity
    class: VelocityMetric
    config:
      time_unit: frame
```
:::


### ðŸ‹ï¸ Model Variants and Weights

You only need to modify the weights if you are adding a new pose estimator to MaskBench.
In that case, place your weights in the `weights/` folder.
By default, MaskBench automatically downloads the following weights:

* MediaPipe: `pose_landmarker_{lite, full, heavy}.task`
* Yolo11: `yolo11{n, s, m, l, x}-pose`
* OpenPose overlay_strategy: `BODY_25, BODY_25B, COCO`
* MaskAnyone overlay_strategy: `mp_pose, openpose, openpose_body25b`

### ðŸ“Š Output

All results, including plots, pose files, inference times and renderings, will be saved in the output directory.
For each run of MaskBench a folder is created with the name of the dataset and a timestamp. Example:

```bash
output/
 â”œâ”€â”€ TedTalks_2025-08-11_15-42-10/
 â”‚    â”œâ”€â”€ plots/
 â”‚    â”œâ”€â”€ poses/
 â”‚    â”œâ”€â”€ renderings/
 â”‚    â”œâ”€â”€ inference_times.json
```

# Related Work {#sec-related-work}

Human pose estimation is a core task in computer vision, concerned with identifying the spatial positions of body jointsâ€”such as shoulders, elbows, and kneesâ€”from images or video sequences. Existing approaches are typically classified into two broad strategies: top-down and bottom-up. Top-down methods begin by detecting individual persons within an image, after which a separate pose estimation model is applied to each detected instance. In contrast, bottom-up approaches first detect all keypoints across the image and then group them to form full-body poses for each individual [@saiwa2025openpose; @kaim2024comparison].

Among widely used frameworks, **OpenPose** [@openpose-3] is a prominent example of a bottom-up pose estimation method. It first identifies keypoints across the entire image and then assembles them into person-wise skeletons using Part Affinity Fields (PAFs). OpenPose supports multiple configurations, including 18- and 25-keypoint body models, and offers full-body tracking, hand and facial landmark estimation. The framework is optimized for GPU execution and is widely used in applications requiring multi-person pose estimation.

**YOLO11** [@yolo, @yolo11_ultralytics], on the other hand, follows a top-down approach. It extends the YOLO family of real-time object detectors by incorporating pose estimation capabilities. After detecting bounding boxes for each person, YOLO11 predicts 17 body keypoints per individual, using a topology aligned with the COCO keypoint format. It is designed for high-performance scenarios and is optimized for GPU usage, making it suitable for real-time, multi-person tracking in high-resolution video streams.

**MediaPipe Pose** [@mediapipe] is a lightweight, top-down framework designed specifically for real-time pose estimation on CPU-only devices. Built upon BlazePose, it employs a 33-landmark skeleton that extends the standard COCO format with additional joints to improve anatomical precision. The pipeline consists of an initial detection and tracking stage, followed by landmark prediction and overlay. MediaPipe is particularly suited for single-person applications in mobile and browser environments, where computational efficiency and low latency are critical.

Several studies have benchmarked popular pose estimation models across different datasets, conditions, and use cases. The following works are particularly relevant to our benchmarking framework:

* **Comparision Of ML Models For Posture** [@kaim2024comparison]
    Compared YOLOv7 Pose and MediaPipe Pose. YOLOv7 achieved a slightly higher accuracy score of 87.8\% versus MediaPipeâ€™s 84.1\%. However, MediaPipe demonstrated superior real-time performance on CPU-only devices, achieving 16-18 frames per second (FPS) compared to YOLOv7â€™s 4-5 FPS. In low-light environments, MediaPipe maintained detection consistency, whereas YOLOv7 performed better in occluded scenarios, successfully recognizing hidden body parts. 

* **OpenPose vs MediaPipe: A Practical and Architectural Comparison** [@saiwa2025openpose]
    A recent blog post by Saiwa presents a detailed comparison between OpenPose and MediaPipe, discussing their architectural differences, device compatibility, and practical applications. OpenPose uses a bottom-up approach with Part Affinity Fields and is optimized for multi-person full-body tracking, whereas MediaPipe follows a top-down strategy focusing on speed and cross-platform deployment.

In addition to standalone pose estimation models, **MaskAnyone** [@maskanyone-github] [@schilling2023maskanyone] is a multi-stage framework developed at the Hasso Plattner Institute (HPI) that combines object detection, segmentation, de-identification, and pose estimation within a unified pipeline. The system begins by detecting human instances in a video using YOLO. For each detected bounding box, SAM2 [@sam2] is applied to segment the individual subject. Depending on the configuration, pose estimation is then performed using either OpenPose or MediaPipe. Last but not least, it produces a de-identified video using the SAM2 segmentation masks. The framework supports both fully automatic processing (we refer to it as MaskAnyoneAPI) and a human-in-the-loop approach (referred to as MaskAnyoneUI), where users can manually select specific frames, refine the segmentation output of SAM2 and thereafter start the pose estimation. This combination of automated and user-guided steps allows for finer control in scenarios where automatic segmentation may be insufficient or require correction.


# MaskBench Architecture {#sec-architecture}
![MaskBench Architecture](./images/maskbench-workflow.jpg){#fig-architecture}

The general workflow of MaskBench is shown in Figure @fig-architecture.
It begins with loading the dataset, pose estimators, and evaluation metrics.
The application then creates a checkpoint folder in the specified output directory, named according to the dataset and a timestamp (e.g., `/output/TedTalks-20250724-121127`).
Subsequently, inference is performed on all videos in the dataset using the pose estimators specified in the configuration file.
For the MaskAnyoneUI pose estimators, the user is required to perform semi-automatic annotation of the videos using MaskAnyone.
A poses folder is created within the checkpoint, containing a subfolder for each pose estimator and a single JSON file for each video.
The application then evaluates all specified metrics and generates plots, which are stored in the plots folder within the checkpoint.
Finally, for each video, the application produces a set of rendered videosâ€”one for each pose estimatorâ€”which are stored in the renderings folder in the checkpoint.

Each component of MaskBench is implemented in a modular way, so it can be easily extended and modified.
We will discuss this in the following sections.

## Dataset {#sec-architecture-dataset}
The dataset provides video data for pose estimation and, if available, ground truth data for evaluation.
When adding a new dataset, the user must create a new class that inherits from the Dataset class and override the `_load_samples` method, which generates one VideoSample object for each video in the dataset.
If the dataset provides ground truth data, the user must also override the `get_gt_pose_results` and `get_gt_keypoint_pairs` methods.
For each video, the `get_gt_pose_results` method should return a `VideoPoseResult` object.
The `get_gt_keypoint_pairs` method is used to render the ground truth keypoints and contains a list of tuples, each specifying the indices of two keypoints to be connected in the rendered video.
Default keypoint pairs for YoloPose, MediaPipePose, and various implementations of OpenPose models are provided in the `keypoint_pairs.py` file.

Below is the code implementation for the abstract dataset class, the simple TED Talks dataset (without ground truth), and the more complex TragicTalkers dataset (with pseudo-ground truth data).

::: {.callout-note collapse="true"}
## Dataset Class
```{.python include="../src/datasets/dataset.py" code-line-numbers="true" filename="src/datasets/dataset.py"}
```
:::

::: {.callout-note collapse="true"}
## TED Talks Dataset
```{.python include="../src/datasets/ted_dataset.py" code-line-numbers="true" filename="src/datasets/ted_dataset.py"}
```
:::

::: {.callout-note collapse="true"}
## Tragic Talkers Dataset
```{.python include="../src/datasets/tragic_talkers_dataset.py" code-line-numbers="true" filename="src/datasets/tragic_talkers_dataset.py"}
```
:::

## Inference {#sec-architecture-inference}

### Video Pose Result {#sec-architecture-video-pose-result}
The VideoPoseResult object represents the standardized output of a pose prediction model.
It is a nested structure containing a `FramePoseResult` object for each frame in the video.
Each frame pose result includes a list of `PersonPoseResult` objects, one for each person detected in the frame.
Every personâ€™s result contains a list of `PoseKeypoint` objects, one for each keypoint in the modelâ€™s output format, providing x and y coordinates along with an optional confidence score.

::: {.callout-note collapse="true"}
## Video Pose Result Class
```{.python include="../src/inference/pose_result.py" code-line-numbers="true" filename="src/inference/pose_result.py"}
```
:::

### Pose Estimator {#sec-architecture-pose-estimators}
Pose estimators are responsible for predicting the poses of persons in a video by wrapping calls to specific AI models or pose estimation pipelines.
Each model is implemented in a separate class that inherits from the abstract `PoseEstimator` class.
The output of each estimator is a standardized `VideoPoseResult` object.

To add a new pose estimator, users must implement methods for pose estimation and for retrieving keypoint pairs.
Special care must be taken to ensure that the output meets the following constraints:

1.	The number of frames in the pose results matches the number of frames in the video.
2.	If no persons are detected in a frame, the persons list should be empty.
3.	For detected persons with missing keypoints, those keypoints should have values `x=0, y=0, confidence=None`.
4.	The number of keypoints per person remains constant across all frames.
5.	Keypoints with low confidence should be masked out using the confidence_threshold configuration parameter.
6.	Keypoints must be mapped to the COCO format if the `save_keypoints_in_coco_format` configuration parameter is set to true.


As an example, we provide the implementation of the abstract pose estimator class and the implementation of the YOLO model.

::: {.callout-note collapse="true"}
## Pose Estimator Class
```{.python include="../src/models/pose_estimator.py" code-line-numbers="true" filename="src/models/pose_estimator.py"}
```
:::

::: {.callout-note collapse="true"}
## YOLO Model
```{.python include="../src/models/yolo_pose_estimator.py" code-line-numbers="true" filename="src/models/yolo_pose_estimator.py"}
```
:::

MaskBench supports seven pose estimators, including pure AI models such as YOLOv11-Pose, MediaPipePose, and OpenPose.
Additionally, it incorporates MaskAnyone as a pose estimator, which combines multiple expert models.
We distinguish between two variants of the MaskAnyone estimator: the MaskAnyoneAPI pose estimator, which runs fully automatically during inference, and the MaskAnyoneUI pose estimator, which employs a human-in-the-loop approach allowing manual adjustment of the mask for the persons of interest.
The latter requires manual execution by the user prior to running MaskBench, with the resulting pose files provided as one file per video.

### Inference Engine {#sec-architecture-inference-engine}
The inference engine is responsible for running pose estimators on videos and saving the results as JSON files in the poses folder.
If a checkpoint name is specified in the configuration file, the inference engine will load existing results from the checkpoint and skip inference for videos that already have corresponding outputs.
This feature allows the user to resume an already started inference process or to bypass the time-consuming inference entirely and perform only metric evaluation and rendering.
The inference engine returns a nested dictionary that maps pose estimator names to video names and their corresponding VideoPoseResult objects.
Additionally, it records the inference times for each pose estimator and video, saving this information as a JSON file within the checkpoint folder.


## Evaluation {#sec-architecture-evaluation}

### Metric {#sec-architecture-evaluation-metric}
Each metric inherits from the abstract Metric class and implements a computation method that takes as input a predicted video pose result, an optional ground truth pose result, and the name of the pose estimator.
Metrics play a crucial role in quantitatively evaluating the accuracy and quality of pose predictions against ground truth data.
We support a range of metrics, including Velocity, Acceleration, Jerk, Euclidean Distance, Percentage of Correct Keypoints (PCK), and Root Mean Square Error (RMSE).
This method outputs a MetricResult object containing the metric values for the video (see section @sec-architecture-evaluation-metric-result).

::: {.callout-note collapse="true"}
## Metric Class 
```{.python include="../src/evaluation/metrics/metric.py" code-line-numbers="true" filename="src/evaluation/metrics/metric.py"}
```
:::

MaskBench currently implements ground truth-based metrics for Euclidean Distance, Percentage of Correct Keypoints (PCK), and Root Mean Square Error (RMSE).
Furthermore, we provide kinematic metrics for velocity, acceleration, and jerk.
Section @sec-metrics contains a more extensive description of the implemented metrics.

**Matching Person Indices**

For some metrics, it is essential to ensure that the order of persons in the predicted video pose results matches that of the reference.
The metric class provides a method called `match_person_indices` to align person indices between ground truth and predicted results.
This method is used not only in ground-truth-based metrics but also in kinematic metrics, which require consistent person indices across consecutive frames to compute velocity, acceleration, and other temporal measures.
The implementation employs the Hungarian algorithm, using the mean position of a personâ€™s keypoints to find the optimal matching between all persons in the reference and predicted pose results.

Let $N$ denote the number of persons in the reference, $M$ the number in the prediction, and $K$ the number of keypoints per person.
The output of the `match_person_indices` method is an array with shape $\text{max}(N, M) \times K \times 2$.
The first $N$ entries correspond to persons ordered as in the reference, while the remaining $M - N$ entries (if $M > N$) represent additional persons present only in the prediction.

Edge cases include situations where a person appears in one frame but not in the next.
In such cases, the unmatched person is assigned an index with infinite values to indicate absence, while the other persons retain consistent indices.
This also applies when the prediction contains fewer persons than the reference (M < N).
Each metric can then handle these infinite values appropriatelyâ€”for example, by converting them to NaN in kinematic metrics or assigning predefined values in Euclidean distance and ground truthâ€“based metrics.

**Unit Testing**

Implementing unit tests for metric classes is essential to ensure that their outputs are accurate and consistent.
We provide unit tests for all metrics in the `src/tests` folder, which can be executed using the `pytest` command.
Running these tests after any modifications to the metric classes helps guarantee that existing functionality remains intact.

### Metric Result {#sec-architecture-evaluation-metric-result}
The output of a metricâ€™s compute method is a MetricResult object.
This object contains metric values stored in a multi-dimensional array, where each axis is labeled with descriptive names such as â€œframe,â€ â€œperson,â€ and â€œkeypoint.â€
The class provides an aggregate function that reduces these values using a specified method along selected axes only.
Currently, MaskBench supports aggregation methods including mean, median, Root Mean Square Error (RMSE), vector magnitude, sum, minimum, and maximum.
The result of the aggregation is another MetricResult object with reduced dimensionality, retaining only the axes that were not aggregated.

This flexible approach of storing the results with their axes names and using the names in the aggregation method allows for the visualization of the results in a variety of ways, for example, as a per-keypoint plot, distribution plot, or as a single scalar value. Furthermore, it allows extending the framework with new metrics (possibly containing different axis names) and also different visualizations.

### Evaluator {#sec-architecture-evaluation-evaluator}
Given a list of metrics, the evaluator executes each configured metric on the pose estimation results for all pose estimators and videos.
It returns a nested dictionary that maps metric names to pose estimator names, then to video names, and finally to their corresponding `MetricResult` objects.
It does not perform aggregation over the videos or pose estimators in order to allow for more flexibility in the visualization of the results.

## Visualization {#sec-architecture-visualization}
After evaluation, the results are visualized in plots and tables.

### Visualizer
An abstract `BaseVisualizer` class defines the interface for all visualization components.
We implemented a MaskBench-specific visualizer class tailored to our experiments, which can be reused for other studies or extended to accommodate new types of visualizations.

::: {.callout-note collapse="true"}
## MaskBench Visualizer
```{.python include="../src/evaluation/visualizer/maskbench_visualizer.py" code-line-numbers="true" filename="src/visualization/maskbench_visualizer.py"}
```
:::

The visualizer saves the plots and tables in the `plots` folder in the checkpoint.

### Plots
Each plot inherits from the abstract Plot class and implements the draw method.
This method accepts various forms of input data, most commonly the results produced by the evaluator.
Each plot can define a specific approach to aggregating and organizing the data, such as computing the median over all videos for a given pose estimator.

::: {.callout-note collapse="true"}
## Plot Class
```{.python include="../src/evaluation/plots/plot.py" code-line-numbers="true" filename="src/evaluation/plots/plot.py"}
```
:::

::: {.callout-note collapse="true"}
## Kinematic Distribution Plot
```{.python include="../src/evaluation/plots/kinematic_distribution_plot.py" code-line-numbers="true" filename="src/evaluation/plots/kinematic_distribution_plot.py"}
```
:::

We provide the following plots and tables:

- **Kinematic Distribution Plot**: Visualizes the distribution of kinematic values for each pose estimator.
- **Per Keypoint Plot**:  Displays the median kinematic metric values or Euclidean distance for each COCO keypoint. This plot requires keypoints to be stored in COCO format.
- **Inference Time Plot**: Visualizes the average inference time associated with each pose estimator.
- **Result Table**: Aggregates results per metric and pose estimator across all videos, presenting the data in tabular form.


## Rendering {#sec-architecture-rendering}
Video rendering is handled by the `Renderer` class.
For each video in the dataset, the renderer creates a dedicated folder within the `renderings` directory of the checkpoint folder.
Inside each video folder, it generates one video per pose estimator, displaying the rendered keypoints.
Special attention was given to maintaining consistent colors for each pose estimator across all videos and plots, using a predefined, color-blindâ€“friendly palette.


# Datasets {#sec-datasets}
This study uses four video-based datasets, each representing a different level of complexity, from simple, controlled settings to more dynamic and interactive scenarios. To capture this range, we selected or created four distinct datasets: TED Kid Video, TED Talks [@ted], Tragic Talkers [@tragic-talkers] and a masked video dataset. Each dataset was chosen based on specific conditions to evaluate pose estimation models under varying degrees of difficulty.

## TED Kid Video {#sec-datasets-ted-kid}
The TED Kid Video is a short, 10-second clip featuring a child in a well-lit environment with high video quality. Throughout the sequence, all body parts remain clearly visible, and there is no occlusion or obstruction of the subject. This video serves as a controlled scenario to evaluate pose estimation methods under ideal conditions, providing a baseline for comparison with more complex datasets. 

We first tested our models' performance and evaluation metrics on this video to verify that our metrics function correctly in an ideal setting and that the implementation is accurate. This initial validation ensures that subsequent experiments on more challenging datasets can be interpreted with confidence in the correctness of our evaluation pipeline.

## TED Talks {#sec-datasets-ted-talks}
For the TED Talks [@ted], we selected ten videos featuring diverse speakers to capture a range of conditions. The selection criteria included speaker gender, skin tone, clothing types (e.g., long dresses vs. short garments), partial occlusion, videos where only the hands, upper body, or lower body are visible, and variations in movement style and speed.

We focused on how the models perform under more complex conditionsâ€”such as when the scene changes, when there are noises such as audience, when only the lower or upper body is visible, when only hands or feet appear, or when body parts are not easily distinguishable (e.g., when someone wears a long dress). We also considered cases where visual elements like images or patterns are present on the speakerâ€™s clothing. In each TED Talk video, our analysis focuses solely on the primary speaker.

## Tragic Talkers {#sec-datasets-tragic-talkers}
We wanted to evaluate the models' performance under conditions where more than one person is present and interacting. The Tragic Talkers dataset [@tragic-talkers] was selected because it includes 2D pseudo-ground truth annotations generated by the AI model OpenPose, allowing us to establish a baseline and test metrics such as Percentage of Correct Keypoints (PCK).

The dataset features a man in regular clothing and a woman wearing a long dress. It includes four distinct video scenarios, each originally recorded from twenty-two different camera angles. However, for our analysis, we used only four angles, as many were too similar in viewpoint.
The video scenarios are:

**Monologue (Male and Female):** Individual speakers deliver monologues with relatively simple and slow movements.

**Conversation:** A male and female speaker engage in dialogue with limited movement.

**Interactive 1:** A conversation between a male and female speaker that includes physical interaction (e.g., hand contact), with the man sitting close to the woman.

**Interactive 4:** A more dynamic dialogue featuring faster movements, partial occlusion, and moments of full occlusion.

These scenarios were chosen to reflect a variety of real-world human interactions, allowing us to test how well pose estimation models perform under conditions such as occlusion, multi-person scenes, and varied movement patterns.

## Masked Video Dataset {#sec-datasets-masked-video}
The masked video dataset is a colllection of three videos.
It contains the TED kid video, a chunk from the TED talk "Let curiosity lead" [@ted-curiosity] and the video "interactive1_t1-cam06" from the Tragic Talkers dataset.
The purpose of this dataset is to evaluate the performance of pose estimators when inference is performed on masked videos, to adress the challenge of distributing datasets between researchers, which contain sensitive information that 

For the creation of this dataset, we used MaskAnyoneUI to manually mask the persons of interest with four different hiding strategies (blurring, pixelation, contours and solid fill) in each of the three videos.
We therefore obtained a total of 15 videos, including the original videos.


## Data Preprocessing
Data preprocessing was carried out to remove unnecessary parts of the videos and to split the TED Talk videos into shorter segments compatible with MaskAnyone. Since MaskAnyone cannot process videos longer than 2.5 minutes, and is already resource-intensive even at that limit, we divided the TED Talk videos into chunks of 30 or 50 seconds, depending on the content.

TED Talks also showed some inconsistency in structure. Some videos were straightforward, with only the speaker and audience visible, making them easy to segment at any point. However, others included additional visual content such as slides, pictures, or unrelated scenes, which made it more difficult to determine clean chunking points.

For these more complex videos, we carefully selected segment boundaries to ensure that each chunk started with frames where a human was clearly visible. When necessary, we manually trimmed the beginning of chunks to avoid starting with empty or unrelated frames. This step was critical because if a video starts with non-human content, MaskAnyone may incorrectly classify objects in the first frame as humans and then continue misdetecting them in subsequent frames.

After preprocessing, the chunks from each TED Talk were merged back into a single video file. This allowed all pose estimation models to be evaluated on the same content and ensured consistency across results.

No preprocessing was required for the Tragic Talkers dataset, as the videos were already clean and free of noise or unrelated visual content.


# Evaluation Metrics {#sec-metrics}
In the following sections, we outline the metrics used for evaluating accuracy, smoothness and jitter of different pose estimators.

## Ground-Truth Metrics
The metrics in this section are based on ground truth data provided by the dataset and primarily evaluate the accuracy of the pose estimation compared to the reference ground truth.

### Euclidean Distance {#sec-metrics-euclidean-distance}
The Euclidean distance metric measures the spatial accuracy of pose estimation by calculating the normalized distance between predicted and ground truth keypoint positions. 
For each keypoint of a person in a frame, it computes the L2 norm (Euclidean distance) between the predicted position $(x_p, y_p)$ and the ground truth position $(x_{gt}, y_{gt})$:

$$ d = \frac{\sqrt{(x_p - x_{gt})^2 + (y_p - y_{gt})^2}}{s} $$

where $s$ is a normalization factor. 
The normalization is crucial to make the metric scale-invariant and comparable across different person sizes. 
The metric is set up to support three normalization strategies, out of which we only implemented the first one:

1. **Bounding Box Size**: The distance is normalized by the maximum of width and height of the person's bounding box, which is computed from the ground truth keypoints. While this adapts to different person sizes, it has the minor drawback of pose-dependent scaling variance.

2. **Head Size**: Normalization by the head bone link size (not implemented).

3. **Torso Size**: Normalization by the torso diameter (not implemented).

Head and torso normalization inherently solve the shorter limb problem as smaller limbs are typically accompanied by proportionally smaller torso diameters and head bone links. 
We outline future work for the implementation of head and torso normalization in section @sec-future-work.

The metric handles several edge cases to ensure robust evaluation:

- **Different Order of Persons**: The metric uses the Hungarian algorithm as described in section @sec-architecture-evaluation-metric to match person indices between ground truth and predictions, ensuring that distances are calculated between corresponding persons even if they appear in different orders.

- **Keypoint Missing in Ground Truth but not in Prediction**: When a keypoint is missing in the ground truth (coordinates are (0,0)) but was detected in the prediction, the corresponding distance is set to `NaN` and excluded from aggregation calculations, as a distance to a missing keypoint is undefined.

- **Keypoint Missing in Prediction but Present in Ground Truth**: When a keypoint exists in the ground truth but is missing in the prediction (coordinates are (0,0)), the distance is set to a predetermined fill value (in this case a large distance value of 1). This penalizes the model for failing to detect keypoints without overly impacting the aggregated results.

- **Undetected Persons**: If a person present in the ground truth is entirely undetected in the prediction, all keypoint distances for that person are set to the same distance fill value.

Euclidean distance is the basis for the computation of the Percentage the PCK and RMSE metrics.

### Percentage of Keypoints (PCK) {#sec-metrics-pck}
The Percentage of Correct Keypoints (PCK) metric evaluates pose estimation accuracy by determining the proportion of predicted keypoints that fall within a specified threshold distance of their ground truth locations. 
A keypoint is considered "correct" if its normalized Euclidean distance to the ground truth is less than the threshold.

For each frame, PCK is calculated as:

$$
PCK = \frac{\text{number of keypoints with distance < threshold}}{\text{total number of valid keypoints}}
$$

The metric produces a value between 0 and 1, where 1 indicates perfect prediction (all keypoints within the threshold) and 0 indicates complete failure (no keypoints within the threshold). 
PCK is particularly useful for understanding the overall reliability of pose predictions at a given precision level defined by the threshold.

### Root Mean Square Error (RMSE) {#sec-metrics-rmse}
The Root Mean Square Error (RMSE) provides a single aggregate measure of pose estimation accuracy by computing the root mean square of normalized Euclidean distances across all keypoints and persons in a frame.
RMSE is calculated as:

$$
RMSE = \sqrt{\frac{1}{N} \sum_{i=1}^{N} d_i^2}
$$

where $N$ is the total number of valid keypoints in the frame and $d_i$ is the normalized Euclidean distance for keypoint $i$. 
RMSE penalizes larger errors more heavily due to the squared term, making it particularly sensitive to outliers. 

## Kinematic Metrics
Velocity, acceleration and jerk are kinematic metrics that are useful for identifying unnatural or jittery movements in pose estimations, as they highlight rapid changes in motion.

### Velocity {#sec-metrics-velocity}
The velocity metric calculates the rate of change in keypoint positions between consecutive frames. 
For each keypoint of a person, it measures how quickly that keypoint moves in pixels per frame, giving insights into the smoothness and consistency of the pose estimation across frames.

The calculation of velocity follows a three-step process:

1. First, person indices are matched between consecutive frames as described in @sec-architecture-evaluation-metric to ensure we track the same person across frames.

2. The velocity is then computed with $v_t = p_{t+1} - p_t$ as the difference between keypoint positions in consecutive frames, where $p_t$ represents the keypoint position at frame $t$ and $v_t$ is the resulting velocity vector.

3. Finally, the metric can be configured to report velocities in either pixels per frame or pixels per second. For the latter, the frame-based velocity is divided by the time delta between frames (1/fps).

Several edge cases are handled specifically:

- For videos with fewer than 2 frames, the metric returns `NaN` values as velocity cannot be computed.
- When a keypoint is missing in either of two consecutive frames (masked or invalid), the velocity for that keypoint is set to `NaN`.
- The result will have one less frame than the input video, because the velocity metric produces one metric value for any consecutive pair of frames.
- The output contains a coordinate axis (for x and y) to represent the velocity as a vector and to serve as a basis for the acceleration and jerk metrics. For evaluation and visualization, the metric result should therefore be aggregated with the method `vector_magnitude` along the coordinate axis to obtain a scalar velocity value.

### Acceleration {#sec-metrics-acceleration}
The acceleration metric measures the rate of change in velocity over time, representing how quickly the movement speed of keypoints changes. 
It is computed by $a_t = v_{t+1} - v_t$, where $a_t$ is the acceleration at time $t$, $v_t$ represents the velocity, and $p_t$ the keypoint position. 
Similar to the velocity metric, the acceleration can be reported in either pixels per frame squared or pixels per second squared, with the latter requiring division by the squared time delta between frames (1/fpsÂ²).


### Jerk {#sec-metrics-jerk}
The jerk metric, which measures the rate of change in acceleration, provides insights into the smoothness of motion by quantifying how abruptly the acceleration changes.
It is calculated with $j_t = a_{t+1} - a_t$ as the difference between consecutive acceleration values, where $j_t$ is the jerk at time $t$ and $a_t$ represents the acceleration. 
The metric can be configured to output values in pixels per frame cubed or pixels per second cubed. For the latter, the frame-based jerk is divided by the cubed time delta between frames (1/fpsÂ³).

# Experimental Setup {#sec-experiments}
In this section, we outline the experimental setup for the evaluation of the pose estimators on the four datasets.

## General Setup
We executed a total of seven pose estimators on the four datasets (TED kid video, TED talks, Tragic Talkers and masked video dataset).
The pose estimators are: YoloPose (v11-l), MediaPipePose (pose_landmarker_heavy), OpenPose (body_25), MaskAnyoneAPI-MediaPipe, MaskAnyoneAPI-OpenPose, MaskAnyoneUI-MediaPipe and MaskAnyoneUI-OpenPose.
We set a confidence threshold of 0.3 for YoloPose and MediaPipePose, and 0.15 for OpenPose, for filtering out low-confidence detections.
A confidence threshold of zero is used for all MaskAnyone pose estimators, as the output of MaskAnyone does not contain a confidence score.
All keypoints are stored in the COCO format to allow for a per-keypoint comparison of the results.

## TED Kid Video and TED Talks {#sec-experiments-ted-kid-talks}
For the TED kid video and the TED talks, we evaluated the metrics velocity, acceleration and jerk for each pose estimator.
Because no ground truth is available for the TED talks, we cannot evaluate the accuracy of the pose estimation.

## Tragic Talkers {#sec-experiments-tragic-talkers}
For the Tragic Talkers dataset, we evaluated metrics based on ground-truth like Euclidean distance, PCK and RMSE, and also kinematic metrics like velocity, acceleration and jerk for each pose estimator.
Important to note is, that the Tragic Talkers dataset contains only "pseudo-ground truth" annotations, which were created by the AI model OpenPose.
Unfortunately, it is not noted in the dataset nor paper, which OpenPose model was used to create the pose results, which limits the comparability of the results.

## Inference on Raw vs. Masked Videos {#sec-experiments-inference-raw-masked}
For the masked video dataset, we first performed inference on the raw videos with all seven pose estimators to establish a baseline without any masking.
Thereafter, for each hiding strategy, the pose estimators were executed again on the masked videos.
The results were then compared to the baseline to evaluate the impact of the different hiding strategies on the pose estimation performance.
We used the PCK and RMSE metrics to evaluate the accuracy of the pose estimation compared to the pose result on the raw videos.

It is important to note, that such a pipeline run is not yet natively implemented in MaskBench and is rather preliminary implementation in a script to serve as a proof of concept.
We plan to extend MaskBench to support this workflow in the future, see section @sec-future-work-pipelining.


# Results {#sec-results}
In the following sections, we present the results of the experiments.
In order to increase readability, we will not append the unit to the kinematic metric values in the textual description of the results.
Velocity is measured in pixels/frame, acceleration in pixels/frameÂ² and jerk in pixels/frameÂ³.
Our evaluation will focus mostly on the acceleration and jerk metrics, as they are most suited to detect jittery movements and instability in pose estimation.


## TED Kid Video {#sec-results-ted-kid}
@tbl-results-ted-kid shows the average velocity, acceleration and jerk metric results for different pose estimators on the TED-kid video.
Standard pose estimation models like YoloPose, MediaPipePose and OpenPose exhibit relatively high values across all metrics, indicating more eratic and less stable pose estimations.
MediaPipePose has the highest values for velocity (3.36), acceleration (4.10) and jerk (5.56).
In contrast, all evaluated MaskAnyone pose estimators achieve consistently lower values for acceleration and jerk, with MaskAnyoneUI-MediaPipe achieving the best overall results (velocity: 1.97, acceleration: 1.20, jerk: 1.89).
These results suggest that the mixture-of-expert-model approach of MaskAnyone substantially enhances temporal smoothness and stability in pose estimation.
Notably, the automated MaskAnyoneAPI variants also outperform the standard pose estimators but are slightly less effective than the UI-based counterparts, highlighting the added benefit of a human-in-the-loop approach.
Lastly, it is also observable that MediaPipePose benefits a lot more from the MaskAnyone approach than OpenPose.
While MaskAnyone decreases the average acceleration value of MediaPipe by 2.9 from 4.10 to 1.20 and jerk by 5.31, the average acceleration value of OpenPose is only decreased by 1.11 and jerk by 2.81.



::: {#tbl-results-ted-kid}
```{=html}
<table class="results"><thead>
  <tr>
    <th>Pose Estimator</th>
    <th>Velocity</th>
    <th>Acceleration</th>
    <th>Jerk</th>
  </tr></thead>
<tbody>
  <tr>
    <td>YoloPose</td>
    <td>2.81</td>
    <td>2.95</td>
    <td>5.04</td>
  </tr>
  <tr>
    <td>MediaPipePose</td>
    <td>3.36</td>
    <td>4.10</td>
    <td>7.18</td>
  </tr>
  <tr>
    <td>OpenPose</td>
    <td>2.71</td>
    <td>3.20</td>
    <td>5.56</td>
  </tr>
  <tr class="maskanyone-api border-top">
    <td>MaskAnyoneAPI-MediaPipe</td>
    <td class="second">2.00</td>
    <td class="second">1.21</td>
    <td class="best">1.87</td>
  </tr>
  <tr class="maskanyone-api border-bottom">
    <td>MaskAnyoneAPI-OpenPose</td>
    <td>2.73</td>
    <td>2.46</td>
    <td>3.53</td>
  </tr>
  <tr class="maskanyone-ui border-top">
    <td>MaskAnyoneUI-MediaPipe</td>
    <td class="best">1.97</td>
    <td class="best">1.20</td>
    <td class="second">1.89</td>
  </tr>
  <tr class="maskanyone-ui border-bottom">
    <td>MaskAnyoneUI-OpenPose</td>
    <td>2.62</td>
    <td>2.09</td>
    <td>2.75</td>
  </tr>
</tbody>
</table>
```
Average metric results for different pose estimators on the TED-kid video.
:::

@fig-ted-kid-acceleration-distribution and @fig-ted-kid-jerk-distribution show the distribution of the acceleration and jerk metrics for the different pose estimators.
These plots show the percentage of keypoints within fixed value ranges for the acceleration and jerk metrics over all frames.
The ideal curve for a stable pose estimation follows an exponential decay curve, having many kinematic values close to zero and only a few large values.
Both plots confirm the results from @tbl-results-ted-kid, showing that the MaskAnyone pose estimators have a very high concentration of low acceleration and jerk values.
Both the UI and API variant of MaskAnyone-MediaPipe most closely resemble the ideal curve, having more than 80% of keypoints with an acceleration value of less than 1 pixels/frameÂ².
MaskAnyone-OpenPose pose estimators rank third and fourth achieving around 57% of of these low acceleration values.
According to both the chart and table, YoloPose ranks fifth after the MaskAnyone pose estimators (52%), followed by OpenPose (40%).
Lastly, the chart also indicates that MediaPipePose is the most unstable pose estimator, with only 30% of keypoints having a low acceleration value of less than 1 pixels/frameÂ² and the curve having a relatively flat slope.
Similar patterns can be observed for the jerk distribution.

@fig-ted-kid-acceleration-per-keypoint shows the median acceleration per keypoint for the different pose estimators.
Each keypoint contains a set of seven bars, one for each pose estimator, indicating the median acceleration value for that keypoint and pose estimator.
The first observation is, that keypoints like the wrist, elbow, hip and ankle have consistently higher median acceleration values than other body parts like the eyes, ears and nose.
This is expected, as these body parts are more likely to move and change their position more frequently.
The second observation is, that MaskAnyoneAPI-MediaPipe and MaskAnyoneUI-MediaPipe consistently achieve the lowest acceleration values for all keypoints.
Furthermore, both MaskAnyoneUI-MediaPipe and MaskAnyoneUI-OpenPose improve their respective default pose estimators MediaPipePose and OpenPose across all keypoints.
The most significant improvement occurs for the hips, knees and ankles, where MaskAnyoneUI-MediaPipe reduces the median acceleration of MediaPipePose from four to six pixels/frameÂ² to less than one pixels/frameÂ².

::: {#fig-ted-kid-plots layout="[[1,1], [1]]"}

![Acceleration Distribution](plots/TED-kid/acceleration_distribution.png){#fig-ted-kid-acceleration-distribution}

![Jerk Distribution](plots/TED-kid/jerk_distribution.png){#fig-ted-kid-jerk-distribution}

![Median Acceleration per Keypoint](plots/TED-kid/keypoint_plot_acceleration.png){#fig-ted-kid-acceleration-per-keypoint}

**Comparison of pose estimation models on the TED-kid video.**
(a, b) Acceleration and jerk distribution showing the proportion of keypoints within a fixed value ranges. 
A larger concentration at lower values indicates a smoother and more stable pose, whereas a high number of keypoints with large values indicates more jittery movements.
(c) Median acceleration per keypoint, indicating stability across individual body parts. Keypoints like the wrist, elbow and ankle are expected to have a higher median acceleration than other body parts, which tend to be more stable during movements, like eyes, ears and nose.
:::

Last but not least, it is important to not only analytically evaluate the pose estimation results, but also to visually inspect the quality of the poses.
@tbl-results-ted-kid-videos shows the rendered videos for the seven pose estimators on the TED-kid video.

Looking at the rendered video of MediaPipePose, one can observe that the pose estimation is very unstable, with many jittery movements and sudden changes in the pose.
In the beginning of the video, the model fails to detect the right elbow joint, which all other pose estimators detect correctly.
Furthermore, especially the hip joints, ankles and elbows exhibit a lot of rapid, jerky movements.
Comparing the rendered videos of MaskAnyoneUI-MediaPipe and MaskAnyoneAPI-MediaPipe, one can observe that both variants are much more stable and less jittery than pure MediaPipe.
Except for the normal movement of the person, all keypoints stay in a fixed and stable position.
When closely observing the other pose estimators, it becomes apparent, that they are not as stable as MaskAnyoneUI-MediaPipe, but still more stable than pure MediaPipePose.
This observation confirms the results from table @tbl-results-ted-kid and @fig-ted-kid-plots, and validates that our approach of measuring the kinematic metrics is a good indicator for assessing the stability of the pose estimation.


::: {#tbl-results-ted-kid-videos}
```{=html}
<table class="video-table">
    <tr>
        <td>
            <div class="video-zoom-wrapper ted-kid-video">
                <video class="video-zoom ted-kid-sync" autoplay muted playsinline>
                    <source src="videos/TED-kid/TED-kid.mp4" type="video/mp4">
                </video>
            </div>
            <div class="video-caption">Raw Video</div>
        </td>
        <td>
            <div class="video-zoom-wrapper ted-kid-video">
                <video class="video-zoom ted-kid-sync" autoplay muted playsinline>
                    <source src="videos/TED-kid/TED-kid_YoloPose.mp4" type="video/mp4">
                </video>
            </div>
            <div class="video-caption">YOLOPose</div>
        </td>
    </tr>
    <tr>
        <td>
            <div class="video-zoom-wrapper ted-kid-video">
                <video class="video-zoom ted-kid-sync" autoplay muted playsinline>
                    <source src="videos/TED-kid/TED-kid_MediaPipePose.mp4" type="video/mp4">
                </video>
            </div>
            <div class="video-caption">MediaPipe Pose</div>
        </td>
        <td>
            <div class="video-zoom-wrapper ted-kid-video">
                <video class="video-zoom ted-kid-sync" autoplay muted playsinline>
                    <source src="videos/TED-kid/TED-kid_OpenPose.mp4" type="video/mp4">
                </video>
            </div>
            <div class="video-caption">OpenPose</div>
        </td>
    </tr>
    <tr>
        <td>
            <div class="video-zoom-wrapper ted-kid-video">
                <video class="video-zoom ted-kid-sync" autoplay muted playsinline>
                    <source src="videos/TED-kid/TED-kid_MaskAnyoneAPI-MediaPipe.mp4" type="video/mp4">
                </video>
            </div>
            <div class="video-caption">MaskAnyoneAPI-MediaPipe</div>
        </td>
        <td>
            <div class="video-zoom-wrapper ted-kid-video">
                <video class="video-zoom ted-kid-sync" autoplay muted playsinline>
                    <source src="videos/TED-kid/TED-kid_MaskAnyoneAPI-OpenPose.mp4" type="video/mp4">
                </video>
            </div>
            <div class="video-caption">MaskAnyoneAPI-OpenPose</div>
        </td>
    </tr>
    <tr>
        <td>
            <div class="video-zoom-wrapper ted-kid-video">
                <video class="video-zoom ted-kid-sync" autoplay muted playsinline>
                    <source src="videos/TED-kid/TED-kid_MaskAnyoneUI-MediaPipe.mp4" type="video/mp4">
                </video>
            </div>
            <div class="video-caption">MaskAnyoneUI-MediaPipe</div>
        </td>
        <td>
            <div class="video-zoom-wrapper ted-kid-video">
                <video class="video-zoom ted-kid-sync" autoplay muted playsinline>
                    <source src="videos/TED-kid/TED-kid_MaskAnyoneUI-OpenPose.mp4" type="video/mp4">
                </video>
            </div>
            <div class="video-caption">MaskAnyoneUI-OpenPose</div>
        </td>
    </tr>
</table>

<script>
document.addEventListener('DOMContentLoaded', function() {
    const videos = document.querySelectorAll('.ted-kid-sync');
    let endedCount = 0;

    function restartAllVideos() {
        videos.forEach(video => {
            video.currentTime = 0;
            video.play();
        });
        endedCount = 0;
    }

    videos.forEach(video => {
        video.addEventListener('ended', () => {
            endedCount++;
            if (endedCount === videos.length) {
                restartAllVideos();
            }
        });
    });
});
</script>
```
Rendered result videos of different pose estimators on the TED-kid video.
:::


## TED Talks {#sec-results-ted-talks}
The results on 10 full TED-Talk videos are very similar to the results on the single TED-kid video  as detailed in @tbl-results-ted-talks.
Among the evaluated pose estimators, MaskAnyoneUI-MediaPipe consistently demonstrated superior stability, achieving the lowest average velocity, acceleration, and jerk values of 1.25, 1.08, and 1.83 respectively.
MaskAnyoneAPI-MediaPipe followed, exhibiting the second-best performance in acceleration and jerk, closely trailed by YoloPose. OpenPose ranked next, and both MaskAnyone-OpenPose variants show an even higher instability than the pure OpenPose model.
Consistent with previous findings, MediaPipePose was identified as the most unstable pose estimator, registering the highest values across all metrics: 3.29 for velocity, 4.52 for acceleration, and 7.94 for jerk.
An additional observation is the clear trend that MediaPipe-based MaskAnyone variants generally outperform OpenPose-based variants in terms of stability, as indicated by their consistently lower velocity, acceleration, and jerk values.

::: {#tbl-results-ted-talks}
```{=html}
<table class="results"><thead>
  <tr>
    <th>Pose Estimator</th>
    <th>Velocity</th>
    <th>Acceleration</th>
    <th>Jerk</th>
  </tr></thead>
<tbody>
  <tr>
    <td>YoloPose</td>
    <td class="second">1.35</td>
    <td>1.46</td>
    <td>2.44</td>
  </tr>
  <tr>
    <td>MediaPipePose</td>
    <td>3.29</td>
    <td>4.52</td>
    <td>7.94</td>
  </tr>
  <tr>
    <td>OpenPose</td>
    <td>1.58</td>
    <td>2.22</td>
    <td>3.44</td>
  </tr>
  <tr class="maskanyone-api border-top">
    <td>MaskAnyoneAPI-MediaPipe</td>
    <td>1.44</td>
    <td class="second">1.25</td>
    <td class="second">2.04</td>
  </tr>
  <tr class="maskanyone-api border-bottom">
    <td>MaskAnyoneAPI-OpenPose</td>
    <td>2.30</td>
    <td>2.42</td>
    <td>4.07</td>
  </tr>
  <tr class="maskanyone-ui border-top">
    <td>MaskAnyoneUI-MediaPipe</td>
    <td class="best">1.25</td>
    <td class="best">1.08</td>
    <td class="best">1.83</td>
  </tr>
  <tr class="maskanyone-ui border-bottom">
    <td>MaskAnyoneUI-OpenPose</td>
    <td>2.07</td>
    <td>2.29</td>
    <td>3.72</td>
  </tr>
</tbody>
</table>
```
Average metric results for different pose estimators aggregated over all TED talk videos.
:::

::: {#fig-ted-talks-plots layout="[[1,1], [1]]"}

![Acceleration Distribution](plots/TED-talks/acceleration_distribution.png){#fig-ted-talks-acceleration-distribution}

![Jerk Distribution](plots/TED-talks/jerk_distribution.png){#fig-ted-talks-jerk-distribution}

![Median Acceleration per Keypoint](plots/TED-talks/keypoint_plot_acceleration.png){#fig-ted-kid-acceleration-per-keypoint}

**Comparison of pose estimation models on the TED talks dataset.**
(a, b) Acceleration and jerk distribution showing the proportion of keypoints within a fixed value ranges. 
A larger concentration at lower values indicates a smoother and more stable pose, whereas a high number of keypoints with large values indicates more jittery movements.
(c) Median acceleration per keypoint, indicating stability across individual body parts.
:::

@fig-ted-talks-acceleration-distribution and @fig-ted-talks-jerk-distribution show high acceleration and jerk values are more common in the TED talks than in the TED-kid video.
This is likely due to the fact, that each TED-talk contains camera movements, scene changes, parts where no person is visible or views of the audience, which are not present in the TED-kid video.
We included two particularly challenging chunks in @tbl-results-ted-videos.
In the first column, we present the performance of the qualitatively worst pose estimator, MediaPipePose, while the second column shows the result of the best performing pose estimator, MaskAnyoneUI-MediaPipe.

The woman in the first row [@ted-tarana] is wearing a long dress, the video contains many scene changes, views of the audience with the speaker in the background and also a part, where the speaking person is not visible.
MaskAnyone greatly improves the stability and visual accuracy of the pose estimation in all these cases.

In the second row [@ted-song], the main challenges are the rapidly changing camera views and the close-up shots of the singing woman.
Both MaskAnyoneUI-MediaPipe and raw MediaPipe struggle with close up shots of the hips and arms of the woman.
The model tries to fit an entire human pose into the small space of an arm or hip, which leads to an incorrect pose estimation and a lot of jitter.
It appears as if as soon as the model detects one joint of a human pose, it tries to detect the remaining parts as well.
This is a drawback, which we saw mostly for MediaPipe models (incl. MaskAnyone-MediaPipe variants) during our experiments and not for other pose estimators.
Nonetheless, MaskAnyoneUI-MediaPipe still achieves a more stable and accurate pose estimation than MediaPipePose for most frames in this video.

::: {#tbl-results-ted-videos}
```{=html}
<table class="video-table">
    <tr>
        <td>
            <div class="video-zoom-wrapper">
                <video class="video-zoom tarana-sync" autoplay muted playsinline>
                    <source src="videos/ted-talks/tarana_chunk17_MediaPipePose.mp4" type="video/mp4">
                </video>
            </div>
        </td>
        <td>
            <div class="video-zoom-wrapper">
                <video class="video-zoom tarana-sync" autoplay muted playsinline>
                    <source src="videos/ted-talks/tarana_chunk17_MaskAnyoneUI-MediaPipe.mp4" type="video/mp4">
                </video>
            </div>
        </td>
    </tr>
    <tr>
        <td>
            <div class="video-zoom-wrapper">
                <video class="video-zoom song-sync" autoplay muted playsinline>
                    <source src="videos/ted-talks/song_chunk1_MediaPipePose.mp4" type="video/mp4">
                </video>
            </div>
            <div class="video-caption">MediaPipePose</div>
        </td>
        <td>
            <div class="video-zoom-wrapper">
                <video class="video-zoom song-sync" autoplay muted playsinline>
                    <source src="videos/ted-talks/song_chunk1_MaskAnyoneUI-MediaPipe.mp4" type="video/mp4">
                </video>
            </div>
            <div class="video-caption">MaskAnyoneUI-MediaPipe</div>
        </td>
    </tr>
</table>

<script>
document.addEventListener('DOMContentLoaded', function() {
    const videos = document.querySelectorAll('.tarana-sync');
    let endedCount = 0;

    function restartAllVideos() {
        videos.forEach(video => {
            video.currentTime = 0;
            video.play();
        });
        endedCount = 0;
    }

    videos.forEach(video => {
        video.addEventListener('ended', () => {
            endedCount++;
            if (endedCount === videos.length) {
                restartAllVideos();
            }
        });
    });
});
document.addEventListener('DOMContentLoaded', function() {
    const videos = document.querySelectorAll('.song-sync');
    let endedCount = 0;

    function restartAllVideos() {
        videos.forEach(video => {
            video.currentTime = 0;
            video.play();
        });
        endedCount = 0;
    }

    videos.forEach(video => {
        video.addEventListener('ended', () => {
            endedCount++;
            if (endedCount === videos.length) {
                restartAllVideos();
            }
        });
    });
});
</script>
```
Two TED-talk chunks overlayed with MediaPipePose and MaskAnyoneUI-MediaPipe poses on two challenging chunks with scene changes, camera movements and parts where no person is visible.
The first row contains a chunk from the TED talk "Me Too is a movement, not a moment" [@ted-tarana] and the second row contains a chunk from the TED talk "Universe / Statues / Liberation" [@ted-song].
:::


## Tragic Talkers {#sec-results-tragic-talkers}
@tbl-results-tragic-talkers shows the average metric results for different pose estimators on the Tragic Talkers dataset.

With respect to the accuracy of the pose estimation compared to the pseudo-ground truth, YoloPose achieves the best result with a PCK of 96%, followed by OpenPose with 87%.
Except for MediaPipePose, which detects only 69% of the keypoints correctly, all pose estimators achieve a PCK of more than 83%.

However, the PCK and RMSE accuracy of the pose estimation is misleading, as the pseudo-ground truth poses were generated by an AI model instead of human annotators, and are therefore imperfect.
One should therefore interpret our results as how closely the pose estimators can estimate the OpenPose output, rather than the quality of the pose estimation itself.
Because it is neither outlined by @tragic-talkers, which variant of OpenPose was used, nor the exact parameters of the pose estimation or any post-processing steps, we consider a PCK accuracy of more than 80% as a good result, indicating that the poses are generally well estimated.

The results are clearer for the kinematic metrics, especially acceleration and jerk. 
MaskAnyoneAPI-MediaPipe performs the best, achieving the lowest acceleration and jerk values with 2.86 pixels/frameÂ² and 5.01 pixels/frameÂ³ respectively.
MaskAnyoneUI-MediaPipe achieves the second best results (3.26 pixels/frameÂ² for acceleration and 5.10 pixels/frameÂ³ for jerk), followed by YoloPose with an acceleration value of 3.27 and jerk of 5.57.
The MaskAnyone-OpenPose variants still exhibit less acceleration and jerk than standard OpenPose, but can already be considered quite jerky with acceleration values between 5.12 and 6.08 and jerk values of 9.06 to 10.22.
Pure MediaPipePose is once again the most unstable pose estimator, achieving an average acceleration of 9.80 pixels/frameÂ² and a jerk of 17.58 pixels/frameÂ³.

::: {#tbl-results-tragic-talkers}
```{=html}
<table class="results"><thead>
  <tr>
    <th>Pose Estimator</th>
    <th>PCK</th>
    <th>RMSE</th>
    <th>Velocity</th>
    <th>Acceleration</th>
    <th>Jerk</th>
  </tr></thead>
<tbody>
  <tr>
    <td>YoloPose</td>
    <td class="best">0.96</td>
    <td class="second">0.11</td>
    <td>4.36</td>
    <td>3.27</td>
    <td>5.57</td>
  </tr>
  <tr>
    <td>MediaPipePose</td>
    <td>0.69</td>
    <td>0.47</td>
    <td>6.48</td>
    <td>9.80</td>
    <td>17.58</td>
  </tr>
  <tr>
    <td>OpenPose</td>
    <td class="second">0.87</td>
    <td>0.33</td>
    <td>4.63</td>
    <td>6.38</td>
    <td>10.00</td>
  </tr>
  <tr class="maskanyone-api border-top">
    <td>MaskAnyoneAPI-MediaPipe</td>
    <td>0.78</td>
    <td>0.12</td>
    <td class="second">3.46</td>
    <td class="best">2.86</td>
    <td class="best">5.01</td>
  </tr>
  <tr class="maskanyone-api border-bottom">
    <td>MaskAnyoneAPI-OpenPose</td>
    <td>0.85</td>
    <td>0.36</td>
    <td>5.69</td>
    <td>6.08</td>
    <td>10.22</td>
  </tr>
  <tr class="maskanyone-ui border-top">
    <td>MaskAnyoneUI-MediaPipe</td>
    <td>0.83</td>
    <td class="best">0.07</td>
    <td class="best">3.26</td>
    <td class="second">2.91</td>
    <td class="second">5.10</td>
  </tr>
  <tr class="maskanyone-ui border-bottom">
    <td>MaskAnyoneUI-OpenPose</td>
    <td>0.85</td>
    <td>0.36</td>
    <td>5.53</td>
    <td>5.12</td>
    <td>9.06</td>
  </tr>
</tbody>
</table>
```
Average metric results for different pose estimators aggregated over four camera angles of five Tragic Talkers sequences with pseudo-ground truth.
:::

@fig-tragic-talkers-acceleration-distribution and @fig-tragic-talkers-jerk-distribution confirm the results from @tbl-results-tragic-talkers.
Both plots show that the MaskAnyone-MediaPipe pose estimators achieve the highest proportion of low acceleration and jerk values, followed by YoloPose, the MaskAnyone-OpenPose pose estimators and OpenPose.
MediaPipePose once again has a very flat curve, indicating a lot of large acceleration and jerk values.

@fig-tragic-talkers-acceleration-per-keypoint shows, that MediaPipePose is among the pose estimators with the highest median acceleration values for all keypoints.
YoloPose, MaskAnyoneAPI-MediaPipe and MaskAnyoneUI-MediaPipe achieve consistently low median acceleration values for all keypoints.

::: {#fig-tragic-talkers-plots layout="[[1,1], [1]]"}

![Acceleration Distribution](plots/TragicTalkers/acceleration_distribution.png){#fig-tragic-talkers-acceleration-distribution}

![Jerk Distribution](plots/TragicTalkers/jerk_distribution.png){#fig-tragic-talkers-jerk-distribution}

![Median Acceleration per Keypoint](plots/TragicTalkers/keypoint_plot_Acceleration.png){#fig-tragic-talkers-acceleration-per-keypoint}

**Comparison of pose estimation models on the Tragic Talkers dataset.**
(a, b) Acceleration and jerk distribution showing the proportion of keypoints within a fixed value ranges. 
A larger concentration at lower values indicates a smoother and more stable pose, whereas a high number of keypoints with large values indicates more jittery movements.
(c) Median acceleration per keypoint, indicating stability across individual body parts.
:::

Interestingly, although the MaskAnyone-OpenPose pose estimators achieve lower acceleration values for nose, eye, ear, shoulder and ankle keypoints than pure OpenPose, they perform worse for the elbow, hip and knee keypoints.
A potential reason for this could be that MaskAnyone uses a higher confidence threshold for keypoints than our OpenPose implementation, which leads to the elbow, hip and knee keypoints not being detected nor rendered.
As an example, consider @tbl-results-tragic-talkers-bad-legs, which shows the first seconds of the rendered video for OpenPose, MaskAnyoneAPI-OpenPose and MaskAnyoneUI-OpenPose for the "conversation1_t3-cam08" sequence.

::: {#tbl-results-tragic-talkers-bad-legs}
```{=html}
<table class="video-table">
    <tr>
        <td>
            <div class="video-zoom-wrapper">
                <video class="video-zoom tt-legs-sync" autoplay muted playsinline>
                    <source src="videos/TragicTalkers/conversation1_t3-cam08/conversation1_t3-cam08_OpenPose.mp4#t=0,10" type="video/mp4">
                </video>
            </div>
            <div class="video-caption">OpenPose</div>
        </td>
        <td>
            <div class="video-zoom-wrapper">
                <video class="video-zoom tt-legs-sync" autoplay muted playsinline>
                    <source src="videos/TragicTalkers/conversation1_t3-cam08/conversation1_t3-cam08_MaskAnyoneAPI-OpenPose.mp4#t=0,10" type="video/mp4">
                </video>
            </div>
            <div class="video-caption">MaskAnyoneAPI-OpenPose</div>
        </td>
        <td>
            <div class="video-zoom-wrapper">
                <video class="video-zoom tt-legs-sync" autoplay muted playsinline>
                    <source src="videos/TragicTalkers/conversation1_t3-cam08/conversation1_t3-cam08_MaskAnyoneUI-OpenPose.mp4#t=0,10" type="video/mp4">
                </video>
            </div>
            <div class="video-caption">MaskAnyoneUI-OpenPose</div>
        </td>
    </tr>
</table>

<script>
document.addEventListener('DOMContentLoaded', function() {
    const videos = document.querySelectorAll('.tt-legs-sync');
    let videosReady = 0;
    
    function checkAllVideosEnded(video) {
        if (video.currentTime >= 10) {  // Check if we've reached the fragment end time
            videosReady++;
            if (videosReady === videos.length) {
                videos.forEach(v => {
                    v.currentTime = 0;
                    v.play();
                });
                videosReady = 0;
            }
        }
    }

    videos.forEach(video => {
        video.addEventListener('timeupdate', () => checkAllVideosEnded(video));
    });
});
</script>
```
First 10 seconds of the rendered Tragic Talkers videos for OpenPose, MaskAnyoneAPI-OpenPose and MaskAnyoneUI-OpenPose for the "conversation1_t3-cam08" sequence.
:::

Last but not least, we want to qualitatively compare MaskAnyoneAPI-MediaPipe, MaskAnyoneUI-MediaPipe and YoloPose on the "interactive4_t3-cam08" sequence (@tbl-results-tragic-talkers-best-models).
These three pose estimators have the lowest overall average acceleration values, according to @tbl-results-tragic-talkers.
We noticed two important aspects:

1. YoloPose is the only pose estimator that correctly identifies the woman, when she is turning around, facing with her back to the camera. Both MaskAnyone pose estimators fail to do so. 
2. Directly at the start of the sequence, where both actors stand with their hands stretched out, only YoloPose correctly identifies the lower part of the woman's body. Both MaskAnyone pose estimators show a completely incorrect pose in the upper part of the woman's body. As soon as she is taking her arms down towards the hanging position, the pose adapts until it "locks on" to the correct position.

Qualitatively, this makes YoloPose the best pose estimator for this sequence.


::: {#tbl-results-tragic-talkers-best-models}
```{=html}
<table class="video-table">
    <tr>
        <td>
            <div class="video-zoom-wrapper">
                <video class="video-zoom tt-best-models-sync" autoplay muted playsinline>
                    <source src="videos/TragicTalkers/interactive4_t3-cam08/interactive4_t3-cam08_YoloPose.mp4" type="video/mp4">
                </video>
            </div>
            <div class="video-caption">YoloPose</div>
        </td>
        <td>
            <div class="video-zoom-wrapper">
                <video class="video-zoom tt-best-models-sync" autoplay muted playsinline>
                    <source src="videos/TragicTalkers/interactive4_t3-cam08/interactive4_t3-cam08_MaskAnyoneAPI-MediaPipe.mp4" type="video/mp4">
                </video>
            </div>
            <div class="video-caption">MaskAnyoneAPI-MediaPipe</div>
        </td>
        <td>
            <div class="video-zoom-wrapper">
                <video class="video-zoom tt-best-models-sync" autoplay muted playsinline>
                    <source src="videos/TragicTalkers/interactive4_t3-cam08/interactive4_t3-cam08_MaskAnyoneUI-MediaPipe.mp4" type="video/mp4">
                </video>
            </div>
            <div class="video-caption">MaskAnyoneUI-MediaPipe</div>
        </td>
    </tr>
</table>

<script>
document.addEventListener('DOMContentLoaded', function() {
    const videos = document.querySelectorAll('.tt-best-models-sync');
    let endedCount = 0;

    function restartAllVideos() {
        videos.forEach(video => {
            video.currentTime = 0;
            video.play();
        });
        endedCount = 0;
    }

    videos.forEach(video => {
        video.addEventListener('ended', () => {
            endedCount++;
            if (endedCount === videos.length) {
                restartAllVideos();
            }
        });
    });
});
</script>
```
The most stable pose estimators on the Tragic Talkers "interactive4_t3-cam08" sequence.
:::



## Inference on Raw vs. Masked Videos {#sec-results-inference-raw-masked}
::: {#tbl-pck-raw-masked}
```{=html}
<table class="results raw-masked-table">
  <thead>
    <tr>
      <th>Pose Estimator</th>
      <th>Blurring</th>
      <th>Pixelation</th>
      <th>Contours</th>
      <th>Solid Fill</th>
      <th>Average</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>YoloPose</td>
      <td class="best">0.95</td>
      <td>0.09</td>
      <td class="best">0.93</td>
      <td class="second">0.32</td>
      <td class="second">0.57</td>
    </tr>
    <tr>
      <td>MediaPipePose</td>
      <td class="best">0.95</td>
      <td class="best">0.81</td>
      <td>0.56</td>
      <td class="best">0.34</td>
      <td class="best">0.67</td>
    </tr>
    <tr>
      <td>OpenPose</td>
      <td class="second">0.88</td>
      <td>0.10</td>
      <td class="second">0.62</td>
      <td>0.01</td>
      <td>0.40</td>
    </tr>
    <tr class="maskanyone-api border-top">
      <td>MaskAnyoneAPI-MediaPipe</td>
      <td>0.85</td>
      <td>0.30</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.29</td>
    </tr>
    <tr class="maskanyone-api border-bottom">
      <td>MaskAnyoneAPI-OpenPose</td>
      <td>0.75</td>
      <td>0.00</td>
      <td>0.36</td>
      <td>0.00</td>
      <td>0.28</td>
    </tr>
    <tr class="maskanyone-ui border-top">
      <td>MaskAnyoneUI-MediaPipe</td>
      <td class="best">0.95</td>
      <td class="second">0.63</td>
      <td>0.00</td>
      <td>0.07</td>
      <td>0.41</td>
    </tr>
    <tr class="maskanyone-ui border-bottom">
      <td>MaskAnyoneUI-OpenPose</td>
      <td>0.87</td>
      <td>0.03</td>
      <td>0.58</td>
      <td>0.00</td>
      <td>0.37</td>
    </tr>
    <tr>
      <td>Average</td>
      <td>0.86</td>
      <td>0.23</td>
      <td>0.44</td>
      <td>0.11</td>
      <td>/</td>
    </tr> 
  </tbody>
</table>
```
Percentage of correct keypoints (PCK) for different pose estimators on videos masked by different hiding strategies.
:::


::: {#tbl-rmse-raw-masked}
```{=html}
<table class="results raw-masked-table">
  <thead>
    <tr>
      <th>Pose Estimator</th>
      <th>Blurring</th>
      <th>Pixelation</th>
      <th>Contours</th>
      <th>Solid Fill</th>
      <th>Average</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>YoloPose</td>
      <td class="second">0.12</td>
      <td>0.92</td>
      <td class="best">0.13</td>
      <td class="second">0.74</td>
      <td class="second">0.48</td>
    </tr>
    <tr>
      <td>MediaPipePose</td>
      <td class="second">0.12</td>
      <td class="best">0.26</td>
      <td>0.49</td>
      <td class="best">0.64</td>
      <td class="best">0.38</td>
    </tr>
    <tr>
      <td>OpenPose</td>
      <td>0.25</td>
      <td>0.94</td>
      <td class="second">0.47</td>
      <td>1.0</td>
      <td>0.67</td>
    </tr>
    <tr class="maskanyone-api border-top">
      <td>MaskAnyoneAPI-MediaPipe</td>
      <td>0.27</td>
      <td>0.74</td>
      <td>1.00</td>
      <td>0.99</td>
      <td>0.75</td>
    </tr>
    <tr class="maskanyone-api border-bottom">
      <td>MaskAnyoneAPI-OpenPose</td>
      <td>0.43</td>
      <td>0.99</td>
      <td>0.75</td>
      <td>1.00</td>
      <td>0.79</td>
    </tr>
    <tr class="maskanyone-ui border-top">
      <td>MaskAnyoneUI-MediaPipe</td>
      <td class="best">0.07</td>
      <td class="second">0.41</td>
      <td>1.00</td>
      <td>0.94</td>
      <td>0.60</td>
    </tr>
    <tr class="maskanyone-ui border-bottom">
      <td>MaskAnyoneUI-OpenPose</td>
      <td>0.24</td>
      <td>0.98</td>
      <td>0.52</td>
      <td>1.00</td>
      <td>0.69</td>
    </tr>
    <tr>
      <td>Average</td>
      <td>0.21</td>
      <td>0.78</td>
      <td>0.62</td>
      <td>0.90</td>
      <td>/</td>
    </tr> 
  </tbody>
</table>
```
Root mean square error (RMSE) for different pose estimators on videos masked by different hiding strategies.
:::

As described in @sec-datasets-masked-video, we masked three different videos with four hiding strategies.
@tbl-pck-raw-masked and @tbl-rmse-raw-masked show the percentage of correct keypoints (PCK) and the root mean square error (RMSE) for different pose estimators on the masked videos compared to the original videos.

**Comparison of pose estimators**

MediaPipePose achieves the highest average PCK of 67% and the lowest average RMSE of 0.38, indicating robustness across all hiding strategies.
YoloPose also performs well with an average PCK of 57%, especially on the blurring and contours hiding strategies, where it detects 95% and 93% of the keypoints from the original videos.
In contrast, the performance of OpenPose is weaker, resulting in an average PCK of only 40% and a high RMSE of 0.67.

Among the MaskAnyone variants, the UI-based models generally outperformed API-based ones, especially MaskAnyoneUI-MediaPipe, which reached a moderate PCK of 41% and RMSE of 0.6. 
The API-variants of MaskAnyone perform poor, with an average PCK of 28% and 29%, suggesting that human input is required to improve the performance on masked videos.
However, contrary to the other datasets,the UI-variants of MaskAnyone do not enhance the performance of the pure AI-models, but rather degrade it.
A possible reason for this is that MaskAnyone uses a higher confidence threshold for keypoints than the pure AI-models we implemented, leading to the keypoints not being detected.
Furhthermore, we hypothesize that if the first stage of MaskAnyone (where YoloPose detects the person) performs poorly, the second stage (using SAM2 [@sam2] to segment the person and crop it) will also perform poorly, leading to low quality input to the third stage, where the pose estimator is applied.


**Comparison of hiding strategies**

Last but not least, we want to compare the hiding strategies with regards to balancing privacy and performance of pose estimators on masked videos.

Blurring achieved the highest average PCK over all pose estimators, with a value of 86%.
This indicates, that models are able to detect the person in the video very well, although it is obstructed by blurring.
This observation provides interesting insights into the working principles of pose estimation AI models.
Although not all joints or face keypoints are visible any more, the models still makes predictions for the keypoints, which is an indicator that not the visibility of individual body parts, but the overall body structure and context plays a central role in pose estimation. 
The models likely infer joint positions by leveraging spatial relationships and body priors learned during training, such as limb proportions, symmetry, and typical human postures.
For instance, even when the eyes or hands are blurred or occluded, as seen in the contour and blur examples, the surrounding geometry (e.g., head, shoulders or arm direction) provides enough contextual cues for the models to estimate keypoints.
This suggests that the pose estimators rely heavily on learned pose patterns rather than pixel-level detail.

The results for other hiding strategies are more mixed.
While YoloPose achieves an astonishing 93% PCK on videos masked with contours, indicating that it leverages edge and shape information of different body parts rather than detailed texture and color information, the other pose estimators perform poorly on this hiding strategy.
Both MaskAnyone-API variants detect nothing, while the remaining pose estimators achieve a PCK of 36% to 62%.

For pixelation, only MediaPipePose is able to detect the persons reasonably well, with a PCK of 81%.
YoloPose, OpenPose and both MaskAnyone-OpenPose variants detect less than 10% of the keypoints.
This signals, that the used level of pixelation greatly reduces the amount of information available to the pose estimators and that the current implementation is not suitable for pose estimation.

The solid fill hiding strategy is the most challenging one, because it removes all information about the person, except for the outline.
Consequently the average PCK is the lowest of all hiding strategies, with a value of 11%.
The best model, MediaPipePose, achieves a PCK of 34% on this hiding strategy.

As a conclusion, we can say that blurring provides the best compromise between privacy and performance of pose estimators on masked videos.
Although it might not completely de-identify the person, it provides enough information for the pose estimators to make reasonable predictions.
The contours hiding strategy can be used, if even more privacy is desired, at the expense of pose estimation accuracy, except for YoloPose.

@tbl-results-raw-masked-rendered shows the qualitative results of the best performing pose estimators on the masked videos of the TED sequence "Let curiosity lead" and the Tragic Talkers "interactive1_t1-cam06" sequence.


::: {#tbl-results-raw-masked-rendered}
```{=html}
<style>
/* Style for raw-masked videos table */
.video-table {
    border-collapse: collapse; /* Remove spacing between cells */
}
.video-table td {
    text-align: center;
    vertical-align: middle;
    padding: 4px; /* Minimal padding around cells */
}
.video-table .video-zoom-wrapper {
    display: flex;
    justify-content: center;
    align-items: center;
}
.video-table .video-zoom-wrapper video {
    height: 200px; /* Fixed height for all videos in this table */
    width: auto;   /* Maintain aspect ratio */
}
</style>

<table class="video-table">
    <tr>
        <td>
            <div class="video-zoom-wrapper">
                <video class="video-zoom ted-raw-masked-sync" autoplay muted playsinline>
                    <source src="videos/raw-masked/Blurring-TED-curiosity-chunk_YoloPose.mp4" type="video/mp4">
                </video>
                <!-- <div class="video-caption">YoloPose</div> -->
            </div>
        </td>
        <td>
            <div class="video-zoom-wrapper">
                <video class="video-zoom tt-raw-masked-sync" autoplay muted playsinline>
                    <source src="videos/raw-masked/Blurring-TT-interactive1_t1-cam06_YoloPose.mp4" type="video/mp4">
                </video>
                <!-- <div class="video-caption">YoloPose</div> -->
            </div>
        </td>
    </tr>
    <tr>
        <td>
            <div class="video-zoom-wrapper">
                <video class="video-zoom ted-raw-masked-sync" autoplay muted playsinline>
                    <source src="videos/raw-masked/Pixelation-TED-curiosity-chunk_MaskAnyoneUI-MediaPipe.mp4" type="video/mp4">
                </video>
                <!-- <div class="video-caption">MaskAnyoneUI-MediaPipe</div> -->
            </div>
        </td>
        <td>
            <div class="video-zoom-wrapper">
                <video class="video-zoom tt-raw-masked-sync" autoplay muted playsinline>
                    <source src="videos/raw-masked/Pixelation-TT-interactive1_t1-cam06_MediaPipePose.mp4" type="video/mp4">
                </video>
                <!-- <div class="video-caption">MediaPipePose</div> -->
            </div>
        </td>
    </tr>
    <tr>
        <td>
            <div class="video-zoom-wrapper">
                <video class="video-zoom ted-raw-masked-sync" autoplay muted playsinline>
                    <source src="videos/raw-masked/Contours-TED-curiosity-chunk_YoloPose.mp4" type="video/mp4">
                </video>
                <!-- <div class="video-caption">YoloPose</div> -->
            </div>
        </td>
        <td>
            <div class="video-zoom-wrapper">
                <video class="video-zoom tt-raw-masked-sync" autoplay muted playsinline>
                    <source src="videos/raw-masked/Contours-TT-interactive1_t1-cam06_YoloPose.mp4" type="video/mp4">
                </video>
                <!-- <div class="video-caption">YoloPose</div> -->
            </div>
        </td>
    </tr>
    <tr>
        <td>
            <div class="video-zoom-wrapper">
                <video class="video-zoom ted-raw-masked-sync" autoplay muted playsinline>
                    <source src="videos/raw-masked/SolidFill-TED-curiosity-chunk_YoloPose.mp4" type="video/mp4">
                </video>
                <!-- <div class="video-caption">YoloPose</div> -->
            </div>
        </td>
        <td>
            <div class="video-zoom-wrapper">
                <video class="video-zoom tt-raw-masked-sync" autoplay muted playsinline>
                    <source src="videos/raw-masked/SolidFill-TT-interactive1_t1-cam06_MaskAnyoneUI-MediaPipe.mp4" type="video/mp4">
                </video>
                <!-- <div class="video-caption">MaskAnyoneUI-MediaPipe</div> -->
            </div>
        </td>
    </tr>
</table>

<script>
document.addEventListener('DOMContentLoaded', function() {
    const videos = document.querySelectorAll('.ted-raw-masked-sync');
    let endedCount = 0;

    function restartAllVideos() {
        videos.forEach(video => {
            video.currentTime = 0;
            video.play();
        });
        endedCount = 0;
    }

    videos.forEach(video => {
        video.addEventListener('ended', () => {
            endedCount++;
            if (endedCount === videos.length) {
                restartAllVideos();
            }
        });
    });
});

document.addEventListener('DOMContentLoaded', function() {
    const videos = document.querySelectorAll('.tt-raw-masked-sync');
    let endedCount = 0;

    function restartAllVideos() {
        videos.forEach(video => {
            video.currentTime = 0;
            video.play();
        });
        endedCount = 0;
    }

    videos.forEach(video => {
        video.addEventListener('ended', () => {
            endedCount++;
            if (endedCount === videos.length) {
                restartAllVideos();
            }
        });
    });
});
</script>
```
The qualitatively best performing pose estimators on the masked videos of the TED sequence "Let curiosity lead" and the Tragic Talkers "interactive1_t1-cam06" sequence.
The rows contain videos masked with different hiding strategies in the following order: Blurring, Pixelation, Contours, Solid Fill.
The pose estimators shown for the TED sequence (first column) are YoloPose, MaskAnyoneUI-MediaPipe, YoloPose and YoloPose.
The pose estimators shown for the Tragic Talkers sequence (second column) are YoloPose, MediaPipePose, YoloPose and MaskAnyoneUI-MediaPipe.
:::



# Future Work & Limitations {#sec-future-work}
(Zainab - 4.)
(Tim - 5.)

Implement head and torso normalization. 


## MaskBench Outlook
With our promising results, we are laying the foundation for a more feature-rich benchmarking framework for pose estimation on masked videos.
There are several aspects and directions in which we want to extend our work.

### Pipelining {#sec-future-work-pipelining}
Currently, MaskBench supports only one workflow, which is to perform inference on a set of videos, evaluate them with metrics, visualize the results and render the videos with the poses.
As shown with the experiment on the masked video dataset, there are many more sets of pipelines, which could be executed.
Our idea is to add an extensible and customizable pipeline class to MaskBench, which defines a certain workflow (i.e. our current workflow, the masked video dataset workflow, or other, yet to be defined workflows), that would chain the components of MaskBench together in a certain way, reusing the existing components and adding new ones as needed.

Specifically for the masked video dataset workflow, the pipeline could look like this:

1. Perform inference on the raw videos with all pose estimators.
2. Evaluate the results with metrics.
3. Visualize the results of the pose estimators on the raw videos in plots or tables.
4. Render the raw videos with the poses.
5. Reuse the SAM2 masks provided by MaskAnyone to mask the videos with different hiding strategies. The masking parameters can be configured by the user to evaluate not only different hiding strategies, but also different levels of masking, giving more insights into which degree of which hiding strategy balances privacy and performance of the pose estimators the best.
6. For each pose estimator, execute it on all videos of all hiding strategies.
7. Evaluate the results with metrics.
8. Visualize the results of the pose estimators on the masked videos in plots or tables.
9. Render the masked videos with the poses.

### Evaluation of downstream tasks {#sec-future-work-downstream-tasks}
Estimating the pose of a person can be a preliminary step for many downstream tasks, like gesture recognition, 3D human reconstruction, action classification and many more.
MaskBench could be extended to support the evaluation of different models on downstream tasks for raw or masked videos, giving researchers more insights into which up-stream pose estimator to use for a specific downstream task.
In general, researchers could use MaskBench to mask their datasets, obstructing the identity of the person in the videos, and provide the pose outputs from the raw videos in addition to the masked videos to other researchers.
These could use the pose outputs along with the masked videos for their downstream tasks.
MaskBench should support an evaluation framework to assess the potential performance loss of downstream tasks when using masked videos.

### User interface
Adding a user interface to MaskBench in the form of a web application would allow for a more user-friendly experience.
The current workflow requires technical knowledge to run docker containers, set environment variables and edit the configuration files.
The aim of the user interface would be to provide a simple and intuitive way to configure and run the pipeline, visualize the results and see the rendered videos.

### Additional improvements
In this section, we want to list some minor improvements, which can be implemented in the future:

- Currently, the Euclidean distance metric only supports normalization by bounding box.
Implementing head and torso normalization would allow for a more accurate evaluation of pose estimators, resolving the shorter-limb problem described in @sec-metrics-euclidean-distance.
This requires knowing the index of the head and torso keypoints.
Care should be taken to not restraint the whole application from working on a specific keypoint set (like COCO) but still maintain flexibility in supporting various keypoint formats.
- Adding a logger to MaskBench would allow for a cleaner and more consistent terminal output for the user.
For debugging purposes, it would be helpful to add an option to also show all output of the other docker containers to pin down errors during development.
- Currently, MaskBench does not support face and hand keypoints.
Introducing this feature would allow to evaluate a greater set of downstream tasks.
- Adding more metrics and plots, especially for the evaluation of ground-truth independent aspects of pose estimation can help to better understand the performance of pose estimators.
Such metrics could, for example, measure, whether an estimated pose is physically plausible, given the constraints of the human body.
This would be a step towards evaluating the plausibility of poses rather than their pure quality in terms of acceleration, jerk and jitter.
- As a long-term goal, MaskBench could also support either 3D human pose estimation or the projection of 3D ground truth pose keypoints onto a 2D image plane using camera calibration parameters.
This would alleviate the issue of evaluating pose estimators on pseudo-ground truth data or 2D ground truth data from human annotators.
Instead, marker-based motion capture data could be used to assess the quality of marker-less pose estimators on real-world data, which promises to be more accurate than 2D ground truth data.
An example for such a dataset is the BioCV dataset published by the University of Barth [@bio-cv].

# Conclusion {#sec-conclusion}
This work introduced MaskBench, a modular and extensible benchmarking framework for evaluating pose estimation models under diverse conditions, including privacy-preserving masking strategies.
We evaluated four datasets of increasing complexity, including real-world TED Talk recordings to examine how models perform in unconstrained, natural scenarios rather than under controlled laboratory conditions.
The study includes popular pose estimators such as YoloPose [@yolo], MediaPipe [@mediapipe], and OpenPose [@openpose-1], alongside the mixture-of-expert-model pipeline MaskAnyone [@schilling2023maskanyone], to assess their performance across these varied settings.

Our quantitative evaluation, using acceleration and jerk metrics to measure temporal stability, showed that the MaskAnyone pipeline, particularly the human-in-the-loop MediaPipe variant, substantially improves stability by reducing acceleration and jerk compared to standard models.
YoloPose was the most robust standalone estimator, while MediaPipePose consistently exhibited the highest instability.
Visual inspection of the output poses confirmed these findings, with noticeably smoother and more consistent motion in cases where the metrics indicated high stability.

Our small study on masked videos revealed that blurring offers the best trade-off between privacy and accuracy, maintaining high PCK values across models, whereas pixelation and solid fills significantly degraded performance.
Model-specific responses to masking strategies highlighted that pose estimation often relies more on overall body structure than on pixel-level detail.

While results are promising, limitations remain, including reliance on pseudo-ground truth in some datasets and the preliminary implementation of masked-video workflows.
Future work will focus on extending MaskBench with flexible pipelining, downstream task evaluation, and user-friendly interfaces, enabling systematic exploration of how privacy-preserving transformations affect pose estimation and subsequent applications.


# References


