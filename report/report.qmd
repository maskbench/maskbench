---
title: "MaskBench - A Comprehensive Benchmark Framework for Video De-Identification"
date: "2025 08 08"
author:
    - name: Tim Riedel
      affiliation: Hasso Plattner Institute, University of Potsdam, Germany
    - name: Zainab Zafari
      affiliation: Hasso Plattner Institute, University of Potsdam, Germany
    - name: Sharjeel Shaik
      affiliation: University of Potsdam, Germany
    - name: Babajide Alamu Owoyele
      affiliation: Hasso Plattner Institute, University of Potsdam, Germany
    - name: Wim Pouw
      affiliation: Tilburg University, Netherlands
contact:
    - name: Tim Riedel
      email: tim.riedel@student.hpi.de
    - name: Zainab Zafari
      email: zainab.zafari@student.hpi.de
    - name: Babajide Alamu Owoyele
      email: babajide.owoyele@student.hpi.de
    - name: Wim Pouw
      email: w.pouw@tilburguniversity.edu
bibliography: dependencies/refs.bib
css: dependencies/styles.css
theme: journal
format:
    html:
        toc: true
        toc-location: left
        toc-title: "Contents"
        toc-depth: 3
        number-sections: true
        code-fold: true
        code-tools: true
filters:
    - include-code-files
# engine: knitr
jupyter: python3
---

# Abstract / Overview {#sec-abstract}
(Tim - 6.)



# Getting Started {#sec-installation}
(Zainab - 3.)

## Installation / Setup {#sec-installation-setup}

## Usage {#sec-installation-usage}

## Configuration of Experiments {#sec-installation-configuration}




# Introduction {#sec-introduction}
(Zainab - 5.)



# Related Work {#sec-related-work}
Human pose estimation aims to localize body joints (e.g., shoulders, elbows, knees) from visual input such as images or videos. Existing methods are generally categorized into two main strategies: top-down and bottom-up. In the top-down approach, human instances are first detected in the image, and pose estimation is then performed within each bounding box. In contrast, the bottom-up approach detects all keypoints in the image first and subsequently associates them with individual persons[@saiwa2025openpose], [@kaim2024comparison].

Several studies have benchmarked popular pose estimation models across different datasets, conditions, and use cases. The following works are particularly relevant to our benchmarking framework:

* **Comparision Of ML Models For Posture** [@kaim2024comparison]
conducted a detailed empirical comparison between YOLOv7 Pose and MediaPipe Pose using both image (COCO) and video (Penn Action) datasets. The study evaluated key metrics such as keypoint accuracy (PCK@0.2), inference speed, and robustness under challenging conditions like low light and occlusion.
    YOLOv7 achieved a slightly higher accuracy score of 87.8\% versus MediaPipe’s 84.1\%. However, MediaPipe demonstrated superior real-time performance on CPU-only devices, achieving 16--18 frames per second (FPS) compared to YOLOv7’s 4--5 FPS. In low-light environments, MediaPipe maintained detection consistency, whereas YOLOv7 performed better in occluded scenarios, successfully recognizing hidden body parts. The study highlights a trade-off: YOLOv7 is more robust under visually difficult conditions, while MediaPipe excels in lightweight, single-person use cases with real-time requirements.

* **OpenPose vs MediaPipe: A Practical and Architectural Comparison** [@saiwa2025openpose]
  A recent blog post by Saiwa presents a detailed comparison between OpenPose and MediaPipe, discussing their architectural differences, device compatibility, and practical applications. OpenPose uses a bottom-up approach with Part Affinity Fields and is optimized for multi-person full-body tracking, whereas MediaPipe follows a top-down strategy focusing on speed and cross-platform deployment. MediaPipe's modular design and high CPU efficiency make it suitable for real-time mobile apps, while OpenPose provides detailed skeletal tracking but at higher computational cost.

* **YOLOv7 Pose vs MediaPipe in Human Pose Estimation in videos** [@learnopencv2022yolopose] 
This benchmark-focused blog compares YOLOv7 Pose and MediaPipe on a variety of video scenarios such as sports, dance, crowd scenes, and occlusions. The comparison is qualitative and highlights where each model performs better. MediaPipe excels in handling low-resolution input, achieves faster CPU inference, and is well-suited for detecting distant individuals. However, it is limited to single-person detection. YOLOv7 performs better in occlusion-heavy scenarios, supports multi-person estimation, and handles fast motion more accurately, especially with high-resolution input and GPU support. The study underscores that MediaPipe is better for lightweight, real-time single-user applications, while YOLOv7 is better suited for more complex, high-performance environments.

# MaskBench Architecture {#sec-architecture}
Figure of architecture & workflow (Zainab - 2.)

The general workflow of MaskBench is to first load the dataset, pose estimators and evaluation metrics.
The application creates a checkpoint folder in the specified `/output` directory, named after the dataset and a timestamp (e.g. `/output/TedTalks-20250724-121127`).
Thereafter, inference is performed on all videos of the dataset using the pose estimators specified in the configuration file. 
A `poses` folder is created within the checkpoint, with a subfolder for each pose estimator and a single json file for each video.
Thereafter, the application evaluates all the specified metrics and visualizes them in plots, which are stored in the `plots` folder in the checkpoint.
Lastly, for each video, the application creates a multiple rendered video, one for each pose estimator, which are stored in the `renderings` folder in the checkpoint.

Each component of MaskBench is implemented in a modular way, so that it can be easily extended and modified, which we will discuss in the following sections.

## Dataset {#sec-architecture-dataset}
The dataset provides the video data for the pose estimation and ground truth data for the evaluation, if available.
When adding a new dataset, the user needs to create a new class that inherits from the `Dataset` class and overwrite the `_load_samples` method, which creates one `VideoSample` object for each video in the dataset.
If the dataset provides ground truth data, the user additionally needs to overwrite the `get_gt_pose_results` and `get_gt_keypoint_pairs` methods.
For each video, the `get_gt_pose_results` method should return a `VideoPoseResult` object.
The `get_gt_keypoint_pairs` method is used for rendering the ground truth keypoints and contains a list of tuples, where each tuple contains the index of two keypoints to be connected in the rendered video. We provide default keypoint pairs for "YOLO", "Mediapipe" and various implementations of "OpenPose" models in the file `keypoint_pairs.py`.

Below is the code implementation for the abstract dataset class, for the very simple TED talks dataset (without ground truth) and the more complicated TragicTalkers dataset (with ground truth data).

::: {.callout-note collapse="true"}
## Dataset Class
```{.python include="../src/datasets/dataset.py" code-line-numbers="true" filename="src/datasets/dataset.py"}
```
:::

::: {.callout-note collapse="true"}
## TED Talks Dataset
```{.python include="../src/datasets/ted_dataset.py" code-line-numbers="true" filename="src/datasets/ted_dataset.py"}
```
:::

::: {.callout-note collapse="true"}
## Tragic Talkers Dataset
```{.python include="../src/datasets/tragic_talkers_dataset.py" code-line-numbers="true" filename="src/datasets/tragic_talkers_dataset.py"}
```
:::

## Inference {#sec-architecture-inference}

### Video Pose Result {#sec-architecture-video-pose-result}
The `VideoPoseResult` object is the standardized output of a pose prediction model.
It is a nested object that contains a `FramePoseResult` object for each frame in the video.
Within each frame pose result, there is a list of `PersonPoseResult` objects, one for each person in the frame.
Every result for a person contains a list of `PoseKeypoint` objects, one for each keypoint in the model output format, with the x and y coordinates and an optional confidence score.

::: {.callout-note collapse="true"}
## Video Pose Result Class
```{.python include="../src/inference/pose_result.py" code-line-numbers="true" filename="src/inference/pose_result.py"}
```
:::

### Pose Estimator {#sec-architecture-pose-estimators}
The pose estimators are responsible for predicting the poses of the persons in the video and wrap the call to specific AI models or pose estimation pipelines.
Each model is implemented in a separate class that inherits from the abstract `PoseEstimator` class.
It outputs a standardized `VideoPoseResult` object.

If users want to add a new pose estimator, they need to implement the `estimate_pose` method and the `get_keypoint_pairs` method.
Special care needs to be taken to ensure that the output is valid according to the following constraints:

1. The number of frames in the frame results matches the number of frames in the video.
2. If no persons were detected in a frame, the persons list should be empty.
3. If a person was detected, but some keypoints are missing, the missing keypoints should have the values `x=0, y=0, confidence=None`.
4. The number of keypoints for a person is constant across all frames.
5. Low confidence keypoints should be masked out using the `confidence_threshold` config parameter of the pose estimator.
6. Map the keypoints to COCO format if the `save_keypoints_in_coco_format` config parameter is set to true.


As an example, we provide the implementation of the abstract pose estimator class and the implementation of the YOLO model.

::: {.callout-note collapse="true"}
## Pose Estimator Class
```{.python include="../src/models/pose_estimator.py" code-line-numbers="true" filename="src/models/pose_estimator.py"}
```
:::

::: {.callout-note collapse="true"}
## YOLO Model
```{.python include="../src/models/yolo_pose_estimator.py" code-line-numbers="true" filename="src/models/yolo_pose_estimator.py"}
```
:::


### Inference Engine {#sec-architecture-inference-engine}
The inference engine is responsible for running the pose estimators on the videos and saving the results in the `poses` folder as json files.
If a checkpoint name is provided in the configuration file, the inference engine will load the results from the checkpoint and skip inference for the videos that already have results.
This allows to resume the inference process in case it fails or to skip the inference entirely and only evaluate the metrics.
The inference engine returns a nested dictionary, mapping pose estimator names to video names and `VideoPoseResult` objects.


## Evaluation {#sec-architecture-evaluation}

### Metric {#sec-architecture-evaluation-metric}
Each metric inherits from the abstract `Metric` class and implements the `compute` method, which takes as input one predicted video pose result, a ground truth pose result, if available, and the name of the pose estimator.
It outputs a `MetricResult` object, which contains the metric values for the video (see section @sec-architecture-evaluation-metric-result).

::: {.callout-note collapse="true"}
## Metric Class
```{.python include="../src/evaluation/metrics/metric.py" code-line-numbers="true" filename="src/evaluation/metrics/metric.py"}
```
:::

MaskBench currently implements ground truth-based metrics for Euclidean Distance, Percentage of Correct Keypoints (PCK) and Root Mean Square Error (RMSE).
Furthermore, we provide kinematic metrics for velocity, acceleration and jerk.
Section @sec-metrics contains a more extensive description of the implemented metrics.

**Matching Person Indices**

For some metrics, it is crucial to ensure, that the order of persons in video pose results match between a prediction and a reference.
The metric class provides a method called `match_person_indices`, which is used in ground-truth based metrics to ensure that person indices match between the ground truth and the prediction.
Furthermore, kinematic metrics also make use of this method, as they can only be calculated if the person indices match between consecutive frames.
The implementation uses the Hungarian algorithm and the mean of a person's keypoints to find the best match between all persons in the reference and predicted pose result.

Let $N$ be the number of persons in the reference, $M$ be the number of persons in the predicted pose result, and $K$ be the number of keypoints per person.
The output of the match person indices method is an array of shape $\text{max}(N, M) \times K \times 2$, where the first $N$ positions contain the persons in the same order as in the reference.
The last $N$ to $M$ positions (in case $M > N$) contain the remaining persons, which are not present in the reference.

Edge cases include, where the first of two persons appears in one frame, but not in the next.
In this case, the second person is still assigned the second index in the output array, while the first index contains infinite values to signal that the person is not present.
The same applies, if the prediction contains less persons than the reference ($M<N$).
Each metric can then decide how to handle these infinite values, for example by setting them to `NaN` (kinematic metrics) or a pre-defined value (euclidean distance and ground truth-based metrics).

**Unit Testing**

It is good practice to implement unit tests for the various metric classes to ensure that the output is correct and as expected.
We provide unit tests for all metrics in the `src/tests` folder, which can be run with the command `pytest`.
Running these tests after a change to the metric classes ensures that no other functionality is compromised.


### Metric Result {#sec-architecture-evaluation-metric-result}
The output of the metric's `compute` method is a `MetricResult` object.
It contains the metric values in the form of a multi-dimensional array, where the axes are labeled with names, for example with a "frame", "person" and "keypoint" axis.
The `aggregate` function of the class aggregates the values with a given method and along the specified axes only.
Currently MaskBench supports mean, median, Root Mean Square Error, vector magnitude, sum, min and max as aggregation methods.
The output of the aggregation method is once again a metric result, reduced in its dimensionality, having only the axes along which it was not aggregated.

This flexible approach of storing the results with their axes names and using the names in the aggregation method allows for the visualization of the results in a variety of ways, for example as a per keypoint plot, distribution plot or as a single scalar value. Furthermore, it allows extending the framework with new metrics (possibly containing different axis names) and also different visualizations.

### Evaluator {#sec-architecture-evaluation-evaluator}
Given a list of metrics, the evaluator executes the configured metrics on the pose estimation results for all pose estimators and videos.
It returns a nested dictionary, mapping metric names to pose estimator names to video names to `MetricResult` objects.
It does not perform aggregation over the videos or pose estimators in order to allow for more flexibility in the visualization of the results.

## Visualization {#sec-architecture-visualization}

## Rendering {#sec-architecture-rendering}




# Datasets {#sec-datasets}
This study uses three video-based datasets, each representing a different level of complexity, from simple, controlled settings to more dynamic and interactive scenarios. To capture this range, we selected three distinct datasets: TED Kid Video, TED Talks, and Tragic Talkers. Each dataset was chosen based on specific conditions to evaluate pose estimation models under varying degrees of difficulty.

## TED Kid Video {#sec-datasets-ted-kid}
The TED Kid Video is a short, 10-second clip featuring a child in a well-lit environment with high video quality. Throughout the sequence, all body parts remain clearly visible, and there is no occlusion or obstruction of the subject. This video serves as a controlled scenario to evaluate pose estimation methods under ideal conditions, providing a baseline for comparison with more complex datasets. 

We first tested our models' performance and evaluation metrics on this video to verify that our metrics function correctly in an ideal setting and that the implementation is accurate. This initial validation ensures that subsequent experiments on more challenging datasets can be interpreted with confidence in the correctness of our evaluation pipeline.

## TED Talks {#sec-datasets-ted-talks}
For the TED Talks, we selected ten videos featuring diverse speakers to capture a range of conditions. The selection criteria included speaker gender, skin tone, clothing types (e.g., long dresses vs. short garments), partial occlusion, videos where only the hands, upper body, or lower body are visible, and variations in movement style and speed.

We focused on how the models perform under more complex conditions—such as when the scene changes, when there are noises such as audience, when only the lower or upper body is visible, when only hands or feet appear, or when body parts are not easily distinguishable (e.g., when someone wears a long dress). We also considered cases where visual elements like images or patterns are present on the speaker’s clothing. In each TED Talk video, our analysis focuses solely on the primary speaker.

## Tragic Talkers {#sec-datasets-tragic-talkers}

We wanted to evaluate the models' performance under conditions where more than one person is present and interacting. The Tragic Talkers dataset was selected because it includes 2D ground truth annotations, allowing us to establish a baseline and test metrics such as Percentage of Correct Keypoints (PCK).

The dataset features a man in regular clothing and a woman wearing a long dress. It includes four distinct video scenarios, each originally recorded from twenty-two different camera angles. However, for our analysis, we used only four angles, as many were too similar in viewpoint.The video scenarios are:

**Monologue (Male and Female):** Individual speakers deliver monologues with relatively simple and slow movements.

**Conversation:** A male and female speaker engage in dialogue with limited movement.

**Interactive 1:** A conversation between a male and female speaker that includes physical interaction (e.g., hand contact), with the man sitting close to the woman.

**Interactive 4:** A more dynamic dialogue featuring faster movements, partial occlusion, and moments of full occlusion.

These scenarios were chosen to reflect a variety of real-world human interactions, allowing us to test how well pose estimation models perform under conditions such as occlusion, multi-person scenes, and varied movement patterns.


# Experiments & Evaluation Metrics {#sec-experiments-metrics}

## Data Preprocessing
Data preprocessing was carried out to remove unnecessary parts of the videos and to split the TED Talk videos into shorter segments compatible with MaskAnyone. Since MaskAnyone cannot process videos longer than 2.5 minutes, and is already resource-intensive even at that limit, we divided the TED Talk videos into chunks of 30 or 50 seconds, depending on the content.

TED Talks also showed some inconsistency in structure. Some videos were straightforward, with only the speaker and audience visible, making them easy to segment at any point. However, others included additional visual content such as slides, pictures, or unrelated scenes, which made it more difficult to determine clean chunking points.

For these more complex videos, we carefully selected segment boundaries to ensure that each chunk started with frames where a human was clearly visible. When necessary, we manually trimmed the beginning of chunks to avoid starting with empty or unrelated frames. This step was critical because if a video starts with non-human content, MaskAnyone may incorrectly classify objects in the first frame as humans and then continue misdetecting them in subsequent frames.

After preprocessing, the chunks from each TED Talk were merged back into a single video file. This allowed all pose estimation models to be evaluated on the same content and ensured consistency across results.

No preprocessing was required for the Tragic Talkers dataset, as the videos were already clean and free of noise or unrelated visual content.

# Experiments & Evaluation Metrics
(Tim - 2.)

## Evaluation Metrics {#sec-metrics}

### Euclidean Distance {#sec-metrics-euclidean-distance}

### Percentage of Keypoints (PCK) {#sec-metrics-pck}

### Root Mean Square Error (RMSE) {#sec-metrics-rmse}

### Velocity {#sec-metrics-velocity}

### Acceleration {#sec-metrics-acceleration}

### Jerk {#sec-metrics-jerk}



## Experimental Setup {#sec-experiments}
(Tim - 3.)

### TED Kid Video {#sec-experiments-ted-kid}

### TED Talks {#sec-experiments-ted-talks}

### Tragic Talkers {#sec-experiments-tragic-talkers}

### Inference on Raw vs. Masked Videos {#sec-experiments-inference-raw-masked}




# Results {#sec-results}
(Tim - 4.)

## TED Kid Video {#sec-results-ted-kid}

## TED Kid Video {#sec-results-ted-talks}

## Tragic Talkers {#sec-results-tragic-talkers}

## Inference on Raw vs. Masked Videos {#sec-results-inference-raw-masked}




# Future Work & Limitations {#sec-future-work}
(Zainab - 4.)
(Tim - 5.)




# Conclusion {#sec-conclusion}
(Tim - 7.)


# References


