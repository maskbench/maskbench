---
title: "MaskBench - A Comprehensive Benchmark Framework for Video De-Identification"
date: "2025 08 08"
author:
    - name: Tim Riedel
      affiliation: Hasso Plattner Institute, University of Potsdam, Germany
    - name: Zainab Zafari
      affiliation: Hasso Plattner Institute, University of Potsdam, Germany
    - name: Sharjeel Shaik
      affiliation: University of Potsdam, Germany
    - name: Babajide Alamu Owoyele
      affiliation: Hasso Plattner Institute, University of Potsdam, Germany
    - name: Wim Pouw
      affiliation: Tilburg University, Netherlands
contact:
    - name: Tim Riedel
      email: tim.riedel@student.hpi.de
    - name: Zainab Zafari
      email: zainab.zafari@student.hpi.de
    - name: Babajide Alamu Owoyele
      email: babajide.owoyele@student.hpi.de
    - name: Wim Pouw
      email: w.pouw@tilburguniversity.edu
bibliography: dependencies/refs.bib
css: dependencies/styles.css
theme: journal
format:
    html:
        tbl-cap-location: bottom
        toc: true
        toc-location: left
        toc-title: "Contents"
        toc-depth: 3
        number-sections: true
        code-fold: true
        code-tools: true
        grid:
            margin-width: 100px
filters:
    - include-code-files
# engine: knitr
jupyter: python3
---

# Abstract / Overview {#sec-abstract}
(Tim - 6.)



# Getting Started {#sec-installation}
(Zainab - 3.)

## Installation / Setup {#sec-installation-setup}

## Usage {#sec-installation-usage}

## Configuration of Experiments {#sec-installation-configuration}




# Introduction {#sec-introduction}
(Zainab - 5.)



# Related Work {#sec-related-work}
Human pose estimation aims to localize body joints (e.g., shoulders, elbows, knees) from visual input such as images or videos. Existing methods are generally categorized into two main strategies: top-down and bottom-up. In the top-down approach, human instances are first detected in the image, and pose estimation is then performed within each bounding box. In contrast, the bottom-up approach detects all keypoints in the image first and subsequently associates them with individual persons [@saiwa2025openpose], [@kaim2024comparison].

Several studies have benchmarked popular pose estimation models across different datasets, conditions, and use cases. The following works are particularly relevant to our benchmarking framework:

* **Comparision Of ML Models For Posture** [@kaim2024comparison]
conducted a detailed empirical comparison between YOLOv7 Pose and MediaPipe Pose using both image (COCO) and video (Penn Action) datasets. The study evaluated key metrics such as keypoint accuracy (PCK@0.2), inference speed, and robustness under challenging conditions like low light and occlusion.
    YOLOv7 achieved a slightly higher accuracy score of 87.8\% versus MediaPipe’s 84.1\%. However, MediaPipe demonstrated superior real-time performance on CPU-only devices, achieving 16--18 frames per second (FPS) compared to YOLOv7’s 4--5 FPS. In low-light environments, MediaPipe maintained detection consistency, whereas YOLOv7 performed better in occluded scenarios, successfully recognizing hidden body parts. The study highlights a trade-off: YOLOv7 is more robust under visually difficult conditions, while MediaPipe excels in lightweight, single-person use cases with real-time requirements.

* **OpenPose vs MediaPipe: A Practical and Architectural Comparison** [@saiwa2025openpose]
  A recent blog post by Saiwa presents a detailed comparison between OpenPose and MediaPipe, discussing their architectural differences, device compatibility, and practical applications. OpenPose uses a bottom-up approach with Part Affinity Fields and is optimized for multi-person full-body tracking, whereas MediaPipe follows a top-down strategy focusing on speed and cross-platform deployment. MediaPipe's modular design and high CPU efficiency make it suitable for real-time mobile apps, while OpenPose provides detailed skeletal tracking but at higher computational cost.

* **YOLOv7 Pose vs MediaPipe in Human Pose Estimation in videos** [@learnopencv2022yolopose] 
This benchmark-focused blog compares YOLOv7 Pose and MediaPipe on a variety of video scenarios such as sports, dance, crowd scenes, and occlusions. The comparison is qualitative and highlights where each model performs better. MediaPipe excels in handling low-resolution input, achieves faster CPU inference, and is well-suited for detecting distant individuals. However, it is limited to single-person detection. YOLOv7 performs better in occlusion-heavy scenarios, supports multi-person estimation, and handles fast motion more accurately, especially with high-resolution input and GPU support. The study underscores that MediaPipe is better for lightweight, real-time single-user applications, while YOLOv7 is better suited for more complex, high-performance environments.

# MaskBench Architecture {#sec-architecture}
![MaskBench Architecture](./images/maskbench-workflow.jpg){#fig-architecture}

The general workflow of MaskBench is depicted in figure @fig-architecture.
It starts with loading the dataset, pose estimators and evaluation metrics.
The application creates a checkpoint folder in the specified `/output` directory, named after the dataset and a timestamp (e.g. `/output/TedTalks-20250724-121127`).
Thereafter, inference is performed on all videos of the dataset using the pose estimators specified in the configuration file. 
The user is required to annotate the videos semi-automatically for the `MaskAnyoneUI` pose estimators, using MaskAnyone.
A `poses` folder is created within the checkpoint, with a subfolder for each pose estimator and a single json file for each video.
Thereafter, the application evaluates all the specified metrics and visualizes them in plots, which are stored in the `plots` folder in the checkpoint.
Lastly, for each video, the application creates a multiple rendered video, one for each pose estimator, which are stored in the `renderings` folder in the checkpoint.

Each component of MaskBench is implemented in a modular way, so that it can be easily extended and modified, which we will discuss in the following sections.

## Dataset {#sec-architecture-dataset}
The dataset provides the video data for the pose estimation and ground truth data for the evaluation, if available.
When adding a new dataset, the user needs to create a new class that inherits from the `Dataset` class and overwrite the `_load_samples` method, which creates one `VideoSample` object for each video in the dataset.
If the dataset provides ground truth data, the user additionally needs to overwrite the `get_gt_pose_results` and `get_gt_keypoint_pairs` methods.
For each video, the `get_gt_pose_results` method should return a `VideoPoseResult` object.
The `get_gt_keypoint_pairs` method is used for rendering the ground truth keypoints and contains a list of tuples, where each tuple contains the index of two keypoints to be connected in the rendered video. We provide default keypoint pairs for YoloPose, MediaPipePose and various implementations of OpenPose models in the file `keypoint_pairs.py`.

Below is the code implementation for the abstract dataset class, for the very simple TED talks dataset (without ground truth) and the more complicated TragicTalkers dataset (with pseudo-ground truth data).

::: {.callout-note collapse="true"}
## Dataset Class
```{.python include="../src/datasets/dataset.py" code-line-numbers="true" filename="src/datasets/dataset.py"}
```
:::

::: {.callout-note collapse="true"}
## TED Talks Dataset
```{.python include="../src/datasets/ted_dataset.py" code-line-numbers="true" filename="src/datasets/ted_dataset.py"}
```
:::

::: {.callout-note collapse="true"}
## Tragic Talkers Dataset
```{.python include="../src/datasets/tragic_talkers_dataset.py" code-line-numbers="true" filename="src/datasets/tragic_talkers_dataset.py"}
```
:::

## Inference {#sec-architecture-inference}

### Video Pose Result {#sec-architecture-video-pose-result}
The `VideoPoseResult` object is the standardized output of a pose prediction model.
It is a nested object that contains a `FramePoseResult` object for each frame in the video.
Within each frame pose result, there is a list of `PersonPoseResult` objects, one for each person in the frame.
Every result for a person contains a list of `PoseKeypoint` objects, one for each keypoint in the model output format, with the x and y coordinates and an optional confidence score.

::: {.callout-note collapse="true"}
## Video Pose Result Class
```{.python include="../src/inference/pose_result.py" code-line-numbers="true" filename="src/inference/pose_result.py"}
```
:::

### Pose Estimator {#sec-architecture-pose-estimators}
The pose estimators are responsible for predicting the poses of the persons in the video and wrap the call to specific AI models or pose estimation pipelines.
Each model is implemented in a separate class that inherits from the abstract `PoseEstimator` class.
It outputs a standardized `VideoPoseResult` object.

If users want to add a new pose estimator, they need to implement the `estimate_pose` method and the `get_keypoint_pairs` method.
Special care needs to be taken to ensure that the output is valid according to the following constraints:

1. The number of frames in the frame results matches the number of frames in the video.
2. If no persons were detected in a frame, the persons list should be empty.
3. If a person was detected, but some keypoints are missing, the missing keypoints should have the values `x=0, y=0, confidence=None`.
4. The number of keypoints for a person is constant across all frames.
5. Low confidence keypoints should be masked out using the `confidence_threshold` config parameter of the pose estimator.
6. Map the keypoints to COCO format if the `save_keypoints_in_coco_format` config parameter is set to true.


As an example, we provide the implementation of the abstract pose estimator class and the implementation of the YOLO model.

::: {.callout-note collapse="true"}
## Pose Estimator Class
```{.python include="../src/models/pose_estimator.py" code-line-numbers="true" filename="src/models/pose_estimator.py"}
```
:::

::: {.callout-note collapse="true"}
## YOLO Model
```{.python include="../src/models/yolo_pose_estimator.py" code-line-numbers="true" filename="src/models/yolo_pose_estimator.py"}
```
:::

MaskBench supports a total of seven pose estimators, including pure AI models like YOLOv11-Pose, MediaPipePose and OpenPose.
Furthermore, it uses MaskAnyone as a pose estimator, which is a mixture of expert models.
We differentiate between the MaskAnyoneAPI pose estimator, which is executed fully automated at inference time, and the MaskAnyoneUI pose estimator, which is the human-in-the-loop approach, where the user can manually adjust the mask of the person of interest.
The latter has to be executed manually by the user before running MaskBench and the resulting pose files need to be provided as one file per video.

### Inference Engine {#sec-architecture-inference-engine}
The inference engine is responsible for running the pose estimators on the videos and saving the results in the `poses` folder as json files.
If a checkpoint name is provided in the configuration file, the inference engine will load the results from the checkpoint and skip inference for the videos that already have results.
This allows to resume the inference process in case it fails or to skip the inference entirely and only evaluate the metrics.
The inference engine returns a nested dictionary, mapping pose estimator names to video names and `VideoPoseResult` objects.
Furthermore, it saves the inference times for each pose estimator and video in the checkpoint folder in a json file.


## Evaluation {#sec-architecture-evaluation}

### Metric {#sec-architecture-evaluation-metric}
Each metric inherits from the abstract `Metric` class and implements the `compute` method, which takes as input one predicted video pose result, a ground truth pose result, if available, and the name of the pose estimator.
It outputs a `MetricResult` object, which contains the metric values for the video (see section @sec-architecture-evaluation-metric-result).

::: {.callout-note collapse="true"}
## Metric Class 
```{.python include="../src/evaluation/metrics/metric.py" code-line-numbers="true" filename="src/evaluation/metrics/metric.py"}
```
:::

MaskBench currently implements ground truth-based metrics for Euclidean Distance, Percentage of Correct Keypoints (PCK) and Root Mean Square Error (RMSE).
Furthermore, we provide kinematic metrics for velocity, acceleration and jerk.
Section @sec-metrics contains a more extensive description of the implemented metrics.

**Matching Person Indices**

For some metrics, it is crucial to ensure, that the order of persons in video pose results match between a prediction and a reference.
The metric class provides a method called `match_person_indices`, which is used in ground-truth based metrics to ensure that person indices match between the ground truth and the prediction.
Furthermore, kinematic metrics also make use of this method, as they can only be calculated if the person indices match between consecutive frames.
The implementation uses the Hungarian algorithm and the mean of a person's keypoints to find the best match between all persons in the reference and predicted pose result.

Let $N$ be the number of persons in the reference, $M$ be the number of persons in the predicted pose result, and $K$ be the number of keypoints per person.
The output of the match person indices method is an array of shape $\text{max}(N, M) \times K \times 2$, where the first $N$ positions contain the persons in the same order as in the reference.
The last $N$ to $M$ positions (in case $M > N$) contain the remaining persons, which are not present in the reference.

Edge cases include, where the first of two persons appears in one frame, but not in the next.
In this case, the second person is still assigned the second index in the output array, while the first index contains infinite values to signal that the person is not present.
The same applies, if the prediction contains less persons than the reference ($M<N$).
Each metric can then decide how to handle these infinite values, for example by setting them to `NaN` (kinematic metrics) or a pre-defined value (euclidean distance and ground truth-based metrics).

**Unit Testing**

It is good practice to implement unit tests for the various metric classes to ensure that the output is correct and as expected.
We provide unit tests for all metrics in the `src/tests` folder, which can be run with the command `pytest`.
Running these tests after a change to the metric classes ensures that no other functionality is compromised.


### Metric Result {#sec-architecture-evaluation-metric-result}
The output of the metric's `compute` method is a `MetricResult` object.
It contains the metric values in the form of a multi-dimensional array, where the axes are labeled with names, for example with a "frame", "person" and "keypoint" axis.
The `aggregate` function of the class aggregates the values with a given method and along the specified axes only.
Currently MaskBench supports mean, median, Root Mean Square Error, vector magnitude, sum, min and max as aggregation methods.
The output of the aggregation method is once again a metric result, reduced in its dimensionality, having only the axes along which it was not aggregated.

This flexible approach of storing the results with their axes names and using the names in the aggregation method allows for the visualization of the results in a variety of ways, for example as a per keypoint plot, distribution plot or as a single scalar value. Furthermore, it allows extending the framework with new metrics (possibly containing different axis names) and also different visualizations.

### Evaluator {#sec-architecture-evaluation-evaluator}
Given a list of metrics, the evaluator executes the configured metrics on the pose estimation results for all pose estimators and videos.
It returns a nested dictionary, mapping metric names to pose estimator names to video names to `MetricResult` objects.
It does not perform aggregation over the videos or pose estimators in order to allow for more flexibility in the visualization of the results.

## Visualization {#sec-architecture-visualization}
After evaluation, the results are visualized in plots and tables.

### Visualizer
An abstract `BaseVisualizer` class defines the interface for all visualizers.
We defined a MaskBench specific visualizer class for the experiments we conducted, which can either be reused for other experiments or extended to support new visualizations.

::: {.callout-note collapse="true"}
## MaskBench Visualizer
```{.python include="../src/evaluation/visualizer/maskbench_visualizer.py" code-line-numbers="true" filename="src/visualization/maskbench_visualizer.py"}
```
:::

The visualizer saves the plots and tables in the `plots` folder in the checkpoint.

### Plots
Each plot inherits from the abstract `Plot` class and implements the `draw` method.
The draw method can take various forms of input data, most commonly the results of the evaluator.
Each plot can implement a specific way to aggregate and organize the data, for example by taking the median over all videos for a given pose estimator.

::: {.callout-note collapse="true"}
## Plot Class
```{.python include="../src/evaluation/plots/plot.py" code-line-numbers="true" filename="src/evaluation/plots/plot.py"}
```
:::

::: {.callout-note collapse="true"}
## Kinematic Distribution Plot
```{.python include="../src/evaluation/plots/kinematic_distribution_plot.py" code-line-numbers="true" filename="src/evaluation/plots/kinematic_distribution_plot.py"}
```
:::


We provide the following plots and tables:

- **Kinematic Distribution Plot**: Visualizes the distribution of the kinematic values for each pose estimator.
- **Per Keypoint Plot**: Displays the median kinematic metric values or euclidean distance for each COCO keypoint. Requires keypoints to be stored in the COCO format.
- **Inference Time Plot**: Visualizes the inference time for each pose estimator.
- **Result Table**: Aggregates the results for each pose estimator over all videos per metric and outputs a table.


## Rendering {#sec-architecture-rendering}
Rendering of videos is performed by the `Renderer` class.
For each video in the dataset, it creates a new folder in the `renderings` folder in the checkpoint folder.
Within each video folder, it creates one video with the rendered keypoints for each pose estimator.
Special care was taken to ensure that the color of pose estimators is consistent across all videos and plots by using a pre-defined color palette for color-blind friendly plots.


# Datasets {#sec-datasets}
This study uses four video-based datasets, each representing a different level of complexity, from simple, controlled settings to more dynamic and interactive scenarios. To capture this range, we selected or created four distinct datasets: TED Kid Video, TED Talks [@ted], Tragic Talkers [@tragic-talkers] and a masked video dataset. Each dataset was chosen based on specific conditions to evaluate pose estimation models under varying degrees of difficulty.

## TED Kid Video {#sec-datasets-ted-kid}
The TED Kid Video is a short, 10-second clip featuring a child in a well-lit environment with high video quality. Throughout the sequence, all body parts remain clearly visible, and there is no occlusion or obstruction of the subject. This video serves as a controlled scenario to evaluate pose estimation methods under ideal conditions, providing a baseline for comparison with more complex datasets. 

We first tested our models' performance and evaluation metrics on this video to verify that our metrics function correctly in an ideal setting and that the implementation is accurate. This initial validation ensures that subsequent experiments on more challenging datasets can be interpreted with confidence in the correctness of our evaluation pipeline.

## TED Talks {#sec-datasets-ted-talks}
For the TED Talks [@ted], we selected ten videos featuring diverse speakers to capture a range of conditions. The selection criteria included speaker gender, skin tone, clothing types (e.g., long dresses vs. short garments), partial occlusion, videos where only the hands, upper body, or lower body are visible, and variations in movement style and speed.

We focused on how the models perform under more complex conditions—such as when the scene changes, when there are noises such as audience, when only the lower or upper body is visible, when only hands or feet appear, or when body parts are not easily distinguishable (e.g., when someone wears a long dress). We also considered cases where visual elements like images or patterns are present on the speaker’s clothing. In each TED Talk video, our analysis focuses solely on the primary speaker.

## Tragic Talkers {#sec-datasets-tragic-talkers}
We wanted to evaluate the models' performance under conditions where more than one person is present and interacting. The Tragic Talkers dataset [@tragic-talkers] was selected because it includes 2D pseudo-ground truth annotations generated by the AI model OpenPose, allowing us to establish a baseline and test metrics such as Percentage of Correct Keypoints (PCK).

The dataset features a man in regular clothing and a woman wearing a long dress. It includes four distinct video scenarios, each originally recorded from twenty-two different camera angles. However, for our analysis, we used only four angles, as many were too similar in viewpoint.
The video scenarios are:

**Monologue (Male and Female):** Individual speakers deliver monologues with relatively simple and slow movements.

**Conversation:** A male and female speaker engage in dialogue with limited movement.

**Interactive 1:** A conversation between a male and female speaker that includes physical interaction (e.g., hand contact), with the man sitting close to the woman.

**Interactive 4:** A more dynamic dialogue featuring faster movements, partial occlusion, and moments of full occlusion.

These scenarios were chosen to reflect a variety of real-world human interactions, allowing us to test how well pose estimation models perform under conditions such as occlusion, multi-person scenes, and varied movement patterns.

## Masked Video Dataset {#sec-datasets-masked-video}
The masked video dataset is a colllection of three videos.
It contains the TED kid video, a chunk from the TED talk "Let curiosity lead" [@ted-curiosity] and the video "interactive1_t1-cam06" from the Tragic Talkers dataset.
The purpose of this dataset is to evaluate the performance of pose estimators when inference is performed on masked videos.

For the creation of this dataset, we used MaskAnyoneUI to manually mask the persons of interest with four different hiding strategies (blurring, pixelation, contours and inpainting) in each of the three videos.
We therefore obtained a total of 15 videos, including the original videos.


## Data Preprocessing
Data preprocessing was carried out to remove unnecessary parts of the videos and to split the TED Talk videos into shorter segments compatible with MaskAnyone. Since MaskAnyone cannot process videos longer than 2.5 minutes, and is already resource-intensive even at that limit, we divided the TED Talk videos into chunks of 30 or 50 seconds, depending on the content.

TED Talks also showed some inconsistency in structure. Some videos were straightforward, with only the speaker and audience visible, making them easy to segment at any point. However, others included additional visual content such as slides, pictures, or unrelated scenes, which made it more difficult to determine clean chunking points.

For these more complex videos, we carefully selected segment boundaries to ensure that each chunk started with frames where a human was clearly visible. When necessary, we manually trimmed the beginning of chunks to avoid starting with empty or unrelated frames. This step was critical because if a video starts with non-human content, MaskAnyone may incorrectly classify objects in the first frame as humans and then continue misdetecting them in subsequent frames.

After preprocessing, the chunks from each TED Talk were merged back into a single video file. This allowed all pose estimation models to be evaluated on the same content and ensured consistency across results.

No preprocessing was required for the Tragic Talkers dataset, as the videos were already clean and free of noise or unrelated visual content.


# Evaluation Metrics {#sec-metrics}
In the following sections, we outline the metrics used for evaluating accuracy, smoothness and jitter of different pose estimators.

## Ground-Truth Metrics
The metrics in this section are based on ground truth data provided by the dataset and primarily evaluate the accuracy of the pose estimation compared to the reference ground truth.

### Euclidean Distance {#sec-metrics-euclidean-distance}
The Euclidean distance metric measures the spatial accuracy of pose estimation by calculating the normalized distance between predicted and ground truth keypoint positions. 
For each keypoint of a person in a frame, it computes the L2 norm (Euclidean distance) between the predicted position $(x_p, y_p)$ and the ground truth position $(x_{gt}, y_{gt})$:

$$ d = \frac{\sqrt{(x_p - x_{gt})^2 + (y_p - y_{gt})^2}}{s} $$

where $s$ is a normalization factor. 
The normalization is crucial to make the metric scale-invariant and comparable across different person sizes. 
The metric is set up to support three normalization strategies, out of which we only implemented the first one:

1. **Bounding Box Size**: The distance is normalized by the maximum of width and height of the person's bounding box, which is computed from the ground truth keypoints. While this adapts to different person sizes, it has a notable drawback: shorter limbs naturally have smaller variations in joint positions, and even small errors can constitute a large fraction of their true length. Using a fixed normalization factor like the bounding box size doesn't account for this, leading to over-penalization of errors in shorter limbs.

2. **Head Size**: Normalization by the head bone link size (not implemented).

3. **Torso Size**: Normalization by the torso diameter (not implemented).

Head and torso normalization inherently solve the shorter limb problem as smaller limbs are typically accompanied by proportionally smaller torso diameters and head bone links. 
We outline future work for the implementation of head and torso normalization in section @sec-future-work.

The metric handles several edge cases to ensure robust evaluation:

- **Different Order of Persons**: The metric uses the Hungarian algorithm as described in section @sec-architecture-evaluation-metric to match person indices between ground truth and predictions, ensuring that distances are calculated between corresponding persons even if they appear in different orders.

- **Keypoint Missing in Ground Truth but not in Prediction**: When a keypoint is missing in the ground truth (coordinates are (0,0)) but was detected in the prediction, the corresponding distance is set to `NaN` and excluded from aggregation calculations, as a distance to a missing keypoint is undefined.

- **Keypoint Missing in Prediction but Present in Ground Truth**: When a keypoint exists in the ground truth but is missing in the prediction (coordinates are (0,0)), the distance is set to a predetermined fill value (in this case a large distance value of 1). This penalizes the model for failing to detect keypoints without overly impacting the aggregated results.

- **Undetected Persons**: If a person present in the ground truth is entirely undetected in the prediction, all keypoint distances for that person are set to the same distance fill value.

Euclidean distance is the basis for the computation of the Percentage the PCK and RMSE metrics.

### Percentage of Keypoints (PCK) {#sec-metrics-pck}
The Percentage of Correct Keypoints (PCK) metric evaluates pose estimation accuracy by determining the proportion of predicted keypoints that fall within a specified threshold distance of their ground truth locations. 
A keypoint is considered "correct" if its normalized Euclidean distance to the ground truth is less than the threshold.

For each frame, PCK is calculated as:

$$
PCK = \frac{\text{number of keypoints with distance < threshold}}{\text{total number of valid keypoints}}
$$

The metric produces a value between 0 and 1, where 1 indicates perfect prediction (all keypoints within the threshold) and 0 indicates complete failure (no keypoints within the threshold). 
PCK is particularly useful for understanding the overall reliability of pose predictions at a given precision level defined by the threshold.

### Root Mean Square Error (RMSE) {#sec-metrics-rmse}
The Root Mean Square Error (RMSE) provides a single aggregate measure of pose estimation accuracy by computing the root mean square of normalized Euclidean distances across all keypoints and persons in a frame.
RMSE is calculated as:

$$
RMSE = \sqrt{\frac{1}{N} \sum_{i=1}^{N} d_i^2}
$$

where $N$ is the total number of valid keypoints in the frame and $d_i$ is the normalized Euclidean distance for keypoint $i$. 
RMSE penalizes larger errors more heavily due to the squared term, making it particularly sensitive to outliers. 

## Kinematic Metrics
Velocity, acceleration and jerk are kinematic metrics that are useful for identifying unnatural or jittery movements in pose estimations, as they highlight rapid changes in motion.

### Velocity {#sec-metrics-velocity}
The velocity metric calculates the rate of change in keypoint positions between consecutive frames. 
For each keypoint of a person, it measures how quickly that keypoint moves in pixels per frame, giving insights into the smoothness and consistency of the pose estimation across frames.

The calculation of velocity follows a three-step process:

1. First, person indices are matched between consecutive frames as described in @sec-architecture-evaluation-metric to ensure we track the same person across frames.

2. The velocity is then computed with $v_t = p_{t+1} - p_t$ as the difference between keypoint positions in consecutive frames, where $p_t$ represents the keypoint position at frame $t$ and $v_t$ is the resulting velocity vector.

3. Finally, the metric can be configured to report velocities in either pixels per frame or pixels per second. For the latter, the frame-based velocity is divided by the time delta between frames (1/fps).

Several edge cases are handled specifically:

- For videos with fewer than 2 frames, the metric returns `NaN` values as velocity cannot be computed.
- When a keypoint is missing in either of two consecutive frames (masked or invalid), the velocity for that keypoint is set to NaN.
- The result will have one less frame than the input video, because the velocity metric produces one metric value for any consecutive pair of frames.
- The output contains a coordinate axis (for x and y) to represent the velocity as a vector and to serve as a basis for the acceleration and jerk metrics. For evaluation and visualization, the metric result should therefore be aggregated with the method `vector_magnitude` along the coordinate axis to obtain a scalar velocity value.

### Acceleration {#sec-metrics-acceleration}
The acceleration metric measures the rate of change in velocity over time, representing how quickly the movement speed of keypoints changes. 
It is computed by $a_t = v_{t+1} - v_t$, where $a_t$ is the acceleration at time $t$, $v_t$ represents the velocity, and $p_t$ the keypoint position. 
Similar to the velocity metric, the acceleration can be reported in either pixels per frame squared or pixels per second squared, with the latter requiring division by the squared time delta between frames (1/fps²).


### Jerk {#sec-metrics-jerk}
The jerk metric, which measures the rate of change in acceleration, provides insights into the smoothness of motion by quantifying how abruptly the acceleration changes.
It is calculated with $j_t = a_{t+1} - a_t$ as the difference between consecutive acceleration values, where $j_t$ is the jerk at time $t$ and $a_t$ represents the acceleration. 
The metric can be configured to output values in pixels per frame cubed or pixels per second cubed. For the latter, the frame-based jerk is divided by the cubed time delta between frames (1/fps³).

# Experimental Setup {#sec-experiments}
In this section, we outline the experimental setup for the evaluation of the pose estimators on the four datasets.

## General Setup
We executed a total of seven pose estimators on the four datasets (TED kid video, TED talks, Tragic Talkers and masked video dataset).
The pose estimators are: YoloPose (v11-l), MediaPipePose (pose_landmarker_heavy), OpenPose (body_25), MaskAnyoneAPI-MediaPipe, MaskAnyoneAPI-OpenPose, MaskAnyoneUI-MediaPipe and MaskAnyoneUI-OpenPose.
We set a confidence threshold of 0.3 for YoloPose and MediaPipePose, and 0.15 for OpenPose, for filtering out low-confidence detections.
A confidence threshold of zero is used for all MaskAnyone pose estimators, as the output of MaskAnyone does not contain a confidence score.
All keypoints are stored in the COCO format to allow for a per-keypoint comparison of the results.

## TED Kid Video and TED Talks {#sec-experiments-ted-kid-talks}
For the TED kid video and the TED talks, we evaluated the metrics velocity, acceleration and jerk for each pose estimator.
Because no ground truth is available for the TED talks, we cannot evaluate the accuracy of the pose estimation.

## Tragic Talkers {#sec-experiments-tragic-talkers}
For the Tragic Talkers dataset, we evaluated metrics based on ground-truth like Euclidean distance, PCK and RMSE, and also kinematic metrics like velocity, acceleration and jerk for each pose estimator.
Important to note is, that the Tragic Talkers dataset contains only "pseudo-ground truth" annotations, which were created by the AI model OpenPose.
Unfortunately, it is not noted in the dataset nor paper, which OpenPose model was used to create the pose results, which limits the comparability of the results.

## Inference on Raw vs. Masked Videos {#sec-experiments-inference-raw-masked}
For the masked video dataset, we first performed inference on the raw videos with all seven pose estimators to establish a baseline without any masking.
Thereafter, for each hiding strategy, the pose estimators were executed again on the masked videos.
The results were then compared to the baseline to evaluate the impact of the different hiding strategies on the pose estimation performance.
We used the PCK and RMSE metrics to evaluate the accuracy of the pose estimation compared to the pose result on the raw videos.


# Results {#sec-results}
(Tim - 4.)

## TED Kid Video {#sec-results-ted-kid}
::: {#tbl-results-ted-kid}
```{=html}
<table class="results"><thead>
  <tr>
    <th>Pose Estimator</th>
    <th>Velocity</th>
    <th>Acceleration</th>
    <th>Jerk</th>
  </tr></thead>
<tbody>
  <tr>
    <td>YoloPose</td>
    <td>2.81</td>
    <td>2.95</td>
    <td>5.04</td>
  </tr>
  <tr>
    <td>MediaPipePose</td>
    <td>3.36</td>
    <td>4.10</td>
    <td>7.18</td>
  </tr>
  <tr>
    <td>OpenPose</td>
    <td>2.71</td>
    <td>3.20</td>
    <td>5.56</td>
  </tr>
  <tr class="maskanyone-api border-top">
    <td>MaskAnyoneAPI-MediaPipe</td>
    <td class="second">2.00</td>
    <td class="second">1.21</td>
    <td class="best">1.87</td>
  </tr>
  <tr class="maskanyone-api border-bottom">
    <td>MaskAnyoneAPI-OpenPose</td>
    <td>2.73</td>
    <td>2.46</td>
    <td>3.53</td>
  </tr>
  <tr class="maskanyone-ui border-top">
    <td>MaskAnyoneUI-MediaPipe</td>
    <td class="best">1.97</td>
    <td class="best">1.20</td>
    <td class="second">1.89</td>
  </tr>
  <tr class="maskanyone-ui border-bottom">
    <td>MaskAnyoneUI-OpenPose</td>
    <td>2.62</td>
    <td>2.09</td>
    <td>2.75</td>
  </tr>
</tbody>
</table>
```
Average metric results for different pose estimators on the TED-kid video.
:::

::: {#tbl-results-ted-kid-videos}
```{=html}
<table class="video-table">
    <tr>
        <td>
            <div class="video-zoom-wrapper ted-kid-video">
                <video class="video-zoom ted-kid-sync" autoplay muted playsinline>
                    <source src="videos/TED-kid/TED-kid.mp4" type="video/mp4">
                </video>
            </div>
            <div class="video-caption">Raw Video</div>
        </td>
        <td>
            <div class="video-zoom-wrapper ted-kid-video">
                <video class="video-zoom ted-kid-sync" autoplay muted playsinline>
                    <source src="videos/TED-kid/TED-kid_YoloPose.mp4" type="video/mp4">
                </video>
            </div>
            <div class="video-caption">YOLOPose</div>
        </td>
    </tr>
    <tr>
        <td>
            <div class="video-zoom-wrapper ted-kid-video">
                <video class="video-zoom ted-kid-sync" autoplay muted playsinline>
                    <source src="videos/TED-kid/TED-kid_MediaPipePose.mp4" type="video/mp4">
                </video>
            </div>
            <div class="video-caption">MediaPipe Pose</div>
        </td>
        <td>
            <div class="video-zoom-wrapper ted-kid-video">
                <video class="video-zoom ted-kid-sync" autoplay muted playsinline>
                    <source src="videos/TED-kid/TED-kid_OpenPose.mp4" type="video/mp4">
                </video>
            </div>
            <div class="video-caption">OpenPose</div>
        </td>
    </tr>
    <tr>
        <td>
            <div class="video-zoom-wrapper ted-kid-video">
                <video class="video-zoom ted-kid-sync" autoplay muted playsinline>
                    <source src="videos/TED-kid/TED-kid_MaskAnyoneAPI-MediaPipe.mp4" type="video/mp4">
                </video>
            </div>
            <div class="video-caption">MaskAnyoneAPI-MediaPipe</div>
        </td>
        <td>
            <div class="video-zoom-wrapper ted-kid-video">
                <video class="video-zoom ted-kid-sync" autoplay muted playsinline>
                    <source src="videos/TED-kid/TED-kid_MaskAnyoneAPI-OpenPose.mp4" type="video/mp4">
                </video>
            </div>
            <div class="video-caption">MaskAnyoneAPI-OpenPose</div>
        </td>
    </tr>
    <tr>
        <td>
            <div class="video-zoom-wrapper ted-kid-video">
                <video class="video-zoom ted-kid-sync" autoplay muted playsinline>
                    <source src="videos/TED-kid/TED-kid_MaskAnyoneUI-MediaPipe.mp4" type="video/mp4">
                </video>
            </div>
            <div class="video-caption">MaskAnyoneUI-MediaPipe</div>
        </td>
        <td>
            <div class="video-zoom-wrapper ted-kid-video">
                <video class="video-zoom ted-kid-sync" autoplay muted playsinline>
                    <source src="videos/TED-kid/TED-kid_MaskAnyoneUI-OpenPose.mp4" type="video/mp4">
                </video>
            </div>
            <div class="video-caption">MaskAnyoneUI-OpenPose</div>
        </td>
    </tr>
</table>

<script>
document.addEventListener('DOMContentLoaded', function() {
    const videos = document.querySelectorAll('.ted-kid-sync');
    let endedCount = 0;

    function restartAllVideos() {
        videos.forEach(video => {
            video.currentTime = 0;
            video.play();
        });
        endedCount = 0;
    }

    videos.forEach(video => {
        video.addEventListener('ended', () => {
            endedCount++;
            if (endedCount === videos.length) {
                restartAllVideos();
            }
        });
    });
});
</script>
```
Rendered result videos of different pose estimators on the TED-kid video.
:::

## TED Talks {#sec-results-ted-talks}
::: {#tbl-results-ted-talks}
```{=html}
<table class="results"><thead>
  <tr>
    <th>Pose Estimator</th>
    <th>Velocity</th>
    <th>Acceleration</th>
    <th>Jerk</th>
  </tr></thead>
<tbody>
  <tr>
    <td>YoloPose</td>
    <td class="second">1.35</td>
    <td>1.46</td>
    <td>2.44</td>
  </tr>
  <tr>
    <td>MediaPipePose</td>
    <td>3.29</td>
    <td>4.52</td>
    <td>7.94</td>
  </tr>
  <tr>
    <td>OpenPose</td>
    <td>1.58</td>
    <td>2.22</td>
    <td>3.44</td>
  </tr>
  <tr class="maskanyone-api border-top">
    <td>MaskAnyoneAPI-MediaPipe</td>
    <td>1.44</td>
    <td class="second">1.25</td>
    <td class="second">2.04</td>
  </tr>
  <tr class="maskanyone-api border-bottom">
    <td>MaskAnyoneAPI-OpenPose</td>
    <td>2.30</td>
    <td>2.42</td>
    <td>4.07</td>
  </tr>
  <tr class="maskanyone-ui border-top">
    <td>MaskAnyoneUI-MediaPipe</td>
    <td class="best">1.25</td>
    <td class="best">1.08</td>
    <td class="best">1.83</td>
  </tr>
  <tr class="maskanyone-ui border-bottom">
    <td>MaskAnyoneUI-OpenPose</td>
    <td>2.07</td>
    <td>2.29</td>
    <td>3.72</td>
  </tr>
</tbody>
</table>
```
Average metric results for different pose estimators aggregated over all TED talk videos.
:::

## Tragic Talkers {#sec-results-tragic-talkers}
::: {#tbl-results-tragic-talkers}
```{=html}
<table class="results"><thead>
  <tr>
    <th>Pose Estimator</th>
    <th>RMSE</th>
    <th>PCK</th>
    <th>Velocity</th>
    <th>Acceleration</th>
    <th>Jerk</th>
  </tr></thead>
<tbody>
  <tr>
    <td>YoloPose</td>
    <td class="second">0.11</td>
    <td class="best">0.96</td>
    <td>4.36</td>
    <td>3.27</td>
    <td>5.57</td>
  </tr>
  <tr>
    <td>MediaPipePose</td>
    <td>0.47</td>
    <td>0.69</td>
    <td>6.48</td>
    <td>9.80</td>
    <td>17.58</td>
  </tr>
  <tr>
    <td>OpenPose</td>
    <td>0.33</td>
    <td class="second">0.87</td>
    <td>4.63</td>
    <td>6.38</td>
    <td>10.00</td>
  </tr>
  <tr class="maskanyone-api border-top">
    <td>MaskAnyoneAPI-MediaPipe</td>
    <td>0.12</td>
    <td>0.78</td>
    <td class="second">3.46</td>
    <td class="best">2.86</td>
    <td class="best">5.01</td>
  </tr>
  <tr class="maskanyone-api border-bottom">
    <td>MaskAnyoneAPI-OpenPose</td>
    <td>0.36</td>
    <td>0.85</td>
    <td>5.69</td>
    <td>6.08</td>
    <td>10.22</td>
  </tr>
  <tr class="maskanyone-ui border-top">
    <td>MaskAnyoneUI-MediaPipe</td>
    <td class="best">0.07</td>
    <td>0.83</td>
    <td class="best">3.26</td>
    <td class="second">2.91</td>
    <td class="second">5.10</td>
  </tr>
  <tr class="maskanyone-ui border-bottom">
    <td>MaskAnyoneUI-OpenPose</td>
    <td>0.36</td>
    <td>0.85</td>
    <td>5.53</td>
    <td>5.12</td>
    <td>9.06</td>
  </tr>
</tbody>
</table>
```
Average metric results for different pose estimators aggregated over four camera angles of five Tragic Talkers sequences with pseudo-ground truth.
:::


## Inference on Raw vs. Masked Videos {#sec-results-inference-raw-masked}
::: {#tbl-pck-raw-masked}
```{=html}
<table class="results raw-masked-table">
  <thead>
    <tr>
      <th>Pose Estimator</th>
      <th>Blurring</th>
      <th>Pixelation</th>
      <th>Contours</th>
      <th>Inpainting</th>
      <th>Average</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>YoloPose</td>
      <td class="best">0.95</td>
      <td>0.09</td>
      <td class="best">0.93</td>
      <td class="second">0.32</td>
      <td class="second">0.57</td>
    </tr>
    <tr>
      <td>MediaPipePose</td>
      <td class="best">0.95</td>
      <td class="best">0.81</td>
      <td>0.56</td>
      <td class="best">0.34</td>
      <td class="best">0.67</td>
    </tr>
    <tr>
      <td>OpenPose</td>
      <td class="second">0.88</td>
      <td>0.10</td>
      <td class="second">0.62</td>
      <td>0.01</td>
      <td>0.40</td>
    </tr>
    <tr class="maskanyone-api border-top">
      <td>MaskAnyoneAPI-MediaPipe</td>
      <td>0.85</td>
      <td>0.30</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.29</td>
    </tr>
    <tr class="maskanyone-api border-bottom">
      <td>MaskAnyoneAPI-OpenPose</td>
      <td>0.75</td>
      <td>0.00</td>
      <td>0.36</td>
      <td>0.00</td>
      <td>0.28</td>
    </tr>
    <tr class="maskanyone-ui border-top">
      <td>MaskAnyoneUI-MediaPipe</td>
      <td class="best">0.95</td>
      <td class="second">0.63</td>
      <td>0.00</td>
      <td>0.07</td>
      <td>0.41</td>
    </tr>
    <tr class="maskanyone-ui border-bottom">
      <td>MaskAnyoneUI-OpenPose</td>
      <td>0.87</td>
      <td>0.03</td>
      <td>0.58</td>
      <td>0.00</td>
      <td>0.37</td>
    </tr>
  </tbody>
</table>
```
Percentage of correct keypoints (PCK) for different pose estimators on videos masked by different hiding strategies.
:::


::: {#tbl-rmse-raw-masked}
```{=html}
<table class="results raw-masked-table">
  <thead>
    <tr>
      <th>Pose Estimator</th>
      <th>Blurring</th>
      <th>Pixelation</th>
      <th>Contours</th>
      <th>Inpainting</th>
      <th>Average</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>YoloPose</td>
      <td class="best">0.12</td>
      <td>0.92</td>
      <td class="best">0.13</td>
      <td class="second">0.74</td>
      <td class="second">0.48</td>
    </tr>
    <tr>
      <td>MediaPipePose</td>
      <td class="best">0.12</td>
      <td class="best">0.26</td>
      <td>0.49</td>
      <td class="best">0.64</td>
      <td class="best">0.38</td>
    </tr>
    <tr>
      <td>OpenPose</td>
      <td class="second">0.25</td>
      <td>0.94</td>
      <td class="second">0.47</td>
      <td>1.0</td>
      <td>0.67</td>
    </tr>
    <tr class="maskanyone-api border-top">
      <td>MaskAnyoneAPI-MediaPipe</td>
      <td>0.27</td>
      <td>0.74</td>
      <td>1.00</td>
      <td>0.99</td>
      <td>0.75</td>
    </tr>
    <tr class="maskanyone-api border-bottom">
      <td>MaskAnyoneAPI-OpenPose</td>
      <td>0.43</td>
      <td>0.99</td>
      <td>0.75</td>
      <td>1.00</td>
      <td>0.79</td>
    </tr>
    <tr class="maskanyone-ui border-top">
      <td>MaskAnyoneUI-MediaPipe</td>
      <td class="best">0.07</td>
      <td class="second">0.41</td>
      <td>1.00</td>
      <td>0.94</td>
      <td>0.60</td>
    </tr>
    <tr class="maskanyone-ui border-bottom">
      <td>MaskAnyoneUI-OpenPose</td>
      <td>0.24</td>
      <td>0.98</td>
      <td>0.52</td>
      <td>1.00</td>
      <td>0.69</td>
    </tr>
  </tbody>
</table>
```
Root mean square error (RMSE) for different pose estimators on videos masked by different hiding strategies.
:::



# Future Work & Limitations {#sec-future-work}
(Zainab - 4.)
(Tim - 5.)

Implement head and torso normalization. This requires knowing the index of the head and torso keypoints. Care should be taken to not restraint the whole application from working on a specific keypoint set (like COCO) but still maintain flexibility in supporting various keypoint formats.




# Conclusion {#sec-conclusion}
(Tim - 7.)


# References


