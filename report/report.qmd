---
title: "MaskBench - A Comprehensive Benchmark Framework for Video De-Identification"
date: "2025 08 08"
author:
    - name: Tim Riedel
      affiliation: Hasso Plattner Institute, University of Potsdam, Germany
    - name: Zainab Zafari
      affiliation: Hasso Plattner Institute, University of Potsdam, Germany
    - name: Sharjeel Shaik
      affiliation: University of Potsdam, Germany
    - name: Babajide Alamu Owoyele
      affiliation: Hasso Plattner Institute, University of Potsdam, Germany
    - name: Wim Pouw
      affiliation: Tilburg University, Netherlands
contact:
    - name: Tim Riedel
      email: tim.riedel@student.hpi.de
    - name: Zainab Zafari
      email: zainab.zafari@student.hpi.de
    - name: Babajide Alamu Owoyele
      email: babajide.owoyele@student.hpi.de
    - name: Wim Pouw
      email: w.pouw@tilburguniversity.edu
bibliography: dependencies/refs.bib
css: dependencies/styles.css
theme: journal
format:
    html:
        toc: true
        toc-location: left
        toc-title: "Contents"
        toc-depth: 3
        number-sections: true
        code-fold: true
        code-tools: true
filters:
    - include-code-files
# engine: knitr
jupyter: python3
---

# Abstract / Overview {#sec-abstract}
(Tim - 6.)



# Getting Started {#sec-installation}
(Zainab - 3.)

## Installation / Setup {#sec-installation-setup}

## Usage {#sec-installation-usage}

## Configuration of Experiments {#sec-installation-configuration}




# Introduction {#sec-introduction}
(Zainab - 5.)



# Related Work {#sec-related-work}
(Zainab - 4.)



# MaskBench Architecture {#sec-architecture}
Figure of architecture & workflow (Zainab - 2.)

The general workflow of MaskBench is to first load the dataset, pose estimators and evaluation metrics.
The application creates a checkpoint folder in the specified `/output` directory, named after the dataset and a timestamp (e.g. `/output/TedTalks-20250724-121127`).
Thereafter, inference is performed on all videos of the dataset using the pose estimators specified in the configuration file. 
A `poses` folder is created within the checkpoint, with a subfolder for each pose estimator and a single json file for each video.
Thereafter, the application evaluates all the specified metrics and visualizes them in plots, which are stored in the `plots` folder in the checkpoint.
Lastly, for each video, the application creates a multiple rendered video, one for each pose estimator, which are stored in the `renderings` folder in the checkpoint.

Each component of MaskBench is implemented in a modular way, so that it can be easily extended and modified, which we will discuss in the following sections.

## Dataset {#sec-architecture-dataset}
The dataset provides the video data for the pose estimation and ground truth data for the evaluation, if available.
When adding a new dataset, the user needs to create a new class that inherits from the `Dataset` class and overwrite the `_load_samples` method, which creates one `VideoSample` object for each video in the dataset.
If the dataset provides ground truth data, the user additionally needs to overwrite the `get_gt_pose_results` and `get_gt_keypoint_pairs` methods.
For each video, the `get_gt_pose_results` method should return a `VideoPoseResult` object.
The `get_gt_keypoint_pairs` method is used for rendering the ground truth keypoints and contains a list of tuples, where each tuple contains the index of two keypoints to be connected in the rendered video. We provide default keypoint pairs for "YOLO", "Mediapipe" and various implementations of "OpenPose" models in the file `keypoint_pairs.py`.

Below is the code implementation for the abstract dataset class, for the very simple TED talks dataset (without ground truth) and the more complicated TragicTalkers dataset (with ground truth data).

::: {.callout-note collapse="true"}
## Dataset Class
```{.python include="../src/datasets/dataset.py" code-line-numbers="true" filename="src/datasets/dataset.py"}
```
:::

::: {.callout-note collapse="true"}
## TED Talks Dataset
```{.python include="../src/datasets/ted_dataset.py" code-line-numbers="true" filename="src/datasets/ted_dataset.py"}
```
:::

::: {.callout-note collapse="true"}
## Tragic Talkers Dataset
```{.python include="../src/datasets/tragic_talkers_dataset.py" code-line-numbers="true" filename="src/datasets/tragic_talkers_dataset.py"}
```
:::

## Inference {#sec-architecture-inference}

### Video Pose Result {#sec-architecture-video-pose-result}
The `VideoPoseResult` object is the standardized output of a pose prediction model.
It is a nested object that contains a `FramePoseResult` object for each frame in the video.
Within each frame pose result, there is a list of `PersonPoseResult` objects, one for each person in the frame.
Every result for a person contains a list of `PoseKeypoint` objects, one for each keypoint in the model output format, with the x and y coordinates and an optional confidence score.

::: {.callout-note collapse="true"}
## Video Pose Result Class
```{.python include="../src/inference/pose_result.py" code-line-numbers="true" filename="src/inference/pose_result.py"}
```
:::

### Pose Estimator {#sec-architecture-pose-estimators}
The pose estimators are responsible for predicting the poses of the persons in the video and wrap the call to specific AI models or pose estimation pipelines.
Each model is implemented in a separate class that inherits from the abstract `PoseEstimator` class.
It outputs a standardized `VideoPoseResult` object.

If users want to add a new pose estimator, they need to implement the `estimate_pose` method and the `get_keypoint_pairs` method.
Special care needs to be taken to ensure that the output is valid according to the following constraints:

1. The number of frames in the frame results matches the number of frames in the video.
2. If no persons were detected in a frame, the persons list should be empty.
3. If a person was detected, but some keypoints are missing, the missing keypoints should have the values `x=0, y=0, confidence=None`.
4. The number of keypoints for a person is constant across all frames.
5. Low confidence keypoints should be masked out using the `confidence_threshold` config parameter of the pose estimator.
6. Map the keypoints to COCO format if the `save_keypoints_in_coco_format` config parameter is set to true.


As an example, we provide the implementation of the abstract pose estimator class and the implementation of the YOLO model.

::: {.callout-note collapse="true"}
## Pose Estimator Class
```{.python include="../src/models/pose_estimator.py" code-line-numbers="true" filename="src/models/pose_estimator.py"}
```
:::

::: {.callout-note collapse="true"}
## YOLO Model
```{.python include="../src/models/yolo_pose_estimator.py" code-line-numbers="true" filename="src/models/yolo_pose_estimator.py"}
```
:::


### Inference Engine {#sec-architecture-inference-engine}
The inference engine is responsible for running the pose estimators on the videos and saving the results in the `poses` folder as json files.
If a checkpoint name is provided in the configuration file, the inference engine will load the results from the checkpoint and skip inference for the videos that already have results.
This allows to resume the inference process in case it fails or to skip the inference entirely and only evaluate the metrics.
The inference engine returns a nested dictionary, mapping pose estimator names to video names and `VideoPoseResult` objects.


## Evaluation {#sec-architecture-evaluation}

## Visualization {#sec-architecture-visualization}

## Rendering {#sec-architecture-rendering}




# Datasets {#sec-datasets}
(Zainab - 1.)

## TED Kid Video {#sec-datasets-ted-kid}

## TED Talks {#sec-datasets-ted-talks}

## Tragic Talkers {#sec-datasets-tragic-talkers}





# Experiments & Evaluation Metrics {#sec-experiments-metrics}
(Tim - 2.)

## Evaluation Metrics {#sec-metrics}

### Euclidean Distance {#sec-metrics-euclidean-distance}

### Percentage of Keypoints (PCK) {#sec-metrics-pck}

### Root Mean Square Error (RMSE) {#sec-metrics-rmse}

### Velocity {#sec-metrics-velocity}

### Acceleration {#sec-metrics-acceleration}

### Jerk {#sec-metrics-jerk}



## Experimental Setup {#sec-experiments}
(Tim - 3.)

### TED Kid Video {#sec-experiments-ted-kid}

### TED Talks {#sec-experiments-ted-talks}

### Tragic Talkers {#sec-experiments-tragic-talkers}

### Inference on Raw vs. Masked Videos {#sec-experiments-inference-raw-masked}




# Results {#sec-results}
(Tim - 4.)

## TED Kid Video {#sec-results-ted-kid}

## TED Kid Video {#sec-results-ted-talks}

## Tragic Talkers {#sec-results-tragic-talkers}

## Inference on Raw vs. Masked Videos {#sec-results-inference-raw-masked}




# Future Work & Limitations {#sec-future-work}
(Zainab - 4.)
(Tim - 5.)




# Conclusion {#sec-conclusion}
(Tim - 7.)


# References
::: {#refs}
:::

