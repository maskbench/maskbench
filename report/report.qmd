---
title: "MaskBench - A Comprehensive Benchmark Framework for Video De-Identification"
date: "2025 08 08"
author:
    - name: Tim Riedel
      affiliation: Hasso Plattner Institute, University of Potsdam, Germany
    - name: Zainab Zafari
      affiliation: Hasso Plattner Institute, University of Potsdam, Germany
    - name: Sharjeel Shaik
      affiliation: University of Potsdam, Germany
    - name: Babajide Alamu Owoyele
      affiliation: Hasso Plattner Institute, University of Potsdam, Germany
    - name: Wim Pouw
      affiliation: Tilburg University, Netherlands
contact:
    - name: Tim Riedel
      email: tim.riedel@student.hpi.de
    - name: Zainab Zafari
      email: zainab.zafari@student.hpi.de
    - name: Babajide Alamu Owoyele
      email: babajide.owoyele@student.hpi.de
    - name: Wim Pouw
      email: w.pouw@tilburguniversity.edu
bibliography: dependencies/refs.bib
css: dependencies/styles.css
theme: journal
format:
    html:
        toc: true
        toc-location: left
        toc-title: "Contents"
        toc-depth: 3
        number-sections: true
        code-fold: true
        code-tools: true
filters:
    - include-code-files
# engine: knitr
jupyter: python3
---

# Abstract / Overview {#sec-abstract}
(Tim - 6.)



# Getting Started {#sec-installation}
(Zainab - 3.)

## Installation / Setup {#sec-installation-setup}

## Usage {#sec-installation-usage}

## Configuration of Experiments {#sec-installation-configuration}




# Introduction {#sec-introduction}
(Zainab - 5.)



# Related Work {#sec-related-work}
(Zainab - 4.)



# MaskBench Architecture {#sec-architecture}
Figure of architecture & workflow (Zainab - 2.)

The general workflow of MaskBench is to first load the dataset, pose estimators and evaluation metrics.
The application creates a checkpoint folder in the specified `/output` directory, named after the dataset and a timestamp (e.g. `/output/TedTalks-20250724-121127`).
Thereafter, inference is performed on all videos of the dataset using the pose estimators specified in the configuration file. 
A `poses` folder is created within the checkpoint, with a subfolder for each pose estimator and a single json file for each video.
Thereafter, the application evaluates all the specified metrics and visualizes them in plots, which are stored in the `plots` folder in the checkpoint.
Lastly, for each video, the application creates a multiple rendered video, one for each pose estimator, which are stored in the `renderings` folder in the checkpoint.

Each component of MaskBench is implemented in a modular way, so that it can be easily extended and modified, which we will discuss in the following sections.

## Dataset {#sec-architecture-dataset}
The dataset provides the video data for the pose estimation and ground truth data for the evaluation, if available.
When adding a new dataset, the user needs to create a new class that inherits from the `Dataset` class and overwrite the `_load_samples` method, which creates one `VideoSample` object for each video in the dataset.
If the dataset provides ground truth data, the user additionally needs to overwrite the `get_gt_pose_results` and `get_gt_keypoint_pairs` methods.
For each video, the `get_gt_pose_results` method should return a `VideoPoseResult` object.
The `get_gt_keypoint_pairs` method is used for rendering the ground truth keypoints and contains a list of tuples, where each tuple contains the index of two keypoints to be connected in the rendered video. We provide default keypoint pairs for "YOLO", "Mediapipe" and various implementations of "OpenPose" models in the file `keypoint_pairs.py`.

Below is the code implementation for the abstract dataset class, for the very simple TED talks dataset (without ground truth) and the more complicated TragicTalkers dataset (with ground truth data).

::: {.callout-note collapse="true"}
## Dataset Class
```{.python include="../src/datasets/dataset.py" code-line-numbers="true" filename="src/datasets/dataset.py"}
```
:::

::: {.callout-note collapse="true"}
## TED Talks Dataset
```{.python include="../src/datasets/ted_dataset.py" code-line-numbers="true" filename="src/datasets/ted_dataset.py"}
```
:::

::: {.callout-note collapse="true"}
## Tragic Talkers Dataset
```{.python include="../src/datasets/tragic_talkers_dataset.py" code-line-numbers="true" filename="src/datasets/tragic_talkers_dataset.py"}
```
:::

## Inference {#sec-architecture-inference}

### Video Pose Result {#sec-architecture-video-pose-result}
The `VideoPoseResult` object is the standardized output of a pose prediction model.
It is a nested object that contains a `FramePoseResult` object for each frame in the video.
Within each frame pose result, there is a list of `PersonPoseResult` objects, one for each person in the frame.
Every result for a person contains a list of `PoseKeypoint` objects, one for each keypoint in the model output format, with the x and y coordinates and an optional confidence score.

::: {.callout-note collapse="true"}
## Video Pose Result Class
```{.python include="../src/inference/pose_result.py" code-line-numbers="true" filename="src/inference/pose_result.py"}
```
:::

### Pose Estimator {#sec-architecture-pose-estimators}
The pose estimators are responsible for predicting the poses of the persons in the video and wrap the call to specific AI models or pose estimation pipelines.
Each model is implemented in a separate class that inherits from the abstract `PoseEstimator` class.
It outputs a standardized `VideoPoseResult` object.

If users want to add a new pose estimator, they need to implement the `estimate_pose` method and the `get_keypoint_pairs` method.
Special care needs to be taken to ensure that the output is valid according to the following constraints:

1. The number of frames in the frame results matches the number of frames in the video.
2. If no persons were detected in a frame, the persons list should be empty.
3. If a person was detected, but some keypoints are missing, the missing keypoints should have the values `x=0, y=0, confidence=None`.
4. The number of keypoints for a person is constant across all frames.
5. Low confidence keypoints should be masked out using the `confidence_threshold` config parameter of the pose estimator.
6. Map the keypoints to COCO format if the `save_keypoints_in_coco_format` config parameter is set to true.


As an example, we provide the implementation of the abstract pose estimator class and the implementation of the YOLO model.

::: {.callout-note collapse="true"}
## Pose Estimator Class
```{.python include="../src/models/pose_estimator.py" code-line-numbers="true" filename="src/models/pose_estimator.py"}
```
:::

::: {.callout-note collapse="true"}
## YOLO Model
```{.python include="../src/models/yolo_pose_estimator.py" code-line-numbers="true" filename="src/models/yolo_pose_estimator.py"}
```
:::


### Inference Engine {#sec-architecture-inference-engine}
The inference engine is responsible for running the pose estimators on the videos and saving the results in the `poses` folder as json files.
If a checkpoint name is provided in the configuration file, the inference engine will load the results from the checkpoint and skip inference for the videos that already have results.
This allows to resume the inference process in case it fails or to skip the inference entirely and only evaluate the metrics.
The inference engine returns a nested dictionary, mapping pose estimator names to video names and `VideoPoseResult` objects.


## Evaluation {#sec-architecture-evaluation}

### Metric {#sec-architecture-evaluation-metric}
Each metric inherits from the abstract `Metric` class and implements the `compute` method, which takes as input one predicted video pose result, a ground truth pose result, if available, and the name of the pose estimator.
It outputs a `MetricResult` object, which contains the metric values for the video (see section @sec-architecture-evaluation-metric-result).

::: {.callout-note collapse="true"}
## Metric Class
```{.python include="../src/evaluation/metrics/metric.py" code-line-numbers="true" filename="src/evaluation/metrics/metric.py"}
```
:::

MaskBench currently implements ground truth-based metrics for Euclidean Distance, Percentage of Correct Keypoints (PCK) and Root Mean Square Error (RMSE).
Furthermore, we provide kinematic metrics for velocity, acceleration and jerk.
Section @sec-metrics contains a more extensive description of the implemented metrics.

**Matching Person Indices**

For some metrics, it is crucial to ensure, that the order of persons in video pose results match between a prediction and a reference.
The metric class provides a method called `match_person_indices`, which is used in ground-truth based metrics to ensure that person indices match between the ground truth and the prediction.
Furthermore, kinematic metrics also make use of this method, as they can only be calculated if the person indices match between consecutive frames.
The implementation uses the Hungarian algorithm and the mean of a person's keypoints to find the best match between all persons in the reference and predicted pose result.

Let $N$ be the number of persons in the reference, $M$ be the number of persons in the predicted pose result, and $K$ be the number of keypoints per person.
The output of the match person indices method is an array of shape $\text{max}(N, M) \times K \times 2$, where the first $N$ positions contain the persons in the same order as in the reference.
The last $N$ to $M$ positions (in case $M > N$) contain the remaining persons, which are not present in the reference.

Edge cases include, where the first of two persons appears in one frame, but not in the next.
In this case, the second person is still assigned the second index in the output array, while the first index contains infinite values to signal that the person is not present.
The same applies, if the prediction contains less persons than the reference ($M<N$).
Each metric can then decide how to handle these infinite values, for example by setting them to `NaN` (kinematic metrics) or a pre-defined value (euclidean distance and ground truth-based metrics).

**Unit Testing**

It is good practice to implement unit tests for the various metric classes to ensure that the output is correct and as expected.
We provide unit tests for all metrics in the `src/tests` folder, which can be run with the command `pytest`.
Running these tests after a change to the metric classes ensures that no other functionality is compromised.


### Metric Result {#sec-architecture-evaluation-metric-result}
The output of the metric's `compute` method is a `MetricResult` object.
It contains the metric values in the form of a multi-dimensional array, where the axes are labeled with names, for example with a "frame", "person" and "keypoint" axis.
The `aggregate` function of the class aggregates the values with a given method and along the specified axes only.
Currently MaskBench supports mean, median, Root Mean Square Error, vector magnitude, sum, min and max as aggregation methods.
The output of the aggregation method is once again a metric result, reduced in its dimensionality, having only the axes along which it was not aggregated.

This flexible approach of storing the results with their axes names and using the names in the aggregation method allows for the visualization of the results in a variety of ways, for example as a per keypoint plot, distribution plot or as a single scalar value. Furthermore, it allows extending the framework with new metrics (possibly containing different axis names) and also different visualizations.

### Evaluator {#sec-architecture-evaluation-evaluator}
Given a list of metrics, the evaluator executes the configured metrics on the pose estimation results for all pose estimators and videos.
It returns a nested dictionary, mapping metric names to pose estimator names to video names to `MetricResult` objects.
It does not perform aggregation over the videos or pose estimators in order to allow for more flexibility in the visualization of the results.

## Visualization {#sec-architecture-visualization}

## Rendering {#sec-architecture-rendering}




# Datasets {#sec-datasets}
(Zainab - 1.)

## TED Kid Video {#sec-datasets-ted-kid}

## TED Talks {#sec-datasets-ted-talks}

## Tragic Talkers {#sec-datasets-tragic-talkers}





# Experiments & Evaluation Metrics {#sec-experiments-metrics}
(Tim - 2.)

## Evaluation Metrics {#sec-metrics}

### Euclidean Distance {#sec-metrics-euclidean-distance}

### Percentage of Keypoints (PCK) {#sec-metrics-pck}

### Root Mean Square Error (RMSE) {#sec-metrics-rmse}

### Velocity {#sec-metrics-velocity}

### Acceleration {#sec-metrics-acceleration}

### Jerk {#sec-metrics-jerk}



## Experimental Setup {#sec-experiments}
(Tim - 3.)

### TED Kid Video {#sec-experiments-ted-kid}

### TED Talks {#sec-experiments-ted-talks}

### Tragic Talkers {#sec-experiments-tragic-talkers}

### Inference on Raw vs. Masked Videos {#sec-experiments-inference-raw-masked}




# Results {#sec-results}
(Tim - 4.)

## TED Kid Video {#sec-results-ted-kid}

## TED Kid Video {#sec-results-ted-talks}

## Tragic Talkers {#sec-results-tragic-talkers}

## Inference on Raw vs. Masked Videos {#sec-results-inference-raw-masked}




# Future Work & Limitations {#sec-future-work}
(Zainab - 4.)
(Tim - 5.)




# Conclusion {#sec-conclusion}
(Tim - 7.)


# References
::: {#refs}
:::

